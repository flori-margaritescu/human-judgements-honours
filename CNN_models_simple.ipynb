{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Importing Drive Datasets </h1>"
      ],
      "metadata": {
        "id": "In1wlSGFIdVB"
      },
      "id": "In1wlSGFIdVB"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/flori-margaritescu/human-judgements-honours.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk-k5K5aJtaB",
        "outputId": "b36b2b1e-7b6b-4b75-aa61-1489b5e1d1c5"
      },
      "id": "Fk-k5K5aJtaB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'human-judgements-honours'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  14% (1/7)\u001b[K\rremote: Compressing objects:  28% (2/7)\u001b[K\rremote: Compressing objects:  42% (3/7)\u001b[K\rremote: Compressing objects:  57% (4/7)\u001b[K\rremote: Compressing objects:  71% (5/7)\u001b[K\rremote: Compressing objects:  85% (6/7)\u001b[K\rremote: Compressing objects: 100% (7/7)\u001b[K\rremote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 9 (delta 2), reused 7 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  11% (1/9)\rUnpacking objects:  22% (2/9)\rUnpacking objects:  33% (3/9)\rUnpacking objects:  44% (4/9)\rUnpacking objects:  55% (5/9)\rUnpacking objects:  66% (6/9)\rUnpacking objects:  77% (7/9)\rUnpacking objects:  88% (8/9)\rUnpacking objects: 100% (9/9)\rUnpacking objects: 100% (9/9), 4.34 KiB | 2.17 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmOC1OCdIcy7",
        "outputId": "8605842d-c767-4151-c8c5-719c77c20ee4"
      },
      "id": "YmOC1OCdIcy7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/distortion_triplets.zip\" -d \"/content/datasets/distortions\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/hsj_triplets.zip\" -d \"/content/datasets/hsj\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/birds_dataset_triplets.zip\" -d \"/content/datasets/birds-16\""
      ],
      "metadata": {
        "id": "LlfBBzehJFc-"
      },
      "id": "LlfBBzehJFc-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9065aff9",
      "metadata": {
        "id": "9065aff9"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install img2vec-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD9whTxmMTN-",
        "outputId": "77650191-5560-47f4-9696-42f6af4b5d72"
      },
      "id": "zD9whTxmMTN-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting img2vec-pytorch\n",
            "  Downloading img2vec_pytorch-1.0.1-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->img2vec-pytorch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->img2vec-pytorch) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->img2vec-pytorch) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (2022.12.7)\n",
            "Installing collected packages: img2vec-pytorch\n",
            "Successfully installed img2vec-pytorch-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5158285a",
      "metadata": {
        "id": "5158285a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from img2vec_pytorch import Img2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc372be2",
      "metadata": {
        "id": "fc372be2"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: AlexNet and ResNet-18 take in completely different Tensor formats!\n",
        "\n",
        "<h2> Note: the below package does the feature extraction for us aparently - careful with how you use it - better start from scratch for consistency of results!</h2>"
      ],
      "metadata": {
        "id": "FvunBJEsNP77"
      },
      "id": "FvunBJEsNP77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5b2f0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd5b2f0c",
        "outputId": "022700a6-d559-45df-865d-e5f98b5a40a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4096])\n",
            "torch.Size([1, 512, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Initialize Img2Vec with GPU\n",
        "img2vec_alexnet = Img2Vec(cuda=False, model=\"alexnet\")  # default model resnet\n",
        "\n",
        "# Read in an image (rgb format)\n",
        "img = Image.open('test.png').convert('RGB')\n",
        "\n",
        "# Get a vector from img2vec, returned as a torch FloatTensor\n",
        "vec = img2vec_alexnet.get_vec(img, tensor=True)\n",
        "\n",
        "print(vec.size())\n",
        "\n",
        "# Initialize Img2Vec with GPU\n",
        "img2vec_18 = Img2Vec(cuda=False, model=\"resnet-18\")  # default model resnet\n",
        "\n",
        "# Read in an image (rgb format)\n",
        "img = Image.open('test.png').convert('RGB')\n",
        "# Get a vector from img2vec, returned as a torch FloatTensor\n",
        "vec = img2vec_18.get_vec(img, tensor=True)  \n",
        "\n",
        "print(vec.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a4db4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "a6a4db4c",
        "outputId": "50d1209f-eabc-444b-9a22-87ca876c739d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-940622e41615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ref.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mref_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mp0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p0.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp0_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img2vec' is not defined"
          ]
        }
      ],
      "source": [
        "ref = Image.open('ref.png').convert('RGB')\n",
        "ref_vec = img2vec.get_vec(ref, tensor=True)\n",
        "\n",
        "p0 = Image.open('p0.png').convert('RGB')\n",
        "p0_vec = img2vec.get_vec(p0, tensor=True)\n",
        "\n",
        "p1 = Image.open('p1.png').convert('RGB')\n",
        "p1_vec = img2vec.get_vec(p1, tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cf0baa",
      "metadata": {
        "id": "93cf0baa",
        "outputId": "13dd189d-8464-43f2-d48c-d28cb4d7729c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cosine similarity: tensor([0.4113])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using PyTorch Cosine Similarity\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "cos_sim = cos(ref_vec,\n",
        "              p0_vec)\n",
        "print('\\nCosine similarity: {0}\\n'.format(cos_sim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314ba326",
      "metadata": {
        "scrolled": true,
        "id": "314ba326",
        "outputId": "70eaf33a-388b-4b5f-eab0-5b3a961ba893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cosine similarity: tensor([0.3291])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using PyTorch Cosine Similarity\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "cos_sim = cos(ref_vec,\n",
        "              p1_vec)\n",
        "print('\\nCosine similarity: {0}\\n'.format(cos_sim))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1e953e",
      "metadata": {
        "id": "5f1e953e"
      },
      "source": [
        "### Common functions for loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aade6f16",
      "metadata": {
        "id": "aade6f16"
      },
      "outputs": [],
      "source": [
        "def format_image_number(number):\n",
        "    img_no_str = str(number)\n",
        "    \n",
        "    if len(img_no_str) == 1:\n",
        "        im_no_name = \"00000\"\n",
        "    elif len(img_no_str) == 2:\n",
        "        im_no_name = \"0000\"\n",
        "    elif len(img_no_str) == 3:\n",
        "        im_no_name = \"000\"\n",
        "    elif len(img_no_str) == 4:\n",
        "        im_no_name = \"00\"\n",
        "    elif len(img_no_str) == 5:\n",
        "        im_no_name = \"0\"\n",
        "    elif len(img_no_str) == 6:\n",
        "        im_no_name = \"\"\n",
        "        \n",
        "    return im_no_name + img_no_str\n",
        "    \n",
        "\n",
        "def get_images_vector(path, number_of_images):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_list = []\n",
        "    \n",
        "    for image_no in range(number_of_images):\n",
        "        image_no_str = str(image_no)\n",
        "        \n",
        "        im_no_name = format_image_number(image_no_str) + \".png\"\n",
        "        \n",
        "        path_to_image = path + im_no_name\n",
        "\n",
        "        feature_vector_image = Image.open(path_to_image).convert('RGB')\n",
        "        feature_vec = img2vec.get_vec(feature_vector_image, tensor=True)\n",
        "        feature_tensor_list.append(feature_vec)\n",
        "        \n",
        "    return feature_tensor_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d7d1a43",
      "metadata": {
        "id": "2d7d1a43"
      },
      "outputs": [],
      "source": [
        "def obtain_cosine_similarity_model_predictions(number_of_images, refs_vec_list, p0s_rec_list, p1s_rec_list):\n",
        "    cosine_similarity_ref_and_p0 = []\n",
        "    cosine_similarity_ref_and_p1 = []\n",
        "\n",
        "    # list of predictions\n",
        "    cosine_similarity_predictions = []\n",
        "    \n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "    for image_no in range(number_of_images): \n",
        "        reshaped_ref = torch.reshape(refs_vec_list[image_no], (1,512))\n",
        "        reshaped_p0 = torch.reshape(p0s_rec_list[image_no], (1,512))\n",
        "        reshaped_p1 = torch.reshape(p1s_rec_list[image_no], (1,512))\n",
        "        \n",
        "        cosine_similarity_ref_and_p0.append(cos(reshaped_ref, reshaped_p0))\n",
        "        cosine_similarity_ref_and_p1.append(cos(reshaped_ref, reshaped_p1))\n",
        "\n",
        "        if cosine_similarity_ref_and_p0[image_no] >= cosine_similarity_ref_and_p1[image_no]:\n",
        "            cosine_similarity_predictions.append(0)\n",
        "        else:\n",
        "            cosine_similarity_predictions.append(1)   \n",
        "            \n",
        "    return cosine_similarity_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ee8ae1",
      "metadata": {
        "id": "66ee8ae1"
      },
      "outputs": [],
      "source": [
        "def obtain_prediction_accuracy(number_of_images, path_to_decision, actual_predictions):\n",
        "    expected_decision_outputs = []\n",
        "\n",
        "    for image_no in range(number_of_images):\n",
        "            image_no_str = str(image_no)\n",
        "\n",
        "            im_no_name = format_image_number(image_no_str) + \".npy\"\n",
        "            path_to_image = path_to_decision + im_no_name\n",
        "\n",
        "            decision = np.load(path_to_image)\n",
        "\n",
        "            if decision[0] <= 0.5: \n",
        "                expected_decision_outputs.append(0)\n",
        "            else: \n",
        "                expected_decision_outputs.append(1)\n",
        "                \n",
        "    number_wrong_predictions = 0\n",
        "\n",
        "    for image_no in range(number_of_images):\n",
        "        if actual_predictions[image_no] != expected_decision_outputs[image_no]:\n",
        "            number_wrong_predictions += 1\n",
        "\n",
        "    accuracy = (number_of_images - number_wrong_predictions)/(number_of_images)\n",
        "    \n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c2d02d",
      "metadata": {
        "id": "68c2d02d"
      },
      "outputs": [],
      "source": [
        "def reshape_tensor_list(dim, tensor_list):\n",
        "    reshaped_list = []\n",
        "    for t in tensor_list:\n",
        "        reshaped_t = torch.reshape(t, (1,dim))\n",
        "        reshaped_list.append(t)\n",
        "    return reshaped_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49678780",
      "metadata": {
        "id": "49678780"
      },
      "source": [
        "### 1. Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f39e4e",
      "metadata": {
        "id": "73f39e4e"
      },
      "outputs": [],
      "source": [
        "img2vec_res = Img2Vec(cuda=False, model=\"resnet-18\")  # default model resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22520bf3",
      "metadata": {
        "id": "22520bf3"
      },
      "source": [
        "#### 1.1 Traditional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27073e7e",
      "metadata": {
        "id": "27073e7e"
      },
      "outputs": [],
      "source": [
        "traditional_refs = get_images_vector(\"val/traditional/ref/\", 4720)\n",
        "traditional_p0s = get_images_vector(\"val/traditional/p0/\", 4720)\n",
        "traditional_p1s = get_images_vector(\"val/traditional/p1/\", 4720)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2a3043",
      "metadata": {
        "id": "0a2a3043",
        "outputId": "57984062-bb0c-4a12-962c-3d6f15db82c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4720\n",
            "4720\n",
            "4720\n"
          ]
        }
      ],
      "source": [
        "print(len(traditional_refs))\n",
        "print(len(traditional_p0s))\n",
        "print(len(traditional_p1s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a90a85",
      "metadata": {
        "id": "46a90a85"
      },
      "outputs": [],
      "source": [
        "actual_model_predictions = obtain_cosine_similarity_model_predictions(4720, traditional_refs, traditional_p0s, traditional_p1s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b82c5f",
      "metadata": {
        "id": "09b82c5f"
      },
      "outputs": [],
      "source": [
        "prediction_accuracy = obtain_prediction_accuracy(4720, \"val/traditional/judge/\", actual_model_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ca7cef",
      "metadata": {
        "id": "d8ca7cef",
        "outputId": "c17e67e3-3f3e-4ae4-81aa-463eaf143853"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8048728813559322"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07c6190",
      "metadata": {
        "id": "e07c6190"
      },
      "source": [
        "Prediction accuracy for Resnet-18 for traditional distortions: 0.8048728813559322"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077a16c9",
      "metadata": {
        "id": "077a16c9"
      },
      "source": [
        "#### 1.2 CNN-based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95c1374",
      "metadata": {
        "id": "d95c1374"
      },
      "outputs": [],
      "source": [
        "cnn_refs = get_images_vector(\"val/cnn/ref/\", 4720)\n",
        "cnn_p0s = get_images_vector(\"val/cnn/p0/\", 4720)\n",
        "cnn_p1s = get_images_vector(\"val/cnn/p1/\", 4720)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2e7f2d",
      "metadata": {
        "id": "7d2e7f2d",
        "outputId": "eddb6768-5e79-4bca-b6a1-d9f996a8f39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4720\n",
            "4720\n",
            "4720\n"
          ]
        }
      ],
      "source": [
        "print(len(cnn_refs))\n",
        "print(len(cnn_p0s))\n",
        "print(len(cnn_p1s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131aa574",
      "metadata": {
        "id": "131aa574"
      },
      "outputs": [],
      "source": [
        "actual_model_predictions_cnn_distortion = obtain_cosine_similarity_model_predictions(4720, cnn_refs, cnn_p0s, cnn_p1s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d427a1f8",
      "metadata": {
        "id": "d427a1f8"
      },
      "outputs": [],
      "source": [
        "prediction_accuracy_cnn_distortion = obtain_prediction_accuracy(4720, \"val/cnn/judge/\", actual_model_predictions_cnn_distortion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7534f0a4",
      "metadata": {
        "id": "7534f0a4",
        "outputId": "8030cb62-a4a4-43a3-f062-d1cad56f5574"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8442796610169492"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_accuracy_cnn_distortion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93478b18",
      "metadata": {
        "id": "93478b18"
      },
      "source": [
        "Prediction accuracy for Resnet-18 for CNN-based distortions: 0.8442796610169492"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b8f83f",
      "metadata": {
        "id": "95b8f83f"
      },
      "source": [
        "#### 1.3 Colorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca880a7",
      "metadata": {
        "id": "eca880a7"
      },
      "outputs": [],
      "source": [
        "color_refs = get_images_vector(\"val/color/ref/\", 4720)\n",
        "color_p0s = get_images_vector(\"val/color/p0/\", 4720)\n",
        "color_p1s = get_images_vector(\"val/color/p1/\", 4720)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e90d57d",
      "metadata": {
        "id": "1e90d57d",
        "outputId": "84d5d237-6a45-4d71-be67-7a8de491d041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4720\n",
            "4720\n",
            "4720\n"
          ]
        }
      ],
      "source": [
        "print(len(color_refs))\n",
        "print(len(color_p0s))\n",
        "print(len(color_p1s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3829085",
      "metadata": {
        "id": "d3829085"
      },
      "outputs": [],
      "source": [
        "actual_model_predictions_color_distortion = obtain_cosine_similarity_model_predictions(4720, color_refs, color_p0s, color_p1s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcd0778",
      "metadata": {
        "id": "8dcd0778"
      },
      "outputs": [],
      "source": [
        "prediction_accuracy_color_distortion = obtain_prediction_accuracy(4720, \"val/color/judge/\", actual_model_predictions_color_distortion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe4ff36",
      "metadata": {
        "id": "2fe4ff36",
        "outputId": "954388b2-ceb4-470d-d7c4-0328c8b9d54b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6533898305084745"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_accuracy_color_distortion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c68ea4",
      "metadata": {
        "id": "d0c68ea4"
      },
      "source": [
        "Prediction accuracy for Resnet-18 for colorizaton distortions: 0.6533898305084745"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7687f12b",
      "metadata": {
        "id": "7687f12b"
      },
      "source": [
        "#### 1.4 Deblurring --- IGNORE FOR NOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190dcbb3",
      "metadata": {
        "id": "190dcbb3"
      },
      "outputs": [],
      "source": [
        "deb_refs = get_images_vector(\"val/deblur/ref/\", 9440)\n",
        "deb_p0s = get_images_vector(\"val/deblur/p0/\", 9440)\n",
        "deb_p1s = get_images_vector(\"val/deblur/p1/\", 9440)\n",
        "\n",
        "fi_refs = get_images_vector(\"val/frameinterp/ref/\", 1888)\n",
        "fi_p0s = get_images_vector(\"val/frameinterp/p0/\", 1888)\n",
        "fi_p1s = get_images_vector(\"val/frameinterp/p1/\", 1888)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e570fb8a",
      "metadata": {
        "id": "e570fb8a",
        "outputId": "4dd1ca95-e2ae-4ead-f6e9-87b7bf117443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4720\n",
            "4720\n",
            "4720\n"
          ]
        }
      ],
      "source": [
        "print(len(deb_refs))\n",
        "print(len(deb_p0s))\n",
        "print(len(deb_p1s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25912505",
      "metadata": {
        "id": "25912505"
      },
      "outputs": [],
      "source": [
        "actual_model_predictions_deb_distortion = obtain_cosine_similarity_model_predictions(4720, deb_refs, deb_p0s, deb_p1s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b696dd69",
      "metadata": {
        "id": "b696dd69"
      },
      "outputs": [],
      "source": [
        "prediction_accuracy_deb_distortion = obtain_prediction_accuracy(4720, \"val/deblur/judge/\", actual_model_predictions_deb_distortion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fe912d",
      "metadata": {
        "id": "24fe912d",
        "outputId": "61bd0650-3f54-44c4-a02e-adeb73c55076"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6016949152542372"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_accuracy_deb_distortion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dd7e700",
      "metadata": {
        "id": "6dd7e700"
      },
      "source": [
        "Prediction accuracy for Resnet-18 for deblurring distortions: 0.6016949152542372"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f340a2",
      "metadata": {
        "id": "73f340a2"
      },
      "source": [
        "#### 1.4, 1.5 and 1.6 Deblurring, frame interpolation and super resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b99584",
      "metadata": {
        "id": "40b99584",
        "outputId": "fca9b8f2-0ebe-43ca-d9b7-de866672149d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deblurring is complete\n",
            "FI is complete\n",
            "Spr is complete\n"
          ]
        }
      ],
      "source": [
        "deblur_refs = get_images_vector(\"val/deblur/ref/\", 9440)\n",
        "deblur_p0s = get_images_vector(\"val/deblur/p0/\", 9440)\n",
        "deblur_p1s = get_images_vector(\"val/deblur/p1/\", 9440)\n",
        "print(\"Deblurring is complete\")\n",
        "\n",
        "fi_refs = get_images_vector(\"val/frameinterp/ref/\", 1888)\n",
        "fi_p0s = get_images_vector(\"val/frameinterp/p0/\", 1888)\n",
        "fi_p1s = get_images_vector(\"val/frameinterp/p1/\", 1888)\n",
        "print(\"FI is complete\")\n",
        "\n",
        "spr_refs = get_images_vector(\"val/superres/ref/\", 10856)\n",
        "spr_p0s = get_images_vector(\"val/superres/p0/\", 10856)\n",
        "spr_p1s = get_images_vector(\"val/superres/p1/\", 10856)\n",
        "print(\"Spr is complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10596319",
      "metadata": {
        "id": "10596319",
        "outputId": "23d60384-bb5d-4f2b-c600-41b252b4b548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deblur: \n",
            "0.6113347457627119\n",
            "----------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "actual_model_predictions_blur_distortion = obtain_cosine_similarity_model_predictions(9440, deblur_refs, deblur_p0s, deblur_p1s)\n",
        "prediction_accuracy_blur_distortion = obtain_prediction_accuracy(9440, \"val/deblur/judge/\", actual_model_predictions_blur_distortion)\n",
        "print(\"Deblur: \")\n",
        "print(prediction_accuracy_blur_distortion)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e767016",
      "metadata": {
        "id": "5e767016",
        "outputId": "bea9373e-0be1-40a2-9f20-17a820ae6c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fi: \n",
            "0.6604872881355932\n",
            "----------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "actual_model_predictions_frame = obtain_cosine_similarity_model_predictions(1888, fi_refs, fi_p0s, fi_p1s)\n",
        "prediction_accuracy_frame = obtain_prediction_accuracy(1888, \"val/frameinterp/judge/\", actual_model_predictions_frame)\n",
        "print(\"Fi: \")\n",
        "print(prediction_accuracy_frame)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85a9750",
      "metadata": {
        "id": "b85a9750",
        "outputId": "4a0e4ee8-087d-4ac7-aaaf-b859068cae73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spr: \n",
            "0.7198784082535004\n",
            "----------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "actual_model_predictions_cnn_distortion_super = obtain_cosine_similarity_model_predictions(10856, spr_refs, spr_p0s, spr_p1s)\n",
        "prediction_accuracy_cnn_distortion_super = obtain_prediction_accuracy(10856, \"val/superres/judge/\", actual_model_predictions_cnn_distortion_super)\n",
        "print(\"Spr: \")\n",
        "print(prediction_accuracy_cnn_distortion_super)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c4672a",
      "metadata": {
        "id": "45c4672a"
      },
      "source": [
        "Prediction accuracy for Resnet-18 for deblurring distortions: 0.6113347457627119\n",
        "\n",
        "Prediction accuracy for Resnet-18 for frame interpolation distortions: 0.6604872881355932\n",
        "\n",
        "Prediction accuracy for Resnet-18 for super resolution distortions: 0.7198784082535004"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb9f1bc",
      "metadata": {
        "id": "8eb9f1bc"
      },
      "source": [
        "### 2. AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cad9bf9",
      "metadata": {
        "id": "3cad9bf9",
        "outputId": "9c48d905-9cfe-4132-9fd8-a05da2fb3c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alexnet results:\n",
            "Deblurring is complete\n",
            "FI is complete\n",
            "Spr is complete\n",
            "Traditional: \n",
            "0.8108050847457627\n",
            "----------------\n",
            "\n",
            "\n",
            "CNN: \n",
            "0.8544491525423729\n",
            "----------------\n",
            "\n",
            "\n",
            "Colorization: \n",
            "0.6720338983050848\n",
            "----------------\n",
            "\n",
            "\n",
            "Deblur: \n",
            "0.639406779661017\n",
            "----------------\n",
            "\n",
            "\n",
            "Fi: \n",
            "0.666843220338983\n",
            "----------------\n",
            "\n",
            "\n",
            "Spr: \n",
            "0.7424465733235077\n",
            "----------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from img2vec_pytorch import Img2Vec\n",
        "\n",
        "def format_image_number(number):\n",
        "    img_no_str = str(number)\n",
        "    \n",
        "    if len(img_no_str) == 1:\n",
        "        im_no_name = \"00000\"\n",
        "    elif len(img_no_str) == 2:\n",
        "        im_no_name = \"0000\"\n",
        "    elif len(img_no_str) == 3:\n",
        "        im_no_name = \"000\"\n",
        "    elif len(img_no_str) == 4:\n",
        "        im_no_name = \"00\"\n",
        "    elif len(img_no_str) == 5:\n",
        "        im_no_name = \"0\"\n",
        "    elif len(img_no_str) == 6:\n",
        "        im_no_name = \"\"\n",
        "        \n",
        "    return im_no_name + img_no_str\n",
        "    \n",
        "img2vec = Img2Vec(cuda=False, model=\"alexnet\")  # default model resnet\n",
        "\n",
        "def get_images_vector(path, number_of_images):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_list = []\n",
        "    \n",
        "    for image_no in range(number_of_images):\n",
        "        image_no_str = str(image_no)\n",
        "        \n",
        "        im_no_name = format_image_number(image_no_str) + \".png\"\n",
        "        \n",
        "        path_to_image = path + im_no_name\n",
        "\n",
        "        feature_vector_image = Image.open(path_to_image).convert('RGB')\n",
        "        feature_vec = img2vec.get_vec(feature_vector_image, tensor=True)\n",
        "        feature_tensor_list.append(feature_vec)\n",
        "        \n",
        "    return feature_tensor_list\n",
        "\n",
        "def obtain_cosine_similarity_model_predictions(number_of_images, refs_vec_list, p0s_rec_list, p1s_rec_list):\n",
        "    cosine_similarity_ref_and_p0 = []\n",
        "    cosine_similarity_ref_and_p1 = []\n",
        "\n",
        "    # list of predictions\n",
        "    cosine_similarity_predictions = []\n",
        "    \n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "    for image_no in range(number_of_images): \n",
        "        reshaped_ref = refs_vec_list[image_no]\n",
        "        reshaped_p0 = p0s_rec_list[image_no]\n",
        "        reshaped_p1 = p1s_rec_list[image_no]\n",
        "        \n",
        "        cosine_similarity_ref_and_p0.append(cos(reshaped_ref, reshaped_p0))\n",
        "        cosine_similarity_ref_and_p1.append(cos(reshaped_ref, reshaped_p1))\n",
        "\n",
        "        if cosine_similarity_ref_and_p0[image_no] >= cosine_similarity_ref_and_p1[image_no]:\n",
        "            cosine_similarity_predictions.append(0)\n",
        "        else:\n",
        "            cosine_similarity_predictions.append(1)   \n",
        "            \n",
        "    return cosine_similarity_predictions\n",
        "\n",
        "def obtain_prediction_accuracy(number_of_images, path_to_decision, actual_predictions):\n",
        "    expected_decision_outputs = []\n",
        "\n",
        "    for image_no in range(number_of_images):\n",
        "            image_no_str = str(image_no)\n",
        "\n",
        "            im_no_name = format_image_number(image_no_str) + \".npy\"\n",
        "            path_to_image = path_to_decision + im_no_name\n",
        "\n",
        "            decision = np.load(path_to_image)\n",
        "\n",
        "            if decision[0] <= 0.5: \n",
        "                expected_decision_outputs.append(0)\n",
        "            else: \n",
        "                expected_decision_outputs.append(1)\n",
        "                \n",
        "    number_wrong_predictions = 0\n",
        "\n",
        "    for image_no in range(number_of_images):\n",
        "        if actual_predictions[image_no] != expected_decision_outputs[image_no]:\n",
        "            number_wrong_predictions += 1\n",
        "\n",
        "    accuracy = (number_of_images - number_wrong_predictions)/(number_of_images)\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def reshape_tensor_list(dim, tensor_list):\n",
        "    reshaped_list = []\n",
        "    for t in tensor_list:\n",
        "        reshaped_t = torch.reshape(t, (1,dim))\n",
        "        reshaped_list.append(t)\n",
        "    return reshaped_list\n",
        "\n",
        "\n",
        "\n",
        "print(\"Alexnet results:\")\n",
        "\n",
        "img2vec_res = Img2Vec(cuda=False, model=\"alexnet\")  # default model resnet\n",
        "\n",
        "traditional_refs = get_images_vector(\"val/traditional/ref/\", 4720)\n",
        "traditional_p0s = get_images_vector(\"val/traditional/p0/\", 4720)\n",
        "traditional_p1s = get_images_vector(\"val/traditional/p1/\", 4720)\n",
        "\n",
        "cnn_refs = get_images_vector(\"val/cnn/ref/\", 4720)\n",
        "cnn_p0s = get_images_vector(\"val/cnn/p0/\", 4720)\n",
        "cnn_p1s = get_images_vector(\"val/cnn/p1/\", 4720)\n",
        "\n",
        "color_refs = get_images_vector(\"val/color/ref/\", 4720)\n",
        "color_p0s = get_images_vector(\"val/color/p0/\", 4720)\n",
        "color_p1s = get_images_vector(\"val/color/p1/\", 4720)\n",
        "\n",
        "deblur_refs = get_images_vector(\"val/deblur/ref/\", 9440)\n",
        "deblur_p0s = get_images_vector(\"val/deblur/p0/\", 9440)\n",
        "deblur_p1s = get_images_vector(\"val/deblur/p1/\", 9440)\n",
        "print(\"Deblurring is complete\")\n",
        "\n",
        "fi_refs = get_images_vector(\"val/frameinterp/ref/\", 1888)\n",
        "fi_p0s = get_images_vector(\"val/frameinterp/p0/\", 1888)\n",
        "fi_p1s = get_images_vector(\"val/frameinterp/p1/\", 1888)\n",
        "print(\"FI is complete\")\n",
        "\n",
        "spr_refs = get_images_vector(\"val/superres/ref/\", 10856)\n",
        "spr_p0s = get_images_vector(\"val/superres/p0/\", 10856)\n",
        "spr_p1s = get_images_vector(\"val/superres/p1/\", 10856)\n",
        "print(\"Spr is complete\")\n",
        "\n",
        "\n",
        "actual_model_predictions = obtain_cosine_similarity_model_predictions(4720, traditional_refs, traditional_p0s, traditional_p1s)\n",
        "prediction_accuracy = obtain_prediction_accuracy(4720, \"val/traditional/judge/\", actual_model_predictions)\n",
        "print(\"Traditional: \")\n",
        "print(prediction_accuracy)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "actual_model_predictions_cnn_distortion = obtain_cosine_similarity_model_predictions(4720, cnn_refs, cnn_p0s, cnn_p1s)\n",
        "prediction_accuracy_cnn_distortion = obtain_prediction_accuracy(4720, \"val/cnn/judge/\", actual_model_predictions_cnn_distortion)\n",
        "print(\"CNN: \")\n",
        "print(prediction_accuracy_cnn_distortion)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "actual_model_predictions_color_distortion = obtain_cosine_similarity_model_predictions(4720, color_refs, color_p0s, color_p1s)\n",
        "prediction_accuracy_color_distortion = obtain_prediction_accuracy(4720, \"val/color/judge/\", actual_model_predictions_color_distortion)\n",
        "print(\"Colorization: \")\n",
        "print(prediction_accuracy_color_distortion)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "actual_model_predictions_blur_distortion = obtain_cosine_similarity_model_predictions(9440, deblur_refs, deblur_p0s, deblur_p1s)\n",
        "prediction_accuracy_blur_distortion = obtain_prediction_accuracy(9440, \"val/deblur/judge/\", actual_model_predictions_blur_distortion)\n",
        "print(\"Deblur: \")\n",
        "print(prediction_accuracy_blur_distortion)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "actual_model_predictions_frame = obtain_cosine_similarity_model_predictions(1888, fi_refs, fi_p0s, fi_p1s)\n",
        "prediction_accuracy_frame = obtain_prediction_accuracy(1888, \"val/frameinterp/judge/\", actual_model_predictions_frame)\n",
        "print(\"Fi: \")\n",
        "print(prediction_accuracy_frame)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "actual_model_predictions_cnn_distortion_super = obtain_cosine_similarity_model_predictions(10856, spr_refs, spr_p0s, spr_p1s)\n",
        "prediction_accuracy_cnn_distortion_super = obtain_prediction_accuracy(10856, \"val/superres/judge/\", actual_model_predictions_cnn_distortion_super)\n",
        "print(\"Spr: \")\n",
        "print(prediction_accuracy_cnn_distortion_super)\n",
        "print(\"----------------\")\n",
        "print(\"\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1abe4f",
      "metadata": {
        "id": "7d1abe4f"
      },
      "source": [
        "AlexNet: \n",
        "    \n",
        "Traditional: \n",
        "0.8108050847457627\n",
        "\n",
        "\n",
        "CNN: \n",
        "0.8544491525423729\n",
        "\n",
        "\n",
        "\n",
        "Colorization: \n",
        "0.6720338983050848\n",
        "\n",
        "\n",
        "\n",
        "Deblur: \n",
        "0.639406779661017\n",
        "\n",
        "\n",
        "\n",
        "Fi: \n",
        "0.666843220338983\n",
        "\n",
        "\n",
        "\n",
        "Spr: \n",
        "0.7424465733235077"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> NEW START HERE </h1>\n",
        "\n",
        "Link: https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c\n",
        " + modify to adhere to current PyTorch updates"
      ],
      "metadata": {
        "id": "Mq-M8hr-PPD3"
      },
      "id": "Mq-M8hr-PPD3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*What we have done here is created a reference to the layer we want to extract from. Deciding on which layer to extract from is a bit of a science, but something to keep in mind is that early layers in the network are usually learning high-level features such as ‘image contains fur’ or ‘image contains round object’, while lower-level features are more specific to the training data. The ‘avgpool’ layer selected here is at the end of ResNet-18, but if you plan to use images that are very different from ImageNet, you may benefit in using an ealier layer or fine-tuning the model.*\n",
        "\n",
        "Conclusion for me: because later layers are more specific to training data THEN I am expecting all models to perform much better on HSJ and the worst on Distortions.\n",
        "\n",
        "\n",
        "**But if you plan to use images that are very different from ImageNet, you may benefit in using an ealier layer or fine-tuning the model.**"
      ],
      "metadata": {
        "id": "Q04PUL8MP15O"
      },
      "id": "Q04PUL8MP15O"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "HB2oe9hIPOqW"
      },
      "id": "HB2oe9hIPOqW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.cuda()\n",
        "# Use the model object to select the desired layer\n",
        "layer = model._modules.get('avgpool')\n",
        "# Set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBVpBNLPPbdI",
        "outputId": "7e897c3d-bffd-4ba7-a885-52e1c7fcfd78"
      },
      "id": "rBVpBNLPPbdI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeDECMcrY0dz"
      },
      "id": "zeDECMcrY0dz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "VX14Yl0tPgou"
      },
      "id": "VX14Yl0tPgou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()   # used to convert the PIL image to a PyTorch tensor (multidimensional array)"
      ],
      "metadata": {
        "id": "B-hulT0xQa5z"
      },
      "id": "B-hulT0xQa5z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(image_name):\n",
        "    \n",
        "#     # Load the pretrained model\n",
        "#     model = models.resnet18(pretrained=True)\n",
        "#     # Use the model object to select the desired layer\n",
        "#     layer = model._modules.get('avgpool')\n",
        "    \n",
        "    # 1. Load the image with Pillow library\n",
        "    img = Image.open(image_name).convert('RGB')\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\n",
        "    #    The 'avgpool' layer has an output size of 512\n",
        "    \n",
        "    # M1: my_embedding = torch.zeros(1, 512, 1, 1) and later my_embedding.copy_(o.data)\n",
        "    # M2: my_embedding = torch.zeros(512) and later my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    my_embedding = torch.zeros(512).cuda()\n",
        "    \n",
        "    # 4. Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "        my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    # 5. Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # 6. Run the model on our transformed image\n",
        "    model(t_img)\n",
        "    # 7. Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # 8. Return the feature vector\n",
        "    return my_embedding"
      ],
      "metadata": {
        "id": "WVhuqcJzQvYq"
      },
      "id": "WVhuqcJzQvYq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = get_vector(\"ref.png\")\n",
        "ref1_vector = get_vector(\"p0.png\")\n",
        "ref2_vector = get_vector(\"p1.png\")"
      ],
      "metadata": {
        "id": "7uzFV2c5Rn5_"
      },
      "id": "7uzFV2c5Rn5_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using PyTorch Cosine Similarity\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "cos_sim = cos(query_vector.unsqueeze(0),\n",
        "              ref1_vector.unsqueeze(0))\n",
        "print('\\nCosine similarity: {0}\\n'.format(cos_sim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaZmhKNnYZ3z",
        "outputId": "9729938c-1ff9-4ee0-fa92-0a57752b2c3f"
      },
      "id": "gaZmhKNnYZ3z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cosine similarity: tensor([0.8143], device='cuda:0')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using PyTorch Cosine Similarity\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "cos_sim = cos(query_vector.unsqueeze(0),\n",
        "              ref2_vector.unsqueeze(0))\n",
        "print('\\nCosine similarity: {0}\\n'.format(cos_sim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrMsaD6PYhpr",
        "outputId": "d27f9f43-d5a5-430d-8548-95b6e56d8ebd"
      },
      "id": "FrMsaD6PYhpr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cosine similarity: tensor([0.5251], device='cuda:0')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(path, image_type):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector(path+\"/p0/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector(path+\"/p1/\" + im_no_name))\n",
        "\n",
        "        # now load decision\n",
        "        decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "        decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "        if decision[0] <= 0.5: \n",
        "              feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "              feature_tensor_dict[\"decision\"].append(1)\n",
        "        \n",
        "    return feature_tensor_dict"
      ],
      "metadata": {
        "id": "q3nySb9GaiUH"
      },
      "id": "q3nySb9GaiUH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu9hP7FRc-Ln",
        "outputId": "9eb4ea8b-0827-46c5-c49c-5de07cd1afcf"
      },
      "id": "Eu9hP7FRc-Ln",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_18_embedddings = get_image_embeddings(\"datasets/distortions/distortion_triplets/traditional_triplets\", \"png\")"
      ],
      "metadata": {
        "id": "Cs_f5-mydPO9"
      },
      "id": "Cs_f5-mydPO9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_18_embedddings[\"decision\"]"
      ],
      "metadata": {
        "id": "DK7DPu2RnkQ2"
      },
      "id": "DK7DPu2RnkQ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (1000 - number_wrong_predictions)/(1000)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "V6S5kBKBjo-l"
      },
      "id": "V6S5kBKBjo-l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = get_predictions(resnet_18_embedddings)\n",
        "accuracy = get_accuracy(predictions, resnet_18_embedddings)"
      ],
      "metadata": {
        "id": "Wt9bjXlSmkIH"
      },
      "id": "Wt9bjXlSmkIH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcbOFa1EoGFD",
        "outputId": "9d2ff7d7-ac18-4895-c83d-6bb5db1fcc40"
      },
      "id": "KcbOFa1EoGFD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.809"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Loading dataset and drive </h1>"
      ],
      "metadata": {
        "id": "zIgDgfIQxc5h"
      },
      "id": "zIgDgfIQxc5h"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install img2vec-pytorch"
      ],
      "metadata": {
        "id": "2qDJIombxf-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fd392a-5673-4dc8-85e4-98affb13c735"
      },
      "id": "2qDJIombxf-I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting img2vec-pytorch\n",
            "  Downloading img2vec_pytorch-1.0.1-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (0.14.1+cu116)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->img2vec-pytorch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->img2vec-pytorch) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->img2vec-pytorch) (8.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (3.4)\n",
            "Installing collected packages: img2vec-pytorch\n",
            "Successfully installed img2vec-pytorch-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/distortion_triplets.zip\" -d \"/content/datasets/distortions\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/hsj_triplets.zip\" -d \"/content/datasets/hsj\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/birds_dataset_triplets.zip\" -d \"/content/datasets/birds-16\""
      ],
      "metadata": {
        "id": "D5OMjLX7dxhD"
      },
      "id": "D5OMjLX7dxhD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Working framework start </h1>"
      ],
      "metadata": {
        "id": "M1x9m7iwoV6B"
      },
      "id": "M1x9m7iwoV6B"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "############################ REQUIRES MODIFICATION FOR EACH MODEL \n",
        "\n",
        "# scaler = transforms.Resize((224, 224))\n",
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                  std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()   # used to convert the PIL image to a PyTorch tensor (multidimensional array)\n",
        "############################\n",
        "\n",
        "\n",
        "############################## HOW TO GET MODEL:\n",
        "# # Load the pretrained model\n",
        "# model = models.resnet18(pretrained=True)\n",
        "# model.cuda()\n",
        "# # Use the model object to select the desired layer\n",
        "# layer = model._modules.get('avgpool')\n",
        "# # Set model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "##################################\n",
        "\n",
        "def get_vector(image_name, model, layer, scaler, normalize, feature_tensor_size):\n",
        "    \n",
        "#     # Load the pretrained model\n",
        "#     model = models.resnet18(pretrained=True)\n",
        "#     # Use the model object to select the desired layer\n",
        "#     layer = model._modules.get('avgpool')\n",
        "    \n",
        "    # 1. Load the image with Pillow library\n",
        "    img = Image.open(image_name).convert('RGB')\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\n",
        "    #    The 'avgpool' layer has an output size of 512\n",
        "    \n",
        "    # M1: my_embedding = torch.zeros(1, 512, 1, 1) and later my_embedding.copy_(o.data)\n",
        "    # M2: my_embedding = torch.zeros(512) and later my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    my_embedding = torch.zeros(feature_tensor_size).cuda()\n",
        "    \n",
        "    # 4. Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "        my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    # 5. Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # 6. Run the model on our transformed image\n",
        "    model(t_img)\n",
        "    # 7. Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # 8. Return the feature vector\n",
        "    return my_embedding\n",
        "    \n",
        "\n",
        "############################ COSINE SIMILARITY\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "########################### GET FEATURE VECTORS\n",
        "def get_image_embeddings(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector(path+\"/p0/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector(path+\"/p1/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "\n",
        "        # now load decision\n",
        "        if isHSJOrBirds:\n",
        "          feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "          decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "          decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "          if decision[0] <= 0.5: \n",
        "                feature_tensor_dict[\"decision\"].append(0)\n",
        "          else: \n",
        "                feature_tensor_dict[\"decision\"].append(1)\n",
        "        \n",
        "    return feature_tensor_dict\n",
        "\n",
        "\n",
        "########################## GET MODEL PREDICTIONS\n",
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "########################## GET MODEL ACCURACY\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (1000 - number_wrong_predictions)/(1000)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def apply_framework(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):\n",
        "  embeddings = get_image_embeddings(path, \n",
        "                                    image_type,\n",
        "                                    model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\n",
        "  predictions= get_predictions(embeddings)\n",
        "  accuracy = get_accuracy(predictions, embeddings)\n",
        "  print(path + \": \" + str(accuracy))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "7D9166cvobDh"
      },
      "id": "7D9166cvobDh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 1. Load the image with Pillow library\n",
        "img = Image.open(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar/p0/000000.jpg\").convert('RGB')\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "scaler = transforms.Resize((224, 224))\n",
        "t_img = scaler(img)\n",
        "t_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "PtspBYvyumJR",
        "outputId": "435e16a7-eb2b-42d3-8527-833c679dd024"
      },
      "id": "PtspBYvyumJR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F12235E32E0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Resnet-18 + cosine similarity\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.809\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.848\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.623\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.603\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.663\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.723"
      ],
      "metadata": {
        "id": "-pJ_BUwHp5uU"
      },
      "id": "-pJ_BUwHp5uU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.cuda()\n",
        "# Use the model object to select the desired layer\n",
        "layer = model._modules.get('avgpool')\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "print(\"# ----------------------------------------BAPPS 2AFC DIST RESNET-18------------------------#\")\n",
        "print(\"\")\n",
        "################### TRADITIONAL \n",
        "apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                \"png\",\n",
        "                model, layer, scaler, normalize, False)\n",
        "\n",
        "################ CNN\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                \"png\",\n",
        "                model, layer, scaler, normalize, False)\n",
        "\n",
        "############### COLOR\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                \"png\",\n",
        "                model, layer, scaler, normalize, False)\n",
        "\n",
        "################ DEBLUR\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                \"png\",\n",
        "                model, layer, scaler, normalize, False)\n",
        "\n",
        "############### FRAME INTERP\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                \"png\",\n",
        "                model, layer, scaler, normalize, False)\n",
        "\n",
        "################ SUPER RES\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                \"png\",\n",
        "                model, layer, scaler, normalize, False)\n",
        "\n",
        "# datasets/distortions/distortion_triplets/traditional_triplets: 0.809\n",
        "# datasets/distortions/distortion_triplets/cnn_triplets: 0.848\n",
        "# datasets/distortions/distortion_triplets/color_triplets: 0.623\n",
        "# datasets/distortions/distortion_triplets/deblur_triplets: 0.603\n",
        "# datasets/distortions/distortion_triplets/frameinterp_triplets: 0.663\n",
        "# datasets/distortions/distortion_triplets/superres_triplets: 0.723"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do-kyNnap4wa",
        "outputId": "a6f3923b-851b-4fff-e015-34cd393e5398"
      },
      "id": "do-kyNnap4wa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# ----------------------------------------BAPPS 2AFC DIST RESNET-18------------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.809\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.848\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.623\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.603\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.663\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.723\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.723"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BAPPS 2AFC DIST RESNET-18+Cosine Similarity\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.809\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.848\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.623\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.603\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.663\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.723\n",
        "\n",
        "\n",
        "----------------------------------------HSJ------------------------\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.695\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.597\n",
        "\n",
        "----------------------------------------Birds-16------------------------\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.791\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.599\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.786"
      ],
      "metadata": {
        "id": "_uDcYjVTeW1p"
      },
      "id": "_uDcYjVTeW1p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.cuda()\n",
        "# Use the model object to select the desired layer\n",
        "layer = model._modules.get('avgpool')\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "print(\"\")\n",
        "################### HSJ Rank 0 + Dissimilar \n",
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                \"jpg\",\n",
        "                model, layer, scaler, normalize, True)\n",
        "\n",
        "################ HSJ Rank 0 + Rank 1\n",
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                \"jpg\",\n",
        "                model, layer, scaler, normalize, True)\n",
        "\n",
        "print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "print(\"\")\n",
        "################### Birds-16 Rank 0 + Dissimilar \n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                \"jpg\",\n",
        "                model, layer, scaler, normalize, True)\n",
        "\n",
        "################### Birds-16 Rank 0 + Rank 1 \n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                \"jpg\",\n",
        "                model, layer, scaler, normalize, True)\n",
        "\n",
        "################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                \"jpg\",\n",
        "                model, layer, scaler, normalize, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFKC2UV0togy",
        "outputId": "87a117cf-fc38-4b2f-ec03-ff9718e17008"
      },
      "id": "iFKC2UV0togy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.695\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.597\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.791\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.599\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.786\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.786"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model2 = models.resnet34(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "bca6d128c81c4b1596e23411c8939c43",
            "f3e1a8617cb748a39f282c11b9491e0a",
            "d3e31a9020364e64be5c1ee8950e48b3",
            "7548b65877f94225ae7889578eab5db2",
            "75708c28b4694cee90680050814f3aa4",
            "893c1dca55e44bcaa5e35d8c22fa8636",
            "5372c90c9580459ab96dc596ac1361d5",
            "0f7d3090a62545049b4ebc69e7fd5f21",
            "fdd5022ec0ab422db94c6392eb0202f1",
            "009229f9541b4e25b6aa5b8a9724865c",
            "71eaa211a3a9407083de38c547cc3305"
          ]
        },
        "id": "0mIL7Zw4j9T5",
        "outputId": "671418bd-f591-4cab-9ef0-8c5c71caf5fc"
      },
      "id": "0mIL7Zw4j9T5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bca6d128c81c4b1596e23411c8939c43"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Resnet-34\n",
        "\n",
        "----------------------------------------BAPPS 2AFC DIST RESNET-18------------------------\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.801\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.851\n",
        "\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.626\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.601\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.652\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.725\n",
        "\n",
        "----------------------------------------HSJ------------------------\n",
        "\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.691\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.574\n",
        "\n",
        " ----------------------------------------Birds-16------------------------\n",
        "\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.775\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.611\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.\n",
        "753"
      ],
      "metadata": {
        "id": "yoN4xJgekSIK"
      },
      "id": "yoN4xJgekSIK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model2 = models.resnet34(pretrained=True)\n",
        "model2.cuda()\n",
        "# Use the model object to select the desired layer\n",
        "layer = model2._modules.get('avgpool')\n",
        "# Set model to evaluation mode\n",
        "model2.eval()\n",
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "print(\"\")\n",
        "print(\"# ----------------------------------------BAPPS 2AFC DIST-----------------------#\")\n",
        "print(\"\")\n",
        "################### TRADITIONAL \n",
        "apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                \"png\",\n",
        "                model2, layer, scaler, normalize, False)\n",
        "\n",
        "################ CNN\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                \"png\",\n",
        "                model2, layer, scaler, normalize, False)\n",
        "\n",
        "############### COLOR\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                \"png\",\n",
        "                model2, layer, scaler, normalize, False)\n",
        "\n",
        "################ DEBLUR\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                \"png\",\n",
        "                model2, layer, scaler, normalize, False)\n",
        "\n",
        "############### FRAME INTERP\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                \"png\",\n",
        "                model2, layer, scaler, normalize, False)\n",
        "\n",
        "################ SUPER RES\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                \"png\",\n",
        "                model2, layer, scaler, normalize, False)\n",
        "\n",
        "print(\"\")\n",
        "print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "print(\"\")\n",
        "################### HSJ Rank 0 + Dissimilar \n",
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                \"jpg\",\n",
        "                model2, layer, scaler, normalize, True)\n",
        "\n",
        "################ HSJ Rank 0 + Rank 1\n",
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                \"jpg\",\n",
        "                model2, layer, scaler, normalize, True)\n",
        "\n",
        "print(\"\")\n",
        "print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "print(\"\")\n",
        "################### Birds-16 Rank 0 + Dissimilar \n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                \"jpg\",\n",
        "                model2, layer, scaler, normalize, True)\n",
        "\n",
        "################### Birds-16 Rank 0 + Rank 1 \n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                \"jpg\",\n",
        "                model2, layer, scaler, normalize, True)\n",
        "\n",
        "################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                \"jpg\",\n",
        "                model2, layer, scaler, normalize, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdD6QgFzi-vC",
        "outputId": "0a6b0292-1660-4871-d22d-f780fee1c0e7"
      },
      "id": "ZdD6QgFzi-vC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST RESNET-18------------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.801\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.851\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.626\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.601\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.652\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.725\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.691\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.574\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.775\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.611\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.753\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.753"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ResNet 50\n",
        "\n",
        " ----------------------------------------BAPPS 2AFC DIST-----------------------\n",
        "\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.81\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.837\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.635\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.608\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.653\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.721\n",
        "\n",
        " ----------------------------------------HSJ------------------------\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.687\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.588\n",
        "\n",
        "\n",
        "----------------------------------------Birds-16------------------------\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.755\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.605\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.755\n"
      ],
      "metadata": {
        "id": "XtMXz0qZm6cv"
      },
      "id": "XtMXz0qZm6cv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model3 = models.resnet50(pretrained=True)\n",
        "model3.cuda()\n",
        "# Use the model object to select the desired layer\n",
        "layer = model3._modules.get('avgpool')\n",
        "# Set model to evaluation mode\n",
        "model3.eval()\n",
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "print(\"\")\n",
        "print(\"# ----------------------------------------BAPPS 2AFC DIST-----------------------#\")\n",
        "print(\"\")\n",
        "################### TRADITIONAL \n",
        "apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                \"png\",\n",
        "                model3, layer, scaler, normalize, False)\n",
        "\n",
        "################ CNN\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                \"png\",\n",
        "                model3, layer, scaler, normalize, False)\n",
        "\n",
        "############### COLOR\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                \"png\",\n",
        "                model3, layer, scaler, normalize, False)\n",
        "\n",
        "################ DEBLUR\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                \"png\",\n",
        "                model3, layer, scaler, normalize, False)\n",
        "\n",
        "############### FRAME INTERP\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                \"png\",\n",
        "                model3, layer, scaler, normalize, False)\n",
        "\n",
        "################ SUPER RES\n",
        "apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                \"png\",\n",
        "                model3, layer, scaler, normalize, False)\n",
        "\n",
        "print(\"\")\n",
        "print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "print(\"\")\n",
        "################### HSJ Rank 0 + Dissimilar \n",
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                \"jpg\",\n",
        "                model3, layer, scaler, normalize, True)\n",
        "\n",
        "################ HSJ Rank 0 + Rank 1\n",
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                \"jpg\",\n",
        "                model3, layer, scaler, normalize, True)\n",
        "\n",
        "print(\"\")\n",
        "print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "print(\"\")\n",
        "################### Birds-16 Rank 0 + Dissimilar \n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                \"jpg\",\n",
        "                model3, layer, scaler, normalize, True)\n",
        "\n",
        "################### Birds-16 Rank 0 + Rank 1 \n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                \"jpg\",\n",
        "                model3, layer, scaler, normalize, True)\n",
        "\n",
        "################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                \"jpg\",\n",
        "                model3, layer, scaler, normalize, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clXURUNolyLI",
        "outputId": "9cad0835-a009-4ea2-9ae0-dff6d53fed9e"
      },
      "id": "clXURUNolyLI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.81\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.837\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.635\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.608\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.653\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.721\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.687\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.588\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.755\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.605\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.755\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.755"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New - even less lines of code"
      ],
      "metadata": {
        "id": "4vd4hLtDpm8d"
      },
      "id": "4vd4hLtDpm8d"
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_framework_model(modelf):\n",
        "  modelf.cuda()\n",
        "  # Use the model object to select the desired layer\n",
        "  layer = modelf._modules.get('avgpool')\n",
        "  # Set model to evaluation mode\n",
        "  modelf.eval()\n",
        "  scaler = transforms.Resize((224, 224))\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------BAPPS 2AFC DIST-----------------------#\")\n",
        "  print(\"\")\n",
        "  ################### TRADITIONAL \n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False)\n",
        "\n",
        "  ################ CNN\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False)\n",
        "\n",
        "  ############### COLOR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False)\n",
        "\n",
        "  ################ DEBLUR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False)\n",
        "\n",
        "  ############### FRAME INTERP\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                  \"png\",\n",
        "                  model3, layer, scaler, normalize, False)\n",
        "\n",
        "  ################ SUPER RES\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### HSJ Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True)\n",
        "\n",
        "  ################ HSJ Rank 0 + Rank 1\n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### Birds-16 Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Rank 1 \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True)"
      ],
      "metadata": {
        "id": "tWRB-LfBpmBO"
      },
      "id": "tWRB-LfBpmBO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet 101"
      ],
      "metadata": {
        "id": "45EDIZHyqGBG"
      },
      "id": "45EDIZHyqGBG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model5 = models.resnet101(pretrained=True)\n",
        "apply_framework_model(model5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "6678c3284bba4788bbe9d4e45414ffc8",
            "0bb4ca5b79eb47cba9040c4866621628",
            "0dca7d41c2c6492d94e48aa4ddac2b39",
            "cffc167a23f346938726878ea7968396",
            "ce4cd73fae2c4ca6ac365353239ad9fe",
            "ff89d302c7b447b7865767d11316ab95",
            "dec2a15e52af45b8aef578feec6d82c6",
            "0c88bfb26f94466f8ce31588b5e6faff",
            "e2e0448838f24045bb09baa6a82f7d60",
            "9c786b0d9c3a4f908b4bef29511cdf54",
            "229cdc96df7c40029253c5e17f6edf74"
          ]
        },
        "id": "8lJsSoKbpjzi",
        "outputId": "a45c142e-6da9-4885-cb36-3bcd902fe796"
      },
      "id": "8lJsSoKbpjzi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6678c3284bba4788bbe9d4e45414ffc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.808\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.839\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.647\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.594\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.736\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.685\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.595\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.771\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.621\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model6 = models.resnet152(pretrained=True)\n",
        "apply_framework_model(model6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "29ec99b1c78842238a5f1e9a1f7bd471",
            "d1a3ed3a66f5412b908e14b58b920c67",
            "053f280125b1431aba72492658913b36",
            "4561cbe2c1214254a8d670b9f31caed1",
            "1dac5859a10945a1adadb28dfc93ea0e",
            "18f2ceb4c94c4ea091afc17ab882936f",
            "48d0319c81204308a655424e9d974f41",
            "917e6cbf826c4da399841399330f9dd9",
            "92a20835c1ab41af8eee379d345dc6e0",
            "d97d9fc3cfe04e60aebd1723a4cdf263",
            "516db9148baa45f485cbcca81248a1f1"
          ]
        },
        "id": "ipWjsF1EuDrT",
        "outputId": "9066d80c-db16-4500-d52a-8eba83bfd441"
      },
      "id": "ipWjsF1EuDrT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/230M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29ec99b1c78842238a5f1e9a1f7bd471"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.815\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.845\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.637\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.624\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.713\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.68\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.588\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.779\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.605\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling different layers of ResNet - Editting Framework"
      ],
      "metadata": {
        "id": "nqOi5dAb0tiw"
      },
      "id": "nqOi5dAb0tiw"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "############################ REQUIRES MODIFICATION FOR EACH MODEL \n",
        "\n",
        "# scaler = transforms.Resize((224, 224))\n",
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                  std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()   # used to convert the PIL image to a PyTorch tensor (multidimensional array)\n",
        "############################\n",
        "\n",
        "\n",
        "############################## HOW TO GET MODEL:\n",
        "# # Load the pretrained model\n",
        "# model = models.resnet18(pretrained=True)\n",
        "# model.cuda()\n",
        "# # Use the model object to select the desired layer\n",
        "# layer = model._modules.get('avgpool')\n",
        "# # Set model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "##################################\n",
        "\n",
        "def get_vector(image_name, model, layer, scaler, normalize, feature_tensor_size):\n",
        "    \n",
        "#     # Load the pretrained model\n",
        "#     model = models.resnet18(pretrained=True)\n",
        "#     # Use the model object to select the desired layer\n",
        "#     layer = model._modules.get('avgpool')\n",
        "    \n",
        "    # 1. Load the image with Pillow library\n",
        "    img = Image.open(image_name).convert('RGB')\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\n",
        "    #    The 'avgpool' layer has an output size of 512\n",
        "    \n",
        "    # M1: my_embedding = torch.zeros(1, 512, 1, 1) and later my_embedding.copy_(o.data)\n",
        "    # M2: my_embedding = torch.zeros(512) and later my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    my_embedding = torch.zeros(feature_tensor_size).cuda()\n",
        "    \n",
        "    # 4. Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "        my_embedding.copy_(o.data)\n",
        "    # 5. Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # 6. Run the model on our transformed image\n",
        "    model(t_img)\n",
        "    # 7. Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # 8. Return the feature vector\n",
        "    return torch.flatten(my_embedding)\n",
        "\n",
        "############################ COSINE SIMILARITY\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "########################### GET FEATURE VECTORS\n",
        "def get_image_embeddings(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector(path+\"/p0/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector(path+\"/p1/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "\n",
        "        # now load decision\n",
        "        if isHSJOrBirds:\n",
        "          feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "          decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "          decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "          if decision[0] <= 0.5: \n",
        "                feature_tensor_dict[\"decision\"].append(0)\n",
        "          else: \n",
        "                feature_tensor_dict[\"decision\"].append(1)\n",
        "        \n",
        "    return feature_tensor_dict\n",
        "\n",
        "\n",
        "########################## GET MODEL PREDICTIONS\n",
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "########################## GET MODEL ACCURACY\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (1000 - number_wrong_predictions)/(1000)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def apply_framework(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):\n",
        "  embeddings = get_image_embeddings(path, \n",
        "                                    image_type,\n",
        "                                    model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\n",
        "  predictions= get_predictions(embeddings)\n",
        "  accuracy = get_accuracy(predictions, embeddings)\n",
        "  print(path + \": \" + str(accuracy))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "1uJdDnzS0s7O"
      },
      "id": "1uJdDnzS0s7O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_framework_model(modelf, layer, feature_tensor_size):\n",
        "  scaler = transforms.Resize((224, 224))\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------BAPPS 2AFC DIST-----------------------#\")\n",
        "  print(\"\")\n",
        "  ################### TRADITIONAL \n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ CNN\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ############### COLOR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ DEBLUR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ############### FRAME INTERP\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ SUPER RES\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### HSJ Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################ HSJ Rank 0 + Rank 1\n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### Birds-16 Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Rank 1 \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)"
      ],
      "metadata": {
        "id": "j3Bbhfqy1gJP"
      },
      "id": "j3Bbhfqy1gJP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#-------------------------------Resnet-18-------------------------------------#\n",
        "#-------------------------------Layer 2 - conv_3x#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.629\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.828\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.525\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.608\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.682\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.634\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.567\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.604\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.541\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.607\n",
        "#-------------------------------Layer 3 - conv_4x#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.725\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.84\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.554\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.617\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.731\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.648\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.603\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.592\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.56\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.643\n",
        "#-------------------------------Layer 4 - conv_5x#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.803\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.854\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.607\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.613\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.727\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.664\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.593\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.772\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.605\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.753\n",
        "\n",
        "#-------------------------------Resnet-34-------------------------------------#\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "#-------------------------------Layer 2 - conv_3x#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.636\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.824\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.524\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.605\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.651\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.601\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.57\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.59\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.552\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.583\n",
        "#-------------------------------Layer 3 - conv_4x#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.712\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.841\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.552\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.629\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.74\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.626\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.589\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.583\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.553\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.639\n",
        "#-------------------------------Layer 4 - conv_5x#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.803\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.841\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.624\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.626\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.464\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.724\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.691\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.591\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.764\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.607\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.748\n"
      ],
      "metadata": {
        "id": "a06RF_CqO8Qf"
      },
      "id": "a06RF_CqO8Qf"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#-------------------------------Resnet-18-------------------------------------#\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "print(\"#-------------------------------Layer 2 - conv_3x#\")\n",
        "layer = model._modules.get(\"layer2\")\n",
        "apply_framework_model(model,layer,[1, 128, 28, 28])\n",
        "\n",
        "print(\"#-------------------------------Layer 3 - conv_4x#\")\n",
        "layer = model._modules.get(\"layer3\")\n",
        "apply_framework_model(model,layer,[1, 256, 14, 14])\n",
        "\n",
        "print(\"#-------------------------------Layer 4 - conv_5x#\")\n",
        "layer = model._modules.get(\"layer4\")\n",
        "apply_framework_model(model,layer,[1, 512, 7, 7])\n",
        "\n",
        "print(\"\")\n",
        "print(\"#-------------------------------Resnet-34-------------------------------------#\")\n",
        "model = models.resnet34(pretrained=True)\n",
        "print(\"#-------------------------------Layer 2 - conv_3x#\")\n",
        "layer = model._modules.get(\"layer2\")\n",
        "apply_framework_model(model,layer,[1, 128, 28, 28])\n",
        "\n",
        "print(\"#-------------------------------Layer 3 - conv_4x#\")\n",
        "layer = model._modules.get(\"layer3\")\n",
        "apply_framework_model(model,layer,[1, 256, 14, 14])\n",
        "\n",
        "print(\"#-------------------------------Layer 4 - conv_5x#\")\n",
        "layer = model._modules.get(\"layer4\")\n",
        "apply_framework_model(model,layer,[1, 512, 7, 7])\n",
        "\n",
        "print(\"\")\n",
        "print(\"#-------------------------------Resnet-50-------------------------------------#\")\n",
        "model = models.resnet50(pretrained=True)\n",
        "print(\"#-------------------------------Layer 2 - conv_3x#\")\n",
        "layer = model._modules.get(\"layer2\")\n",
        "apply_framework_model(model,layer,[1, 512, 28, 28])\n",
        "\n",
        "print(\"#-------------------------------Layer 3 - conv_4x#\")\n",
        "layer = model._modules.get(\"layer3\")\n",
        "apply_framework_model(model,layer,[1, 1024, 14, 14])\n",
        "\n",
        "print(\"#-------------------------------Layer 4 - conv_5x#\")\n",
        "layer = model._modules.get(\"layer4\")\n",
        "apply_framework_model(model,layer,[1, 2048, 7, 7])"
      ],
      "metadata": {
        "id": "WLFyG2D51kUr"
      },
      "id": "WLFyG2D51kUr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alexnet\n",
        "print(\"alexnet\")\n",
        "model = models.alexnet(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 256, 6, 6])\n",
        "\n",
        "\n",
        "# mobilenet\n",
        "print(\"mobilenet v 2\")\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"features\")\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 1280, 7, 7])\n",
        "\n",
        "print(\"mobilenet v 3\")\n",
        "\n",
        "model = models.mobilenet_v3_large(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"features\")\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 960, 7, 7])\n",
        "\n",
        "# # vgg\n",
        "\n",
        "# model = models.mobilenet_v2(pretrained=True)\n",
        "# model.cuda()\n",
        "# layer = model._modules.get(\"features\")\n",
        "# model.eval()\n",
        "# apply_framework_model(model,layer,[1, 1280, 7, 7])\n",
        "\n",
        "\n",
        "# ConvNext --> MOST RECENTLY INTRODUCED\n",
        "print(\"convnext\")\n",
        "model = models.convnext_base(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 1024, 1, 1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJcgVogGLcue",
        "outputId": "5f0518cc-76d8-43b2-cb61-d214c9a4589c"
      },
      "id": "vJcgVogGLcue",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alexnet\n",
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.813\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.877\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.637\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.651\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.678\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.764\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.728\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.603\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.72\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.606\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.704\n",
            "mobilenet v 2\n",
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.795\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.861\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.607\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.626\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.65\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.751\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.661\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.581\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.745\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.58\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.743\n",
            "mobilenet v 3\n",
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.775\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.85\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.588\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.62\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.641\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.72\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.621\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.561\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.704\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.602\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.707\n",
            "convnext\n",
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.782\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.851\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.625\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.612\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.649\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.695\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.601\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.569\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.727\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.59\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet 18-34:\n",
        "\n",
        "Layer2: ([1, 128, 28, 28])\n",
        "\n",
        "Layer3: ([1, 256, 14, 14])\n",
        "\n",
        "Layer4: ([1, 512, 7, 7])\n",
        "\n",
        "ResNet 50: \n",
        "\n",
        "Layer2: ([1, 512, 28, 28])\n",
        "\n",
        "Layer3: ([1, 1024, 14, 14])\n",
        "\n",
        "Layer4: ([1, 2048, 7, 7])"
      ],
      "metadata": {
        "id": "GF4H6PjZB9Kh"
      },
      "id": "GF4H6PjZB9Kh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Different Architectures </h1>"
      ],
      "metadata": {
        "id": "sy2BI9hPQbY6"
      },
      "id": "sy2BI9hPQbY6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. EfficientNet B0 [1, 1280, 1, 1]\n",
        "2. EfficientNet B1 [1, 1280, 1, 1]\n",
        "3. EfficientNet B2 [1, 1408, 1, 1]\n",
        "4. EfficientNet B3 [1, 1536, 1, 1]\n",
        "5. EfficientNet B4 [1, 1792, 1, 1]\n",
        "6. EfficientNet B5 [1, 2048, 1, 1]\n",
        "7. EfficientNet B6 [1, 2304, 1, 1]\n",
        "7. EfficientNet B7 [1, 2560, 1, 1]\n",
        "\n",
        "\n",
        "#-----------EfficientNet B0------------------------#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
        "  warnings.warn(\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.785\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.83\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.624\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.622\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.664\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.704\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.583\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.542\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.813\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.641\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.813\n",
        "#-----------EfficientNet B1------------------------#\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.801\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.849\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.622\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.631\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.651\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.713\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.577\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.541\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.806\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.626\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.815\n",
        "#-----------EfficientNet B2------------------------#\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.787\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.836\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.615\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.601\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.667\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.701\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.568\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.574\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.799\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.619\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.805\n",
        "#-----------EfficientNet B3------------------------#\n"
      ],
      "metadata": {
        "id": "sBvIEMcrQgM1"
      },
      "id": "sBvIEMcrQgM1"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#-----------EfficientNet B0------------------------#\")\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "apply_framework_model(model,layer,[1, 1280, 1, 1])\n",
        "\n",
        "print(\"#-----------EfficientNet B1------------------------#\")\n",
        "model = models.efficientnet_b1(pretrained=True)\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "apply_framework_model(model,layer,[1, 1280, 1, 1])\n",
        "\n",
        "print(\"#-----------EfficientNet B2------------------------#\")\n",
        "model = models.efficientnet_b2(pretrained=True)\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "apply_framework_model(model,layer,[1, 1408, 1, 1])"
      ],
      "metadata": {
        "id": "StWBa1TaQaM7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3d84fa3-2cfd-4b69-b14f-ac33c89964d8"
      },
      "id": "StWBa1TaQaM7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#-----------EfficientNet B0------------------------#\n",
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.785\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.83\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.624\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.622\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.664\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.704\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.583\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.542\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.813\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.641\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.813\n",
            "#-----------EfficientNet B1------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.801\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.849\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.622\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.631\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.651\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.713\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.577\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.541\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.806\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.626\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.815\n",
            "#-----------EfficientNet B2------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.787\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.836\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.615\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.601\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.667\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.701\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.568\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.574\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.799\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.619\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.805\n",
            "#-----------EfficientNet B3------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.77\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c0a9cdba86fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefficientnet_b3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avgpool\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mapply_framework_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#-----------EfficientNet B4------------------------#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a7d1a163575f>\u001b[0m in \u001b[0;36mapply_framework_model\u001b[0;34m(modelf, layer, feature_tensor_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m################ CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n\u001b[0m\u001b[1;32m     20\u001b[0m                   \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                   modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
            "\u001b[0;32m<ipython-input-19-d6c36dd30e7d>\u001b[0m in \u001b[0;36mapply_framework\u001b[0;34m(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misHSJOrBirds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m   embeddings = get_image_embeddings(path, \n\u001b[0m\u001b[1;32m    130\u001b[0m                                     \u001b[0mimage_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                                     model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\n",
            "\u001b[0;32m<ipython-input-19-d6c36dd30e7d>\u001b[0m in \u001b[0;36mget_image_embeddings\u001b[0;34m(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mim_no_name\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"0\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_no\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/ref/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/p0/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/p1/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d6c36dd30e7d>\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(image_name, model, layer, scaler, normalize, feature_tensor_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# 6. Run the model on our transformed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# 7. Detach our copy function from the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mbn_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mbn_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         r\"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#-----------EfficientNet B3------------------------#\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.77\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.83\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.569\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.595\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.659\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.665\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.635\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.569\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.796\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.635\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.816\n",
        "#-----------EfficientNet B4------------------------#\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.737\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.812\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.605\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.601\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.632\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.657\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.628\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.58\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.77\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.61\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.779\n",
        "#-----------EfficientNet B5------------------------#\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B5_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B5_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.763\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.812\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.571\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.6\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.624\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.645\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.57\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.558\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.786\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.608\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.797\n",
        "#-----------EfficientNet B6------------------------#\n",
        "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B6_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B6_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "\n",
        "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.767\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.818\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.569\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.593\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.637\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.582\n",
        "\n",
        "# ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.576\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.553\n",
        "\n",
        "# ----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.772\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.615\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.798"
      ],
      "metadata": {
        "id": "POOzpdTpQeTj"
      },
      "id": "POOzpdTpQeTj"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#-----------EfficientNet B3------------------------#\")\n",
        "model = models.efficientnet_b3(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "  # Use the model object to select the desired layer\n",
        "  # Set model to evaluation mode\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 1536, 1, 1])\n",
        "\n",
        "print(\"#-----------EfficientNet B4------------------------#\")\n",
        "model = models.efficientnet_b4(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "  # Use the model object to select the desired layer\n",
        "  # Set model to evaluation mode\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 1792, 1, 1])\n",
        "\n",
        "print(\"#-----------EfficientNet B5------------------------#\")\n",
        "model = models.efficientnet_b5(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "  # Use the model object to select the desired layer\n",
        "  # Set model to evaluation mode\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 2048, 1, 1])\n",
        "\n",
        "print(\"#-----------EfficientNet B6------------------------#\")\n",
        "model = models.efficientnet_b6(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "  # Use the model object to select the desired layer\n",
        "  # Set model to evaluation mode\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 2304, 1, 1])\n",
        "\n",
        "print(\"#-----------EfficientNet B7------------------------#\")\n",
        "model = models.efficientnet_b7(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "  # Use the model object to select the desired layer\n",
        "  # Set model to evaluation mode\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 2560, 1, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6TbzvYUs6yVM",
        "outputId": "44674cd5-ffd8-4f35-f815-580e896e2ed1"
      },
      "id": "6TbzvYUs6yVM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#-----------EfficientNet B3------------------------#\n",
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.77\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.83\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.569\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.595\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.659\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.665\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.635\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.569\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.796\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.635\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.816\n",
            "#-----------EfficientNet B4------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.737\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.812\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.605\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.601\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.632\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.657\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.628\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.58\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.77\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.61\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.779\n",
            "#-----------EfficientNet B5------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B5_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B5_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.763\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.812\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.571\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.6\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.624\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.645\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.57\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.558\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.786\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.608\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.797\n",
            "#-----------EfficientNet B6------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B6_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B6_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.767\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.818\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.569\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.593\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.637\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.582\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.576\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.553\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.772\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.615\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.798\n",
            "#-----------EfficientNet B7------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c830127eef6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m# Set model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mapply_framework_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2560\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-ef3ced21dcc3>\u001b[0m in \u001b[0;36mapply_framework_model\u001b[0;34m(modelf, layer, feature_tensor_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m################### TRADITIONAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n\u001b[0m\u001b[1;32m     11\u001b[0m                   \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
            "\u001b[0;32m<ipython-input-19-d6c36dd30e7d>\u001b[0m in \u001b[0;36mapply_framework\u001b[0;34m(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misHSJOrBirds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m   embeddings = get_image_embeddings(path, \n\u001b[0m\u001b[1;32m    130\u001b[0m                                     \u001b[0mimage_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                                     model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\n",
            "\u001b[0;32m<ipython-input-19-d6c36dd30e7d>\u001b[0m in \u001b[0;36mget_image_embeddings\u001b[0;34m(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/ref/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/p0/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/p1/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d6c36dd30e7d>\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(image_name, model, layer, scaler, normalize, feature_tensor_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# 6. Run the model on our transformed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# 7. Detach our copy function from the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/ops/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/ops/misc.py\u001b[0m in \u001b[0;36m_scale\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vit_b_16(pretrained=True)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "layer = model._modules.get(\"encoder_layer_11\")\n",
        "  # Use the model object to select the desired layer\n",
        "  # Set model to evaluation mode\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 1536, 1, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "7zcCky3LOxHf",
        "outputId": "163dc86c-acf5-4211-e877-632bd5708fd9"
      },
      "id": "7zcCky3LOxHf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-506d0d8870d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Set model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mapply_framework_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-ef3ced21dcc3>\u001b[0m in \u001b[0;36mapply_framework_model\u001b[0;34m(modelf, layer, feature_tensor_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m################### TRADITIONAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n\u001b[0m\u001b[1;32m     11\u001b[0m                   \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
            "\u001b[0;32m<ipython-input-28-4f9ec1a873c9>\u001b[0m in \u001b[0;36mapply_framework\u001b[0;34m(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misHSJOrBirds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   embeddings = get_image_embeddings(path, \n\u001b[0m\u001b[1;32m    131\u001b[0m                                     \u001b[0mimage_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                                     model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\n",
            "\u001b[0;32m<ipython-input-28-4f9ec1a873c9>\u001b[0m in \u001b[0;36mget_image_embeddings\u001b[0;34m(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mim_no_name\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"0\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_no\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/ref/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/p0/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mfeature_tensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/p1/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_no_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_tensor_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4f9ec1a873c9>\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(image_name, model, layer, scaler, normalize, feature_tensor_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmy_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# 5. Attach that function to our selected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m# 6. Run the model on our transformed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'register_forward_hook'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model._modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVN4caz6SJnL",
        "outputId": "38262bd8-d083-4598-c15d-c49bdfa5f3a9"
      },
      "id": "fVN4caz6SJnL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('conv_proj',\n",
              "              Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))),\n",
              "             ('encoder', Encoder(\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (layers): Sequential(\n",
              "                  (encoder_layer_0): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_1): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_2): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_3): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_4): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_5): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_6): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_7): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_8): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_9): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_10): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                  (encoder_layer_11): EncoderBlock(\n",
              "                    (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (self_attention): MultiheadAttention(\n",
              "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "                    )\n",
              "                    (dropout): Dropout(p=0.0, inplace=False)\n",
              "                    (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "                    (mlp): MLPBlock(\n",
              "                      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                      (1): GELU(approximate='none')\n",
              "                      (2): Dropout(p=0.0, inplace=False)\n",
              "                      (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                      (4): Dropout(p=0.0, inplace=False)\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "              )),\n",
              "             ('heads', Sequential(\n",
              "                (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              "              ))])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_dummy(image_name, model, layer, scaler, normalize, feature_tensor_size):\n",
        "    \n",
        "#     # Load the pretrained model\n",
        "#     model = models.resnet18(pretrained=True)\n",
        "#     # Use the model object to select the desired layer\n",
        "#     layer = model._modules.get('avgpool')\n",
        "    \n",
        "    # 1. Load the image with Pillow library\n",
        "    img = Image.open(image_name).convert('RGB')\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\n",
        "    #    The 'avgpool' layer has an output size of 512\n",
        "    \n",
        "    # M1: my_embedding = torch.zeros(1, 512, 1, 1) and later my_embedding.copy_(o.data)\n",
        "    # M2: my_embedding = torch.zeros(512) and later my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    my_embedding = torch.zeros(feature_tensor_size).cuda()\n",
        "    \n",
        "    # 4. Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "        print(o.data.size())\n",
        "        # my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "        my_embedding.copy_(o.data)\n",
        "    # 5. Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # 6. Run the model on our transformed image\n",
        "    model(t_img)\n",
        "    # 7. Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # 8. Return the feature vector\n",
        "    return torch.flatten(my_embedding)\n",
        "    "
      ],
      "metadata": {
        "id": "OIAIzSSe6B1o"
      },
      "id": "OIAIzSSe6B1o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "# layer = model._modules.get(\"avgpool\")\n",
        "\n",
        "# feature = get_vector_dummy(\"000039.jpg\", model, layer, scaler, normalize, 512)\n",
        "\n",
        "layer = model._modules.get(\"layer4\")\n",
        "\n",
        "feature = get_vector_dummy(\"000039.jpg\", model, layer, scaler, normalize, [1, 256, 14, 14])\n",
        "t = torch.zeros([1, 128, 1, 1])\n",
        "print(t.size())\n",
        "print(torch.flatten(t).size())\n",
        "print(torch.zeros(128).size())"
      ],
      "metadata": {
        "id": "rks04Dtl5vDt"
      },
      "id": "rks04Dtl5vDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQHexWIaBRmZ",
        "outputId": "ba6b128d-d6f8-4bbf-ab9e-97858fa3a6c6"
      },
      "id": "zQHexWIaBRmZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50176])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model._modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODDK5Msd7Qfc",
        "outputId": "fd1245b9-24e9-44bf-a9f5-369ce8d15889"
      },
      "id": "ODDK5Msd7Qfc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('conv1',\n",
              "              Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)),\n",
              "             ('bn1',\n",
              "              BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
              "             ('relu', ReLU(inplace=True)),\n",
              "             ('maxpool',\n",
              "              MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)),\n",
              "             ('layer1', Sequential(\n",
              "                (0): BasicBlock(\n",
              "                  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "                (1): BasicBlock(\n",
              "                  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )),\n",
              "             ('layer2', Sequential(\n",
              "                (0): BasicBlock(\n",
              "                  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (downsample): Sequential(\n",
              "                    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                )\n",
              "                (1): BasicBlock(\n",
              "                  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )),\n",
              "             ('layer3', Sequential(\n",
              "                (0): BasicBlock(\n",
              "                  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (downsample): Sequential(\n",
              "                    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                )\n",
              "                (1): BasicBlock(\n",
              "                  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )),\n",
              "             ('layer4', Sequential(\n",
              "                (0): BasicBlock(\n",
              "                  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (downsample): Sequential(\n",
              "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                )\n",
              "                (1): BasicBlock(\n",
              "                  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (relu): ReLU(inplace=True)\n",
              "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )),\n",
              "             ('avgpool', AdaptiveAvgPool2d(output_size=(1, 1))),\n",
              "             ('fc', Linear(in_features=512, out_features=1000, bias=True))])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# diff layer\n",
        "layer = model._modules.get(\"layer3\")\n",
        "\n",
        "feature = get_vector_dummy(\"000039.jpg\", model, layer, scaler, normalize, 100352)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "4R-A4V7d7J0k",
        "outputId": "b4cef65b-4baf-486d-aaa5-e8e6a38b4a30"
      },
      "id": "4R-A4V7d7J0k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128, 28, 28])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-220ae3f16027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector_dummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"000039.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100352\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-1f889eeb7b2a>\u001b[0m in \u001b[0;36mget_vector_dummy\u001b[0;34m(image_name, model, layer, scaler, normalize, feature_tensor_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 6. Run the model on our transformed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 7. Detach our copy function from the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-e89a1959dc00>\u001b[0m in \u001b[0;36mcopy_data\u001b[0;34m(m, i, o)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmy_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 5. Attach that function to our selected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[128]' is invalid for input of size 100352"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# diff layer\n",
        "layer = model._modules.get(\"layer3\")\n",
        "\n",
        "feature = get_vector_dummy(\"000039.jpg\", model, layer, scaler, normalize, 100352)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "L2TH9o3y8GNO",
        "outputId": "5d09f145-bd6d-4d30-8089-3e43086ff459"
      },
      "id": "L2TH9o3y8GNO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128, 28, 28])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-220ae3f16027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector_dummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"000039.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100352\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-1f889eeb7b2a>\u001b[0m in \u001b[0;36mget_vector_dummy\u001b[0;34m(image_name, model, layer, scaler, normalize, feature_tensor_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 6. Run the model on our transformed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 7. Detach our copy function from the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-e89a1959dc00>\u001b[0m in \u001b[0;36mcopy_data\u001b[0;34m(m, i, o)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmy_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 5. Attach that function to our selected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[128]' is invalid for input of size 100352"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Transformers - new framework </h1>"
      ],
      "metadata": {
        "id": "ZIg0GhlheEJ-"
      },
      "id": "ZIg0GhlheEJ-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tutorial: https://discuss.pytorch.org/t/feature-extraction-in-torchvision-models-vit-b-16/148029/4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "  \n",
        "p0 = Variable(normalize(to_tensor(scaler(Image.open(\"p0.png\").convert('RGB')))).unsqueeze(0)).cuda()\n",
        "p1 = Variable(normalize(to_tensor(scaler(Image.open(\"p1.png\").convert('RGB')))).unsqueeze(0)).cuda()\n",
        "ref = Variable(normalize(to_tensor(scaler(Image.open(\"ref.png\").convert('RGB')))).unsqueeze(0)).cuda()\n",
        "\n",
        "model = torchvision.models.vit_b_16(pretrained=True)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "conv = feature_extractor[0]  \n",
        "encoder = feature_extractor[1]\n",
        "\n",
        "x_p0 = model._process_input(p0)\n",
        "x_p1 = model._process_input(p1)\n",
        "x_ref = model._process_input(ref)\n",
        "\n",
        "\n",
        "n = x_p0.shape[0]\n",
        "\n",
        "batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "\n",
        "x_p0 = torch.cat([batch_class_token, x_p0], dim=1)\n",
        "x_p1 = torch.cat([batch_class_token, x_p1], dim=1)\n",
        "x_ref = torch.cat([batch_class_token, x_ref], dim=1)\n",
        "\n",
        "x_p0 = encoder(x_p0)\n",
        "x_p1 = encoder(x_p1)\n",
        "x_ref = encoder(x_ref)\n",
        "\n",
        "x_p0 = torch.flatten(x_p0[:,0])\n",
        "x_p1 = torch.flatten(x_p1[:,0])\n",
        "x_ref = torch.flatten(x_ref[:,0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "WCrjWJXDTNqS",
        "outputId": "694e7353-3895-41be-b029-8d644ba92040"
      },
      "id": "WCrjWJXDTNqS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0c3f769044ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                   std=[0.229, 0.224, 0.225])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mp0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p0.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p1.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ref.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'p0.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory issue fix: https://stackoverflow.com/questions/70885777/run-out-of-memory-trying-to-create-a-tensor-of-size-2191-512-with-pytorch-to"
      ],
      "metadata": {
        "id": "88U4gdz1sa1c"
      },
      "id": "88U4gdz1sa1c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tutorial: https://discuss.pytorch.org/t/feature-extraction-in-torchvision-models-vit-b-16/148029/4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "model = torchvision.models.vit_b_16(pretrained=True)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "conv = feature_extractor[0]  \n",
        "encoder = feature_extractor[1]\n",
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()\n",
        "    \n",
        "\n",
        "\n",
        "def get_vector(path):\n",
        "  with torch.no_grad():\n",
        "    p0 = Variable(normalize(to_tensor(scaler(Image.open(path).convert('RGB')))).unsqueeze(0)).cuda()\n",
        "\n",
        "    x_p0 = model._process_input(p0)\n",
        "\n",
        "    n = x_p0.shape[0]\n",
        "\n",
        "    batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "\n",
        "    x_p0 = torch.cat([batch_class_token, x_p0], dim=1)\n",
        "\n",
        "    x_p0 = encoder(x_p0)\n",
        "\n",
        "    x_p0 = torch.flatten(x_p0[:,0])\n",
        "\n",
        "    return x_p0\n",
        "\n",
        "\n",
        "def get_image_embeddings(path, image_type, isHSJOrBirds):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector(path+\"/p0/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector(path+\"/p1/\" + im_no_name))\n",
        "\n",
        "        # now load decision\n",
        "        if isHSJOrBirds:\n",
        "          feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "          decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "          decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "          if decision[0] <= 0.5: \n",
        "                feature_tensor_dict[\"decision\"].append(0)\n",
        "          else: \n",
        "                feature_tensor_dict[\"decision\"].append(1)\n",
        "        \n",
        "    return feature_tensor_dict\n",
        "\n",
        "########################## GET MODEL PREDICTIONS\n",
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "########################## GET MODEL ACCURACY\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (1000 - number_wrong_predictions)/(1000)\n",
        "  return accuracy\n",
        "\n",
        "def get_average_reference_similarty_for_dataset(predictions,embeddings):\n",
        "  similarities_p0_p1 = []\n",
        "  similarities_p0_ref = []\n",
        "  similarities_p1_ref = []\n",
        "  \n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      similarities_p0_p1.append(float(cos(embeddings[\"p0\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "      similarities_p0_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0))))\n",
        "      similarities_p1_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "\n",
        "  similarity = np.average(similarities_p0_p1)/(float(np.average(similarities_p0_ref)+np.average(similarities_p1_ref)))\n",
        "\n",
        "  return similarity\n",
        "\n",
        "def apply_framework(path, image_type,isHSJOrBirds):\n",
        "  embeddings = get_image_embeddings(path, \n",
        "                                    image_type,\n",
        "                                    isHSJOrBirds)\n",
        "  predictions= get_predictions(embeddings)\n",
        "  accuracy = get_average_reference_similarty_for_dataset(predictions, embeddings)\n",
        "  print(path + \": \" + str(accuracy))\n",
        "  return accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDWHW3-ZkDhG",
        "outputId": "772c0029-23b4-4cfb-8d6a-8357ebc9380e"
      },
      "id": "pDWHW3-ZkDhG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_framework_model():\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------BAPPS 2AFC DIST-----------------------#\")\n",
        "  print(\"\")\n",
        "  ################### TRADITIONAL \n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                  \"png\",\n",
        "                  False)\n",
        "\n",
        "  ################ CNN\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                  \"png\",\n",
        "                  False)\n",
        "\n",
        "  ############### COLOR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                  \"png\",\n",
        "                  False)\n",
        "\n",
        "  ################ DEBLUR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                  \"png\",\n",
        "                  False)\n",
        "\n",
        "  ############### FRAME INTERP\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                  \"png\",\n",
        "                  False)\n",
        "\n",
        "  ################ SUPER RES\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                  \"png\",\n",
        "                  False)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### HSJ Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  True)\n",
        "\n",
        "  ################ HSJ Rank 0 + Rank 1\n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  True)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### Birds-16 Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  True)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Rank 1 \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  True)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                  \"jpg\",\n",
        "                  True)"
      ],
      "metadata": {
        "id": "9UznXre4s_wa"
      },
      "id": "9UznXre4s_wa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework_model() # reference similarity now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ6NCjaHeunb",
        "outputId": "266f0a23-33c2-4593-a14d-2e1549de530c"
      },
      "id": "NJ6NCjaHeunb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.44550060127941576\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.4440446430166139\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.579516591105198\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.5268853097386452\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.4992213435853494\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.5178743461653065\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.18113079661114512\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.21535694448091444\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.401459205264333\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.4775735705237343\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.4011398050617586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework_model() # reference similarity now"
      ],
      "metadata": {
        "id": "zIc6qlS8gUgx"
      },
      "id": "zIc6qlS8gUgx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "# print(cos(get_vector(\"p0.png\").unsqueeze(0),get_vector(\"ref.png\").unsqueeze(0)))\n",
        "# print(cos(get_vector(\"p1.png\").unsqueeze(0),get_vector(\"ref.png\").unsqueeze(0)))\n",
        "f_dict_new = get_image_embeddings(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\", \"jpg\", True)"
      ],
      "metadata": {
        "id": "l6Qb-IAlThTf"
      },
      "id": "l6Qb-IAlThTf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(f_dict_new[\"ref\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H99zIM3HrUxN",
        "outputId": "be479080-2118-4285-bf80-87377330bee8"
      },
      "id": "H99zIM3HrUxN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT_B_16 - ask chat to describe differences\n",
        "\n",
        " ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.804\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.832\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.619\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.617\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.658\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.71\n",
        "\n",
        " ----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.566\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.532\n",
        "\n",
        "----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.771\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.634\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.794\n",
        "\n"
      ],
      "metadata": {
        "id": "--r7GEafuw1i"
      },
      "id": "--r7GEafuw1i"
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrx2Aekota5B",
        "outputId": "e4e710aa-b934-4737-e7ca-9262204da375"
      },
      "id": "qrx2Aekota5B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.804\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.832\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.619\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.617\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.658\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.71\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.566\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.532\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.771\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.634\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vit_b_32\n",
        "\n",
        "----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.771\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.824\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.577\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.597\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.674\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.686\n",
        "\n",
        "----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.551\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.556\n",
        "\n",
        "----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.787\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.636\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.794"
      ],
      "metadata": {
        "id": "vZsgssuSywV8"
      },
      "id": "vZsgssuSywV8"
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC6F3P5-y0Lp",
        "outputId": "c00168ff-b218-4bd9-b773-7e95168b5351"
      },
      "id": "CC6F3P5-y0Lp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.771\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.824\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.577\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.597\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.674\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.686\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.551\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.556\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.787\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.636\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vit_l_16\n",
        "\n",
        "----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.779\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.832\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.625\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.588\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.66\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.688\n",
        "\n",
        "----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.569\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.54\n",
        "\n",
        "----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.791\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.622\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.825"
      ],
      "metadata": {
        "id": "ijS3i-Qt0dzv"
      },
      "id": "ijS3i-Qt0dzv"
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qyc35P90nO-",
        "outputId": "ae502b14-87c6-4b92-87d8-1225b8177211"
      },
      "id": "2qyc35P90nO-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.779\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.832\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.625\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.588\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.66\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.688\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.569\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.54\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.791\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.622\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vit_l_32\n",
        "\n",
        "----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
        "\n",
        "datasets/distortions/distortion_triplets/traditional_triplets: 0.776\n",
        "\n",
        "datasets/distortions/distortion_triplets/cnn_triplets: 0.839\n",
        "\n",
        "datasets/distortions/distortion_triplets/color_triplets: 0.571\n",
        "\n",
        "datasets/distortions/distortion_triplets/deblur_triplets: 0.577\n",
        "\n",
        "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.657\n",
        "\n",
        "datasets/distortions/distortion_triplets/superres_triplets: 0.637\n",
        "\n",
        "----------------------------------------HSJ------------------------#\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.565\n",
        "\n",
        "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.552\n",
        "\n",
        "----------------------------------------Birds-16------------------------#\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.767\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.616\n",
        "\n",
        "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.76"
      ],
      "metadata": {
        "id": "zzTrwXML89tP"
      },
      "id": "zzTrwXML89tP"
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWj7ZTRk9FAk",
        "outputId": "98a9951e-210a-402a-b06a-ad74c3d44756"
      },
      "id": "AWj7ZTRk9FAk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.776\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.839\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.571\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.577\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.657\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.637\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.565\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.552\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.767\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.616\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> New framework: </h3>"
      ],
      "metadata": {
        "id": "XGFZIECefylI"
      },
      "id": "XGFZIECefylI"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install img2vec-pytorch\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/distortion_triplets.zip\" -d \"/content/datasets/distortions\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/hsj_triplets.zip\" -d \"/content/datasets/hsj\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/birds_dataset_triplets.zip\" -d \"/content/datasets/birds-16\""
      ],
      "metadata": {
        "id": "gAFe3GzEq2KG"
      },
      "id": "gAFe3GzEq2KG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "############################ REQUIRES MODIFICATION FOR EACH MODEL \n",
        "\n",
        "# scaler = transforms.Resize((224, 224))\n",
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                  std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()   # used to convert the PIL image to a PyTorch tensor (multidimensional array)\n",
        "############################\n",
        "\n",
        "\n",
        "############################## HOW TO GET MODEL:\n",
        "# # Load the pretrained model\n",
        "# model = models.resnet18(pretrained=True)\n",
        "# model.cuda()\n",
        "# # Use the model object to select the desired layer\n",
        "# layer = model._modules.get('avgpool')\n",
        "# # Set model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "##################################\n",
        "\n",
        "# def get_vector(image_name, model, layer, scaler, normalize, feature_tensor_size):\n",
        "    \n",
        "# #     # Load the pretrained model\n",
        "# #     model = models.resnet18(pretrained=True)\n",
        "# #     # Use the model object to select the desired layer\n",
        "# #     layer = model._modules.get('avgpool')\n",
        "    \n",
        "#     # 1. Load the image with Pillow library\n",
        "#     img = Image.open(image_name).convert('RGB')\n",
        "#     # 2. Create a PyTorch Variable with the transformed image\n",
        "#     t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
        "\n",
        "    \n",
        "#     feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "#     conv = feature_extractor[0]  \n",
        "#     encoder = feature_extractor[1]\n",
        "\n",
        "#     x_p0 = model._process_input(t_img)\n",
        "#     n = x_p0.shape[0]\n",
        "#     batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "#     x_p0 = torch.cat([batch_class_token, x_p0], dim=1)\n",
        "#     x_p0 = encoder(x_p0)\n",
        "#     x_p0 = torch.flatten(x_p0[:,0])\n",
        "\n",
        "#     return x_p0\n",
        "\n",
        "############################ COSINE SIMILARITY\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "########################### GET FEATURE VECTORS\n",
        "def get_image_embeddings(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector(path+\"/p0/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector(path+\"/p1/\" + im_no_name))\n",
        "\n",
        "        # now load decision\n",
        "        if isHSJOrBirds:\n",
        "          feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "          decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "          decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "          if decision[0] <= 0.5: \n",
        "                feature_tensor_dict[\"decision\"].append(0)\n",
        "          else: \n",
        "                feature_tensor_dict[\"decision\"].append(1)\n",
        "        \n",
        "    return feature_tensor_dict\n",
        "\n",
        "\n",
        "########################## GET MODEL PREDICTIONS\n",
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "########################## GET MODEL ACCURACY\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (1000 - number_wrong_predictions)/(1000)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def apply_framework(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):\n",
        "  embeddings = get_image_embeddings(path, \n",
        "                                    image_type,\n",
        "                                    model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\n",
        "  predictions= get_predictions(embeddings)\n",
        "  accuracy = get_accuracy(predictions, embeddings)\n",
        "  print(path + \": \" + str(accuracy))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "eq3k76SqeNE2"
      },
      "id": "eq3k76SqeNE2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_framework_model(modelf, layer, feature_tensor_size):\n",
        "  scaler = transforms.Resize((224, 224))\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------BAPPS 2AFC DIST-----------------------#\")\n",
        "  print(\"\")\n",
        "  ################### TRADITIONAL \n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ CNN\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ############### COLOR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ DEBLUR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ############### FRAME INTERP\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ SUPER RES\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### HSJ Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################ HSJ Rank 0 + Rank 1\n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### Birds-16 Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Rank 1 \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)"
      ],
      "metadata": {
        "id": "YnfDJ9S2f8hm"
      },
      "id": "YnfDJ9S2f8hm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Similarity between references for each dataset </h1>"
      ],
      "metadata": {
        "id": "xtadaRexcGrg"
      },
      "id": "xtadaRexcGrg"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "############################ REQUIRES MODIFICATION FOR EACH MODEL \n",
        "\n",
        "# scaler = transforms.Resize((224, 224))\n",
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                  std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()   # used to convert the PIL image to a PyTorch tensor (multidimensional array)\n",
        "############################\n",
        "\n",
        "\n",
        "############################## HOW TO GET MODEL:\n",
        "# # Load the pretrained model\n",
        "# model = models.resnet18(pretrained=True)\n",
        "# model.cuda()\n",
        "# # Use the model object to select the desired layer\n",
        "# layer = model._modules.get('avgpool')\n",
        "# # Set model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "##################################\n",
        "\n",
        "def get_vector(image_name, model, layer, scaler, normalize, feature_tensor_size):\n",
        "    \n",
        "#     # Load the pretrained model\n",
        "#     model = models.resnet18(pretrained=True)\n",
        "#     # Use the model object to select the desired layer\n",
        "#     layer = model._modules.get('avgpool')\n",
        "    \n",
        "    # 1. Load the image with Pillow library\n",
        "    img = Image.open(image_name).convert('RGB')\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\n",
        "    #    The 'avgpool' layer has an output size of 512\n",
        "    \n",
        "    # M1: my_embedding = torch.zeros(1, 512, 1, 1) and later my_embedding.copy_(o.data)\n",
        "    # M2: my_embedding = torch.zeros(512) and later my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    my_embedding = torch.zeros(feature_tensor_size).cuda()\n",
        "    \n",
        "    # 4. Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "        my_embedding.copy_(o.data)\n",
        "    # 5. Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # 6. Run the model on our transformed image\n",
        "    model(t_img)\n",
        "    # 7. Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # 8. Return the feature vector\n",
        "    return torch.flatten(my_embedding)\n",
        "\n",
        "############################ COSINE SIMILARITY\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "########################### GET FEATURE VECTORS\n",
        "def get_image_embeddings(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector(path+\"/p0/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector(path+\"/p1/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "\n",
        "        # now load decision\n",
        "        if isHSJOrBirds:\n",
        "          feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "          decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "          decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "          if decision[0] <= 0.5: \n",
        "                feature_tensor_dict[\"decision\"].append(0)\n",
        "          else: \n",
        "                feature_tensor_dict[\"decision\"].append(1)\n",
        "        \n",
        "    return feature_tensor_dict\n",
        "\n",
        "\n",
        "########################## GET MODEL PREDICTIONS\n",
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "########################## GET MODEL ACCURACY\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (1000 - number_wrong_predictions)/(1000)\n",
        "  return accuracy\n",
        "\n",
        "def get_average_reference_similarty_for_dataset(predictions,embeddings):\n",
        "  similarities_p0_p1 = []\n",
        "  similarities_p0_ref = []\n",
        "  similarities_p1_ref = []\n",
        "  \n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      similarities_p0_p1.append(float(cos(embeddings[\"p0\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "      similarities_p0_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0))))\n",
        "      similarities_p1_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "\n",
        "  similarity = np.average(similarities_p0_p1)/(float(np.average(similarities_p0_ref)+np.average(similarities_p1_ref)))\n",
        "\n",
        "  return similarity\n",
        "  \n",
        "\n",
        "def apply_framework(path, image_type, model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size):\n",
        "  embeddings = get_image_embeddings(path, \n",
        "                                    image_type,\n",
        "                                    model, layer, scaler, normalize, isHSJOrBirds, feature_tensor_size)\n",
        "  predictions= get_predictions(embeddings)\n",
        "  accuracy = get_accuracy(predictions, embeddings)\n",
        "  print(path + \": \" + str(accuracy))\n",
        "  return accuracy\n",
        "\n",
        "def apply_framework_model(modelf, layer, feature_tensor_size):\n",
        "  scaler = transforms.Resize((224, 224))\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------BAPPS 2AFC DIST-----------------------#\")\n",
        "  print(\"\")\n",
        "  ################### TRADITIONAL \n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/traditional_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ CNN\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/cnn_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ############### COLOR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/color_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ DEBLUR\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/deblur_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ############### FRAME INTERP\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/frameinterp_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  ################ SUPER RES\n",
        "  apply_framework(\"datasets/distortions/distortion_triplets/superres_triplets\",\n",
        "                  \"png\",\n",
        "                  modelf, layer, scaler, normalize, False, feature_tensor_size)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------HSJ------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### HSJ Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################ HSJ Rank 0 + Rank 1\n",
        "  apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"# ----------------------------------------Birds-16------------------------#\")\n",
        "  print(\"\")\n",
        "  ################### Birds-16 Rank 0 + Dissimilar \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Rank 1 \n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)\n",
        "\n",
        "  ################### Birds-16 Rank 0 + Dissimilar 2rank1 test\n",
        "  apply_framework(\"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1\",\n",
        "                  \"jpg\",\n",
        "                  modelf, layer, scaler, normalize, True, feature_tensor_size)"
      ],
      "metadata": {
        "id": "8FmPCI28cMZW"
      },
      "id": "8FmPCI28cMZW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#-------------------------------Resnet-18-------------------------------------#\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 512, 1, 1])\n",
        "\n",
        "print(\"#-------------------------------AlexNet-------------------------------------#\")\n",
        "print(\"alexnet\")\n",
        "model = models.alexnet(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 256, 6, 6])\n",
        "\n",
        "print(\"#-------------------------------EfficientNet-------------------------------------#\")\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "model.cuda()\n",
        "layer = model._modules.get(\"avgpool\")\n",
        "model.eval()\n",
        "apply_framework_model(model,layer,[1, 1280, 1, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "70b50fdf983e42098f0435261aec9fa3",
            "5e7907f27f8d4b49a3baeaab7fe5e4e7",
            "5137bb0cc231464f9cfd9ed392242948",
            "c1b2fa2f253245898d9b4e05a58080fa",
            "0dd0bbaf98b145d59b62451b7f086a4a",
            "119d4d6375e44ac0be64515d49e889da",
            "a174d6a993554ebfa2ad306f4dd9a76a",
            "1c1d47cf2d1a4100bb6b94ee967daf4c",
            "3d49b8b4c2f24517bc3d444598143e26",
            "ef9cf3c084684193876d0b3438cba00d",
            "5ec4fbb76c3f4effa82e83be008388e0"
          ]
        },
        "id": "pkEW2FLodYo6",
        "outputId": "cd3cc877-f19c-4cb8-9b78-f387dfa1c54d"
      },
      "id": "pkEW2FLodYo6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#-------------------------------Resnet-18-------------------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.4641208374760602\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.47506779143223193\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.5205962052334754\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.5227560103399405\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.49837198871172556\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.5032645443982735\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.46999940363671466\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.48269508874571315\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.4763286114740994\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.4901479407311331\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.4713947121570724\n",
            "#-------------------------------AlexNet-------------------------------------#\n",
            "alexnet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.4364289060079188\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.45928621575531037\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.542875106223054\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.522324235014437\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.5016308570654885\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.504446025573848\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.3888592227278987\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.4373141247893356\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.4305198639769745\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.4636477607016273\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.41671250148154804\n",
            "#-------------------------------EfficientNet-------------------------------------#\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70b50fdf983e42098f0435261aec9fa3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ----------------------------------------BAPPS 2AFC DIST-----------------------#\n",
            "\n",
            "datasets/distortions/distortion_triplets/traditional_triplets: 0.43987050895301505\n",
            "datasets/distortions/distortion_triplets/cnn_triplets: 0.4405119395727711\n",
            "datasets/distortions/distortion_triplets/color_triplets: 0.5492265690689818\n",
            "datasets/distortions/distortion_triplets/deblur_triplets: 0.553268302017516\n",
            "datasets/distortions/distortion_triplets/frameinterp_triplets: 0.49308268301438096\n",
            "datasets/distortions/distortion_triplets/superres_triplets: 0.5104082710720147\n",
            "\n",
            "# ----------------------------------------HSJ------------------------#\n",
            "\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar: 0.10308682840902555\n",
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.14060771732463742\n",
            "\n",
            "# ----------------------------------------Birds-16------------------------#\n",
            "\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.3818553559258767\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.4717009809021449\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar_2rank1: 0.37074582527955985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Training our own models </h1>"
      ],
      "metadata": {
        "id": "3o5pmW80vKRQ"
      },
      "id": "3o5pmW80vKRQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "3hFwtkvHvN1g"
      },
      "id": "3hFwtkvHvN1g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2020)\n",
        "np.random.seed(2020)\n",
        "random.seed(2020)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.get_device_name()"
      ],
      "metadata": {
        "id": "ojBh21llyWv5"
      },
      "id": "ojBh21llyWv5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dims = 2\n",
        "batch_size = 32\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "e55s7MKVygaB"
      },
      "id": "e55s7MKVygaB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HSJ(Dataset):\n",
        "    def __init__(self, path, train=True, transform=None):\n",
        "        self.is_train = train\n",
        "        self.transform = transform\n",
        "        self.to_pil = transforms.ToPILImage()\n",
        "        self.path=path\n",
        "        \n",
        "        if self.is_train:   \n",
        "            self.image_indices_range = (0,699)\n",
        "            self.length = 700  \n",
        "        else: \n",
        "            self.image_indices_range = (700,999)\n",
        "            self.length = 300  \n",
        "        self.scaler = transforms.Resize((28, 28))\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        #     self.images = df.iloc[:, 1:].values.astype(np.uint8)\n",
        "        #     self.labels = df.iloc[:, 0].values\n",
        "        #     self.index = df.index.values\n",
        "        # else:\n",
        "        #     self.images = df.values.astype(np.uint8)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        # item is just an index --> transform into label\n",
        "        im_no_name =  (6-len(str(item)))*\"0\" + str(item) + \".\" + \"jpg\"\n",
        "        print(im_no_name)\n",
        "        image_name_ref = self.path+\"/ref/\" + im_no_name\n",
        "\n",
        "        anchor_img = self.scaler(Image.open(image_name_ref).convert('L'))\n",
        "        # anchor_img = self.images[item].reshape(28, 28, 1)\n",
        "        \n",
        "        if self.is_train:\n",
        "            image_name_p0 = self.path+\"/p0/\" + im_no_name\n",
        "            positive_img = self.scaler(Image.open(image_name_p0).convert('L'))\n",
        "\n",
        "            image_name_p1 = self.path+\"/p1/\" + im_no_name\n",
        "            negative_img = self.scaler(Image.open(image_name_p1).convert('L'))\n",
        "            \n",
        "            if self.transform:\n",
        "                anchor_img = self.transform(anchor_img)\n",
        "                positive_img = self.transform(positive_img)\n",
        "                negative_img = self.transform(negative_img)\n",
        "            \n",
        "            return anchor_img, positive_img, negative_img   # anchor_label\n",
        "        \n",
        "        else:\n",
        "            if self.transform:\n",
        "                anchor_img = self.transform(anchor_img)\n",
        "            return anchor_img"
      ],
      "metadata": {
        "id": "qaaFI8QC1Diw"
      },
      "id": "qaaFI8QC1Diw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = HSJ(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\", \n",
        "                 train=True,\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor()\n",
        "                 ]))\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6T3VI_q-wVW",
        "outputId": "c3c34d42-974d-4f14-cd22-ab4b9dae66ce"
      },
      "id": "G6T3VI_q-wVW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = HSJ(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "                train=False, \n",
        "                transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "Sf2sADdM_CyX"
      },
      "id": "Sf2sADdM_CyX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        \n",
        "    def calc_euclidean(self, x1, x2):\n",
        "        return (x1 - x2).pow(2).sum(1)\n",
        "    \n",
        "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "        distance_positive = self.calc_euclidean(anchor, positive)\n",
        "        distance_negative = self.calc_euclidean(anchor, negative)\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "\n",
        "        return losses.mean()"
      ],
      "metadata": {
        "id": "9vLEN6tS_cc_"
      },
      "id": "9vLEN6tS_cc_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dml_42YOFbPD"
      },
      "id": "Dml_42YOFbPD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, emb_dim=128):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 5),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(32, 64, 5),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64*4*4, 512),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(512, emb_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(-1, 64*4*4)\n",
        "        x = self.fc(x)\n",
        "        # x = nn.functional.normalize(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QhTydXLu_mk1"
      },
      "id": "QhTydXLu_mk1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight)"
      ],
      "metadata": {
        "id": "SkmJAITp_0cp"
      },
      "id": "SkmJAITp_0cp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network(embedding_dims)\n",
        "model.apply(init_weights)\n",
        "model = torch.jit.script(model).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.jit.script(TripletLoss())"
      ],
      "metadata": {
        "id": "QrLJwKd6_2V9"
      },
      "id": "QrLJwKd6_2V9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "    running_loss = []\n",
        "    for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
        "        anchor_img = anchor_img.to(device)\n",
        "        positive_img = positive_img.to(device)\n",
        "        negative_img = negative_img.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        anchor_out = model(anchor_img)\n",
        "        positive_out = model(positive_img)\n",
        "        negative_out = model(negative_img)\n",
        "        \n",
        "        loss = criterion(anchor_out, positive_out, negative_out)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss.append(loss.cpu().detach().numpy())\n",
        "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))"
      ],
      "metadata": {
        "id": "3FSVaiKJ__Tv"
      },
      "id": "3FSVaiKJ__Tv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\"model_state_dict\": model.state_dict(),\n",
        "            \"optimzier_state_dict\": optimizer.state_dict()\n",
        "           }, \"trained_model.pth\")"
      ],
      "metadata": {
        "id": "xLNZMEYIIIAO"
      },
      "id": "xLNZMEYIIIAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img in tqdm(test_loader):\n",
        "        train_results.append(model(img.to(device)).cpu().numpy())\n",
        "        \n",
        "train_results = np.concatenate(train_results)\n",
        "train_results.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "454d4a72e6d947629edb3e4d4d1d94f7",
            "674d6cbdf514454496b8e40e45e1b511",
            "a02ae67e2c9d4003a2f9c2b0e42585b0",
            "555c13acdcf44b00a9c8c493ef5e6e19",
            "4697c877055847ef96d9623af9283562",
            "4c5a5a7462524faca041881f07315276",
            "bd3c1581e728426fab64db29b85b7d1e",
            "fab99da8597f423cad4263503d5d4d74",
            "93be17e1287b434181e33305f3e3a33d",
            "d8218176d5af4cc093500ab9846984f9",
            "fc122f29b68f4636b4337629915a9081"
          ]
        },
        "id": "92zpfp4yIKsC",
        "outputId": "0b64ff4a-9cd2-49b0-fb84-5a685a1fb700"
      },
      "id": "92zpfp4yIKsC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "454d4a72e6d947629edb3e4d4d1d94f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 1. Load the image with Pillow library\n",
        "item=0\n",
        "path = \"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\"\n",
        "im_no_name =  (6-len(str(item)))*\"0\" + str(item) + \".\" + \"jpg\"\n",
        "image_name = path+\"/ref/\" + im_no_name\n",
        "\n",
        "img = to_tensor(scaler(Image.open(image_name).convert('L')))\n",
        "print(img.size())\n",
        "x = img.view(-1, 64*4*4)\n",
        "x.size()\n",
        "# t = transforms.ToPILImage()\n",
        "# img_img = t(img)\n",
        "# # t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
        "# # scaler(img)\n",
        "# img.size() \n",
        "# # feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name, model, layer, scaler, normalize, feature_tensor_size))\n",
        "# img_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET3gYihB5z3M",
        "outputId": "0541a827-8d0f-458f-8d67-324448074b45"
      },
      "id": "ET3gYihB5z3M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 224, 224])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([49, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New model"
      ],
      "metadata": {
        "id": "q6v9-ptV6p5N"
      },
      "id": "q6v9-ptV6p5N"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install img2vec-pytorch\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/distortion_triplets.zip\" -d \"/content/datasets/distortions\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/hsj_triplets.zip\" -d \"/content/datasets/hsj\"\n",
        "!unzip -uq \"/content/drive/MyDrive/Colab Notebooks/birds_dataset_triplets.zip\" -d \"/content/datasets/birds-16\""
      ],
      "metadata": {
        "id": "Ril7hZc86o5p",
        "outputId": "60c45fdc-5e1b-4f29-cdb9-34acb58718d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ril7hZc86o5p",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting img2vec-pytorch\n",
            "  Downloading img2vec_pytorch-1.0.1-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (1.22.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from img2vec-pytorch) (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->img2vec-pytorch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->img2vec-pytorch) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->img2vec-pytorch) (8.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->img2vec-pytorch) (2.0.12)\n",
            "Installing collected packages: img2vec-pytorch\n",
            "Successfully installed img2vec-pytorch-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "m2gadCDs9MPa"
      },
      "id": "m2gadCDs9MPa",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset loader"
      ],
      "metadata": {
        "id": "-76K3XZN1c-Q"
      },
      "id": "-76K3XZN1c-Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import json\n",
        "from torchvision.transforms import transforms\n",
        "import random\n",
        "\n",
        "class HSJ(Dataset):\n",
        "    def __init__(self, path, image_type, train=True, transform=None):\n",
        "        self.image_type = image_type\n",
        "        self.is_train = train\n",
        "        self.transform = transform\n",
        "        self.to_pil = transforms.ToPILImage()\n",
        "        self.path=path\n",
        "        \n",
        "        if self.is_train:   \n",
        "            self.image_indices_range = (0,799)\n",
        "            self.length = 800  \n",
        "        else: \n",
        "            self.image_indices_range = (800,899)\n",
        "            self.length = 100  \n",
        "        self.scaler = transforms.Resize((224, 224))\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        #     self.images = df.iloc[:, 1:].values.astype(np.uint8)\n",
        "        #     self.labels = df.iloc[:, 0].values\n",
        "        #     self.index = df.index.values\n",
        "        # else:\n",
        "        #     self.images = df.values.astype(np.uint8)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        # item is just an index --> transform into label\n",
        "        if self.is_train == False:\n",
        "          item = item + 800\n",
        "          # print(\"VALIDATION NOW---------------------------------------------------------\")\n",
        "        im_no_name =  (6-len(str(item)))*\"0\" + str(item) + \".\" + self.image_type\n",
        "        if (item < 1000) and (item > 900):\n",
        "          print(item)\n",
        "          print(\"This should not be here.\")\n",
        "        # if not self.is_train:\n",
        "        #     print()\n",
        "        #     print(im_no_name)\n",
        "        image_name_ref = self.path+\"/ref/\" + im_no_name\n",
        "        # if not self.is_train:\n",
        "        #     print(image_name_ref)\n",
        "\n",
        "        anchor_img = Image.open(image_name_ref).convert('RGB')\n",
        "        # anchor_img = self.images[item].reshape(28, 28, 1)\n",
        "        \n",
        "        if True:\n",
        "            image_name_p0 = self.path+\"/p0/\" + im_no_name\n",
        "            positive_img = Image.open(image_name_p0).convert('RGB')\n",
        "            # if not self.is_train:\n",
        "            #     print(image_name_p0)\n",
        "\n",
        "            image_name_p1 = self.path+\"/p1/\" + im_no_name\n",
        "            negative_img = Image.open(image_name_p1).convert('RGB')\n",
        "            # if not self.is_train:\n",
        "            #     print(image_name_p1)\n",
        "            #     print(\"---------------------------\")\n",
        "                \n",
        "            if self.transform:\n",
        "                anchor_img = self.transform(anchor_img)\n",
        "                positive_img = self.transform(positive_img)\n",
        "                negative_img = self.transform(negative_img)\n",
        "            \n",
        "            return anchor_img, positive_img, negative_img   # anchor_label"
      ],
      "metadata": {
        "id": "16PZjHajJBRZ"
      },
      "id": "16PZjHajJBRZ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load train and validation sets\n",
        "To load other sets simple change path"
      ],
      "metadata": {
        "id": "QdWOKZZm1g5F"
      },
      "id": "QdWOKZZm1g5F"
    },
    {
      "cell_type": "code",
      "source": [
        "# birds_similar_8rank2 = \"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\"\n",
        "# hsj_similar_8rank2 = \"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\"\n",
        "\n",
        "# def get_train_dataset(IMAGE_SIZE=224):\n",
        "#     train_dataset = HSJ(birds_disimilar_8rank2,\n",
        "#                         \"jpg\",\n",
        "#                         train=True,\n",
        "#                         transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE))]))\n",
        "#     return train_dataset\n",
        "\n",
        "# def get_val_dataset(IMAGE_SIZE=224):\n",
        "#     train_dataset = HSJ(birds_disimilar_8rank2,\n",
        "#                         \"jpg\",\n",
        "#                         train=False,\n",
        "#                         transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE))]))\n",
        "#     return train_dataset"
      ],
      "metadata": {
        "id": "65trlQbEJ6WY"
      },
      "id": "65trlQbEJ6WY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triplet loss function (old) - currently using pytorch triplet margin loss"
      ],
      "metadata": {
        "id": "pZTFFNEl1nug"
      },
      "id": "pZTFFNEl1nug"
    },
    {
      "cell_type": "code",
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    def calc_euclidean(self, x1, x2):\n",
        "        return (x1 - x2).pow(2).sum(1)\n",
        "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "        distance_positive = self.calc_euclidean(anchor, positive)\n",
        "        distance_negative = self.calc_euclidean(anchor, negative)\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "        return losses.mean()"
      ],
      "metadata": {
        "id": "EcRYZv1wK4gV"
      },
      "id": "EcRYZv1wK4gV",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "XIDGyw2TLMBM"
      },
      "id": "XIDGyw2TLMBM",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define params"
      ],
      "metadata": {
        "id": "9F_RNrRC-Db2"
      },
      "id": "9F_RNrRC-Db2"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "#””Pick GPU if available, else CPU”””\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "# worked for ResNet-18\n",
        "# IMAGE_SIZE = 224\n",
        "# BATCH_SIZE = 100\n",
        "# DEVICE = get_default_device()\n",
        "# LEARNING_RATE = 0.00001\n",
        "# EPOCHS = 10\n",
        "\n",
        "# worked for ResNet-152\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 100\n",
        "DEVICE = get_default_device()\n",
        "LEARNING_RATE = 0.00001\n",
        "EPOCHS = 15"
      ],
      "metadata": {
        "id": "_STESorILVTe"
      },
      "id": "_STESorILVTe",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "birds_similar_8rank2 = \"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\"\n",
        "birds_disimilar_8rank2 = \"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\"\n",
        "hsj_similar_8rank2 = \"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\"\n",
        "frameinterp = \"datasets/distortions/distortion_triplets/frameinterp_triplets\"\n",
        "\n",
        "def get_train_dataset(IMAGE_SIZE=100):\n",
        "    train_dataset = HSJ(hsj_similar_8rank2,\n",
        "                        \"jpg\",\n",
        "                        train=True,\n",
        "                        transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE))]))\n",
        "    return train_dataset\n",
        "\n",
        "def get_val_dataset(IMAGE_SIZE=100):\n",
        "    train_dataset = HSJ(hsj_similar_8rank2,\n",
        "                        \"jpg\",\n",
        "                        train=False,\n",
        "                        transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE))]))\n",
        "    return train_dataset\n",
        "\n",
        "train_dataset = get_train_dataset(IMAGE_SIZE = IMAGE_SIZE)\n",
        "val_dataset = get_val_dataset(IMAGE_SIZE = IMAGE_SIZE)\n",
        "train_dl = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=2,pin_memory=True)\n",
        "val_dl = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=2,pin_memory=True)"
      ],
      "metadata": {
        "id": "HKQbVCZxLc3r"
      },
      "id": "HKQbVCZxLc3r",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models defined here"
      ],
      "metadata": {
        "id": "kv-6ZFRTXZF5"
      },
      "id": "kv-6ZFRTXZF5"
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet_Triplet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Feature_Extractor = resnet18(pretrained=True)\n",
        "        num_filters = self.Feature_Extractor.fc.in_features\n",
        "        self.Feature_Extractor.fc = nn.Sequential(\n",
        "                  nn.Linear(num_filters,512),\n",
        "                  nn.LeakyReLU(),\n",
        "                  nn.Linear(512,10))\n",
        "        self.Triplet_Loss = nn.Sequential(\n",
        "                  nn.Linear(10,2))\n",
        "    def forward(self,x):\n",
        "        features = self.Feature_Extractor(x)\n",
        "        triplets = self.Triplet_Loss(features)\n",
        "        return triplets\n",
        "\n",
        "\n",
        "class ResNet_Triplet34(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Feature_Extractor = resnet34(pretrained=True)\n",
        "        num_filters = self.Feature_Extractor.fc.in_features\n",
        "        self.Feature_Extractor.fc = nn.Sequential(\n",
        "                  nn.Linear(num_filters,512),\n",
        "                  nn.LeakyReLU(),\n",
        "                  nn.Linear(512,10))\n",
        "        self.Triplet_Loss = nn.Sequential(\n",
        "                  nn.Linear(10,2))\n",
        "    def forward(self,x):\n",
        "        features = self.Feature_Extractor(x)\n",
        "        triplets = self.Triplet_Loss(features)\n",
        "        return triplets\n",
        "\n",
        "\n",
        "class ResNet_Triplet34_Frozen(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Feature_Extractor = resnet34(pretrained=True)\n",
        "        i=0\n",
        "        for child in self.Feature_Extractor.children():\n",
        "          i += 1\n",
        "          if (i<10):\n",
        "                for param in child.parameters():\n",
        "                      param.requires_grad = False  # freeze the feature extractor\n",
        "\n",
        "        num_filters = self.Feature_Extractor.fc.in_features\n",
        "        self.Feature_Extractor.fc = nn.Sequential(\n",
        "                  nn.Linear(num_filters,512),\n",
        "                  nn.LeakyReLU(),\n",
        "                  nn.Linear(512,10))\n",
        "        self.Triplet_Loss = nn.Sequential(\n",
        "                  nn.Linear(10,2))\n",
        "    def forward(self,x):\n",
        "        features = self.Feature_Extractor(x)\n",
        "        triplets = self.Triplet_Loss(features)\n",
        "        return triplets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet_Triplet152(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Feature_Extractor = torchvision.models.resnet152(pretrained=True)\n",
        "        num_filters = self.Feature_Extractor.fc.in_features\n",
        "        self.Feature_Extractor.fc = nn.Sequential(\n",
        "                  nn.Linear(num_filters,512),\n",
        "                  nn.LeakyReLU(),\n",
        "                  nn.Linear(512,10))\n",
        "        self.Triplet_Loss = nn.Sequential(\n",
        "                  nn.Linear(10,2))\n",
        "    def forward(self,x):\n",
        "        features = self.Feature_Extractor(x)\n",
        "        triplets = self.Triplet_Loss(features)\n",
        "        return triplets\n",
        "\n",
        "\n",
        "class ResNet_Triplet152_Frozen(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Feature_Extractor = torchvision.models.resnet152(pretrained=True)\n",
        "        i=0\n",
        "        for child in self.Feature_Extractor.children():\n",
        "          i += 1\n",
        "          if (i<10):\n",
        "                for param in child.parameters():\n",
        "                      param.requires_grad = False  # freeze the feature extractor\n",
        "\n",
        "        num_filters = self.Feature_Extractor.fc.in_features\n",
        "        self.Feature_Extractor.fc = nn.Sequential(\n",
        "                  nn.Linear(num_filters,512),\n",
        "                  nn.LeakyReLU(),\n",
        "                  nn.Linear(512,10))\n",
        "        self.Triplet_Loss = nn.Sequential(\n",
        "                  nn.Linear(10,2))\n",
        "    def forward(self,x):\n",
        "        features = self.Feature_Extractor(x)\n",
        "        triplets = self.Triplet_Loss(features)\n",
        "        return triplets\n",
        "\n",
        "class ViT_Triplet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Feature_Extractor = torchvision.models.vit_b_32(pretrained=True)\n",
        "        # num_filters = self.Feature_Extractor.heads.in_features\n",
        "        self.Feature_Extractor.heads = nn.Sequential(\n",
        "                  nn.Linear(768,512),\n",
        "                  nn.LeakyReLU(),\n",
        "                  nn.Linear(512,10))\n",
        "        self.Triplet_Loss = nn.Sequential(\n",
        "                  nn.Linear(10,2))\n",
        "    def forward(self,x):\n",
        "        features = self.Feature_Extractor(x)\n",
        "        triplets = self.Triplet_Loss(features)\n",
        "        return triplets\n",
        "\n",
        "\n",
        "class ViT_Triplet_Frozen(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Feature_Extractor = torchvision.models.vit_b_32(pretrained=True)\n",
        "        i=0\n",
        "        for child in self.Feature_Extractor.children():\n",
        "          i += 1\n",
        "          if (i<3):\n",
        "                for param in child.parameters():\n",
        "                      param.requires_grad = False  # freeze the feature extractor\n",
        "\n",
        "        # num_filters = self.Feature_Extractor.heads.in_features\n",
        "        self.Feature_Extractor.heads = nn.Sequential(\n",
        "                  nn.Linear(768,512),\n",
        "                  nn.LeakyReLU(),\n",
        "                  nn.Linear(512,10))\n",
        "        self.Triplet_Loss = nn.Sequential(\n",
        "                  nn.Linear(10,2))\n",
        "    def forward(self,x):\n",
        "        features = self.Feature_Extractor(x)\n",
        "        triplets = self.Triplet_Loss(features)\n",
        "        return triplets\n",
        "# class ResNet_Triplet34_Frozen(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.Feature_Extractor = resnet34(pretrained=True)\n",
        "#         num_filters = self.Feature_Extractor.fc.in_features\n",
        "#         self.Feature_Extractor.fc = nn.Sequential(\n",
        "#                   nn.Linear(num_filters,512),\n",
        "#                   nn.LeakyReLU(),\n",
        "#                   nn.Linear(512,10))\n",
        "#         self.Triplet_Loss = nn.Sequential(\n",
        "#                   nn.Linear(10,2))\n",
        "#     def forward(self,x):\n",
        "#         features = self.Feature_Extractor(x)\n",
        "#         triplets = self.Triplet_Loss(features)\n",
        "#         return triplets"
      ],
      "metadata": {
        "id": "-Ky0D5S7Lu7_"
      },
      "id": "-Ky0D5S7Lu7_",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models\n",
        "\n",
        "vit = torchvision.models.vit_b_32(pretrained=True)\n",
        "vit._modules\n",
        "i=0\n",
        "for child in vit.children():\n",
        "    i += 1\n",
        "    print(\"---------------------------\")\n",
        "    print(i)\n",
        "    print(child)\n",
        "    print(\"---------------------------\")\n"
      ],
      "metadata": {
        "id": "BjiFnfjjwkZK"
      },
      "id": "BjiFnfjjwkZK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit._modules"
      ],
      "metadata": {
        "id": "Cs6jZAuTxQcq"
      },
      "id": "Cs6jZAuTxQcq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = resnet34(pretrained=True)\n",
        "i=0\n",
        "for child in res.children():\n",
        "  i += 1\n",
        "  if (i<10):\n",
        "    print(str(i) + \"---------------------------------------------------\")\n",
        "    print(child)\n",
        "# #     # for param in child.parameters():\n",
        "# #     #     param.requires_grad = False\n",
        "res.fc"
      ],
      "metadata": {
        "id": "ukw-dDB0sHTi"
      },
      "id": "ukw-dDB0sHTi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "#””Pick GPU if available, else CPU”””\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "# worked for ResNet-18\n",
        "# IMAGE_SIZE = 224\n",
        "# BATCH_SIZE = 100\n",
        "# DEVICE = get_default_device()\n",
        "# LEARNING_RATE = 0.00001\n",
        "# EPOCHS = 10\n",
        "\n",
        "# worked for ResNet-152\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 100\n",
        "DEVICE = get_default_device()\n",
        "LEARNING_RATE = 0.00001\n",
        "EPOCHS = 15\n",
        "\n",
        "birds_similar_8rank2 = \"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1\"\n",
        "birds_disimilar_8rank2 = \"datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar\"\n",
        "hsj_similar_8rank2 = \"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\"\n",
        "hsj_dissimilar_8rank2 = \"datasets/hsj/hsj_triplets/hsj_query_rank0_dissimilar\"\n",
        "frameinterp = \"datasets/distortions/distortion_triplets/frameinterp_triplets\"\n",
        "\n",
        "def get_train_dataset(IMAGE_SIZE=100):\n",
        "    train_dataset = HSJ(birds_disimilar_8rank2,\n",
        "                        \"jpg\",\n",
        "                        train=True,\n",
        "                        transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE))]))\n",
        "    return train_dataset\n",
        "\n",
        "def get_val_dataset(IMAGE_SIZE=100):\n",
        "    train_dataset = HSJ(birds_disimilar_8rank2,\n",
        "                        \"jpg\",\n",
        "                        train=False,\n",
        "                        transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE))]))\n",
        "    return train_dataset\n",
        "\n",
        "train_dataset = get_train_dataset(IMAGE_SIZE = IMAGE_SIZE)\n",
        "val_dataset = get_val_dataset(IMAGE_SIZE = IMAGE_SIZE)\n",
        "train_dl = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=2,pin_memory=True)\n",
        "val_dl = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=2,pin_memory=True)"
      ],
      "metadata": {
        "id": "2C1ZO1ylJtzz"
      },
      "id": "2C1ZO1ylJtzz",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet = ResNet_Triplet152()\n",
        "ResNet = ResNet.to(DEVICE)\n",
        "\n",
        "Optimizer = torch.optim.Adam(ResNet.parameters(),lr = LEARNING_RATE)   # non-frozen parameters\n",
        "\n",
        "# parameters = []\n",
        "# parameters.extend(ResNet.Feature_Extractor.heads.parameters())\n",
        "# parameters.extend(ResNet.Triplet_Loss.parameters())\n",
        "# Optimizer = torch.optim.Adam(parameters,lr = LEARNING_RATE)  # frozen params optimizer\n",
        "\n",
        "criterion = TripletLoss()\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)"
      ],
      "metadata": {
        "id": "VKd3wV-7MF4V"
      },
      "id": "VKd3wV-7MF4V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_mean_per_epoch = []\n",
        "val_loss_mean_per_epoch = []\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
        "    ResNet.train()\n",
        "    c = 0\n",
        "    iter = []\n",
        "    running_loss = []\n",
        "    for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):        \n",
        "        iter.append(c)\n",
        "        c += 1\n",
        "        anchor_img = anchor_img.to(DEVICE)\n",
        "        positive_img = positive_img.to(DEVICE)\n",
        "        negative_img = negative_img.to(DEVICE)\n",
        "\n",
        "        Optimizer.zero_grad()\n",
        "\n",
        "        anchor_out = ResNet(anchor_img)\n",
        "        positive_out = ResNet(positive_img)\n",
        "        negative_out = ResNet(negative_img)\n",
        "\n",
        "        loss = criterion(anchor_out, positive_out, negative_out)\n",
        "        loss.backward()\n",
        "        Optimizer.step()\n",
        "        running_loss.append(loss.cpu().detach().numpy())\n",
        "        print(\"Epoch: {}/{} — Loss: {:.4f}\".format(epoch+1, EPOCHS, np.mean(running_loss)))\n",
        "        # plt.plot(iter, running_loss)\n",
        "\n",
        "    ResNet.eval()\n",
        "    running_loss_val = []\n",
        "    with torch.no_grad():\n",
        "      for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(val_dl, desc=\"Validation\", leave=False)):\n",
        "          print(\"Validation: \")        \n",
        "          anchor_img = anchor_img.to(DEVICE)\n",
        "          positive_img = positive_img.to(DEVICE)\n",
        "          negative_img = negative_img.to(DEVICE)\n",
        "\n",
        "          anchor_out = ResNet(anchor_img)\n",
        "          positive_out = ResNet(positive_img)\n",
        "          negative_out = ResNet(negative_img)\n",
        "          \n",
        "          loss = criterion(anchor_out, positive_out, negative_out)\n",
        "          running_loss_val.append(loss.cpu().detach().numpy())\n",
        "          print(\"Epoch: {}/{} — Loss: {:.4f}\".format(epoch+1, EPOCHS, np.mean(running_loss_val)))\n",
        "        # plt.plot([1,2], running_loss_val, color=\"red\")\n",
        "      \n",
        "    print(len(running_loss))\n",
        "    print(len(running_loss_val))\n",
        "    train_loss_mean_per_epoch.append(np.mean(running_loss))\n",
        "    val_loss_mean_per_epoch.append(np.mean(running_loss_val))\n",
        "    # print(len(running_loss))\n",
        "    # print(len(running_loss_val))\n",
        "    # print(running_loss)\n",
        "    # print((running_loss[0]))\n",
        "    # print([x[0] for x in running_loss])\n",
        "    # print([x[0] for x in running_loss_val])\n",
        "    # plt.plot(np.arange(0,len(running_loss)), running_loss)\n",
        "    # plt.plot(np.arange(0,len(running_loss_val)), running_loss_val, color=\"red\")"
      ],
      "metadata": {
        "id": "bvqN6a2jMHWV",
        "outputId": "6e83f96f-9826-43b0-b48e-867b1673ba3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514,
          "referenced_widgets": [
            "8a37de47d89b4f698eac7644dfdbc567",
            "19e037905af34eee848cf04d4af5754c",
            "7b09bcfe79ea4e12ab069f7b15f7cbe4",
            "80990932e3a340d799e43d33e033fc3a",
            "9f6b3a0532b44eafb5fdb9fdaba184b7",
            "6e73af0ba41444d69e514de760620e34",
            "211111650c3f4d1183f2b98169d058ac",
            "6f471e3898354de18f5c33bcca1c8d3c",
            "b0fab6beb33945b3bfb03ebddc5c430a",
            "83a11934a9c74410afdb860a15c7a81e",
            "a57393348a614e509c635b460930f908",
            "989373861fc740228308265601ec5b43",
            "d0da2093d9dd464ebe309ad18df186be",
            "1e9f10c0e55a4a498d82acd03158932f",
            "41b92fca9733439b8095bb378f8b2269",
            "00e3fd75243a4997b53529dd2ec556c6",
            "c5b8d537f780483d9841096034c9998f",
            "52040a39ae0042fdb2bbe983227433f8",
            "6e44466fb9ef47da9593fc3ed4f0f046",
            "23f82c9a0a8b46f1b6f0b6ff1dba1312",
            "82416691ed514d4bb524c675192af724",
            "81a9f870cbe2473ab4f773e9431b5095"
          ]
        }
      },
      "id": "bvqN6a2jMHWV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a37de47d89b4f698eac7644dfdbc567"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "989373861fc740228308265601ec5b43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f813c53bbc5a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0manchor_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpositive_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnegative_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-cbbc748f115c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m                   nn.Linear(10,2))\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature_Extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtriplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTriplet_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtriplets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 308.00 MiB (GPU 0; 39.56 GiB total capacity; 36.58 GiB already allocated; 120.56 MiB free; 37.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"resnet_hsj_similar_loss.npy\",np.array(train_loss_mean_per_epoch))\n",
        "np.save(\"resnet_hsj_similar_loss_val.npy\",np.array(val_loss_mean_per_epoch))\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(np.arange(0,len(train_loss_mean_per_epoch)),train_loss_mean_per_epoch)\n",
        "plt.plot(np.arange(0,len(val_loss_mean_per_epoch)),val_loss_mean_per_epoch, color=\"orange\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sCluxc6L19MO"
      },
      "id": "sCluxc6L19MO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(ResNet, 'ResNet343_FINAL_birds_disimilar.pth')\n",
        "torch.save(ResNet.state_dict(), 'FINAL_FROZEN_Vit_b_32_birds_dissimilar.pth')"
      ],
      "metadata": {
        "id": "16c-sj-fJIX5"
      },
      "id": "16c-sj-fJIX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
        "    running_loss = []\n",
        "    for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):        \n",
        "        anchor_img = anchor_img.to(DEVICE)\n",
        "        positive_img = positive_img.to(DEVICE)\n",
        "        negative_img = negative_img.to(DEVICE)\n",
        "\n",
        "        Optimizer.zero_grad()\n",
        "\n",
        "        anchor_out = ResNet(anchor_img)\n",
        "        positive_out = ResNet(positive_img)\n",
        "        negative_out = ResNet(negative_img)\n",
        "        \n",
        "        loss = criterion(anchor_out, positive_out, negative_out)\n",
        "        loss.backward()\n",
        "        Optimizer.step()\n",
        "        running_loss.append(loss.cpu().detach().numpy())\n",
        "        print(\"Epoch: {}/{} — Loss: {:.4f}\".format(epoch+1, EPOCHS, np.mean(running_loss)))"
      ],
      "metadata": {
        "id": "PaFXotOheE_7"
      },
      "id": "PaFXotOheE_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "um_similar = np.load(\"umbrella_species_similar.npy\")\n",
        "um_dissimilar = np.load(\"umbrella_species_dissimilar.npy\")\n",
        "species_similar = np.load(\"species_similar.npy\")\n",
        "species_dissimilar = np.load(\"species_dissimilar.npy\")\n",
        "\n",
        "dict_umbrella_species = {'Parulidae':'red', 'Icteridae':'blue', 'Passeridae':'orange'}\n",
        "dict_species = {'Yellow-headed Blackbird\\n':'red', 'Tree Sparrow\\n':'green', 'Scott Oriole\\n':'orange', 'Fox Sparrow\\n':'magenta', 'Harris Sparrow\\n':'navy',\n",
        "     'Hooded Oriole\\n':'blue', 'Wilson Warbler\\n':'yellow', 'Kentucky Warbler\\n':'gray', 'Hooded Warbler\\n':'pink', 'Chipping Sparrow\\n':'brown',\n",
        "     'Magnolia Warbler\\n':'purple', 'Bobolink\\n':''}\n"
      ],
      "metadata": {
        "id": "yjDZI_tJt6IV"
      },
      "id": "yjDZI_tJt6IV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns (positive distance, negative distance)\n",
        "ResNet.eval()\n",
        "x = 8"
      ],
      "metadata": {
        "id": "8F7VF7A6NXWG"
      },
      "id": "8F7VF7A6NXWG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "um_similar = np.load(\"species_similar.npy\")\n",
        "dict_umbrella_species = {'Yellow-headed Blackbird\\n':'red', 'Tree Sparrow\\n':'green', 'Scott Oriole\\n':'orange', 'Fox Sparrow\\n':'magenta', 'Harris Sparrow\\n':'navy',\n",
        "     'Hooded Oriole\\n':'blue', 'Wilson Warbler\\n':'yellow', 'Kentucky Warbler\\n':'gray', 'Hooded Warbler\\n':'pink', 'Chipping Sparrow\\n':'brown',\n",
        "     'Magnolia Warbler\\n':'purple', 'Bobolink\\n':'black'}\n",
        "\n",
        "############################ REQUIRES MODIFICATION FOR EACH MODEL \n",
        "\n",
        "# scaler = transforms.Resize((224, 224))\n",
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                  std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()   # used to convert the PIL image to a PyTorch tensor (multidimensional array)\n",
        "############################\n",
        "\n",
        "\n",
        "############################## HOW TO GET MODEL:\n",
        "# # Load the pretrained model\n",
        "# model = models.resnet18(pretrained=True)\n",
        "# model.cuda()\n",
        "# # Use the model object to select the desired layer\n",
        "# layer = model._modules.get('avgpool')\n",
        "# # Set model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "##################################\n",
        "\n",
        "def get_vector1(image_name, model):\n",
        "  with torch.no_grad():\n",
        "      scaler = transforms.Resize((224,224))\n",
        "      \n",
        "  #     # Load the pretrained model\n",
        "  #     model = models.resnet18(pretrained=True)\n",
        "  #     # Use the model object to select the desired layer\n",
        "  #     layer = model._modules.get('avgpool')\n",
        "      \n",
        "      # 1. Load the image with Pillow library\n",
        "      img = Image.open(image_name).convert('RGB')\n",
        "      # 2. Create a PyTorch Variable with the transformed image\n",
        "      t_img = to_tensor(scaler(img)).reshape(1,3,224,224).cuda()\n",
        "      output = model(t_img)\n",
        "\n",
        "  return torch.flatten(output)\n",
        "\n",
        "############################ COSINE SIMILARITY\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "########################### GET FEATURE VECTORS\n",
        "def get_image_embeddings(path, image_type, model, isHSJOrBirds):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        #print(im_no_name)\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector1(path+\"/ref/\" + im_no_name, model))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector1(path+\"/p0/\" + im_no_name, model))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector1(path+\"/p1/\" + im_no_name, model))\n",
        "\n",
        "        # now load decision\n",
        "        if isHSJOrBirds:\n",
        "          feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "          decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "          decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "          if decision[0] <= 0.5: \n",
        "                feature_tensor_dict[\"decision\"].append(0)\n",
        "          else: \n",
        "                feature_tensor_dict[\"decision\"].append(1)\n",
        "      \n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    plt.scatter([x[0].item() for x in feature_tensor_dict[\"ref\"]],[x[1].item() for x in feature_tensor_dict[\"ref\"]], color=[dict_umbrella_species[x] for x in um_similar])\n",
        "    plt.show()\n",
        "    print(feature_tensor_dict[\"ref\"][0])\n",
        "    print(type(feature_tensor_dict[\"ref\"][0]))\n",
        "    print(feature_tensor_dict[\"ref\"][0][0].item())\n",
        "    type(feature_tensor_dict[\"ref\"][0][0])\n",
        "    return feature_tensor_dict\n",
        "\n",
        "\n",
        "########################## GET MODEL PREDICTIONS\n",
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(100):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ########################## GET MODEL PREDICTIONS\n",
        "def get_predictions_distance(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "  \n",
        "  # dist = (tensor1 - tensor2).pow(2).sum(3).sqrt()\n",
        "  for image_no in range(100):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(torch.cdist(embeddings[\"ref\"][image_no].unsqueeze(0),embeddings[\"p0\"][image_no].unsqueeze(0))**2)\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(torch.cdist(embeddings[\"ref\"][image_no].unsqueeze(0),embeddings[\"p1\"][image_no].unsqueeze(0))**2)\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "########################## GET MODEL ACCURACY\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(100):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (300 - number_wrong_predictions)/(300)\n",
        "  return accuracy\n",
        "\n",
        "def get_average_reference_similarty_for_dataset(predictions,embeddings):\n",
        "  similarities_p0_p1 = []\n",
        "  similarities_p0_ref = []\n",
        "  similarities_p1_ref = []\n",
        "  \n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "  for image_no in range(100):\n",
        "      similarities_p0_p1.append(float(cos(embeddings[\"p0\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "      similarities_p0_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0))))\n",
        "      similarities_p1_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "\n",
        "  similarity = np.average(similarities_p0_p1)/(float(np.average(similarities_p0_ref)+np.average(similarities_p1_ref)))\n",
        "\n",
        "  return similarity\n",
        "  \n",
        "\n",
        "def apply_framework(path, image_type, model, isHSJOrBirds):\n",
        "  embeddings = get_image_embeddings(path, \n",
        "                                    image_type,\n",
        "                                    model,isHSJOrBirds)\n",
        "  predictions= get_predictions_distance(embeddings)\n",
        "  accuracy = get_accuracy(predictions, embeddings)\n",
        "  print(path + \": \" + str(accuracy))\n",
        "  return accuracy\n",
        "\n",
        "# def apply_framework_model(modelf, layer):\n",
        "#   scaler = transforms.Resize((224, 224))\n",
        "#   normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                   std=[0.229, 0.224, 0.225])\n",
        "\n",
        "#   ################ HSJ Rank 0 + Rank 1\n",
        "#   apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\",\n",
        "#                   \"jpg\",\n",
        "#                   modelf, layer, scaler, normalize, True, feature_tensor_size)"
      ],
      "metadata": {
        "id": "gf0ATYd9QvKy"
      },
      "id": "gf0ATYd9QvKy",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.rand(100).unsqueeze(0)\n",
        "t2 = torch.rand(100).unsqueeze(0)\n",
        "torch.cdist(t,t2)**2\n",
        "for i in range(700, 1000):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "fLM3dxlWTMWu"
      },
      "id": "fLM3dxlWTMWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd content\n",
        "model1 = ViT_Triplet()\n",
        "model1.load_state_dict(torch.load('FINAL_Vit_b_32_birds_similar.pth')) # path of your weights\n",
        "model1.eval()\n",
        "model1.cuda()\n",
        "\n",
        "model2 = ViT_Triplet()\n",
        "model2.eval()\n",
        "model2.cuda()"
      ],
      "metadata": {
        "id": "wNyiUYEQQZvZ"
      },
      "id": "wNyiUYEQQZvZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework(birds_similar_8rank2, \"jpg\", model1, True)"
      ],
      "metadata": {
        "id": "MhcAC6Lu1DJ1",
        "outputId": "0d2936d2-b86a-4eb9-de55-a71de6d36a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "id": "MhcAC6Lu1DJ1",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.1117, 0.0634], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.1117461919784546\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_rank1: 0.71\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.71"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework(birds_similar_8rank2, \"jpg\", model2, True)"
      ],
      "metadata": {
        "id": "tdLnnENfQ9uz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "866890e2-75f4-4044-9cdb-8fb10cec5282"
      },
      "id": "tdLnnENfQ9uz",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1798, 0.0595], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.17984336614608765\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.8166666666666667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8166666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework(birds_similar_8rank2, \"jpg\", model2, True)"
      ],
      "metadata": {
        "id": "cuCxzxHNwmsU",
        "outputId": "7fe06891-7a4e-4e4a-e094-06d5e33d81c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "id": "cuCxzxHNwmsU",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAGvCAYAAABGl3QgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACUfElEQVR4nOzdd3hUZdrH8e9MOiUJgUAoQUCkI1UgqAiCEMGCYkFhFRZBXcECFlh3RfBV7FgBEQVdYVFQUAFBhMVGqEovioC0hBaSEEL6vH+MBEKmnCRzZibJ77PXXJLz3Oece7KQ5M7znPux2Gw2GyIiIiIiIlJqVl8nICIiIiIiUl6owBIREREREfEQFVgiIiIiIiIeogJLRERERETEQ1RgiYiIiIiIeIgKLBEREREREQ9RgSUiIiIiIuIhKrBEREREREQ8RAWWiIiIiIiIh6jAEhERERER8ZBAMy+enJzMqFGj+Prrr7FarQwYMIA333yTKlWqOD0nMzOTMWPGMHfuXLKysujTpw9TpkyhVq1aAGzevJkXX3yRn376iRMnTtCgQQMeeOABHnnkkYJrrFq1ih49ehS5dmJiIjExMYZyz8/P58iRI1StWhWLxVLMdy4iIiIiIuWFzWbj9OnT1KlTB6vV9RyVqQXWoEGDSExMZPny5eTk5DB06FBGjBjBnDlznJ7z2GOPsXjxYubNm0dERAQjR47k1ltv5eeffwZg48aN1KxZk08++YTY2FhWr17NiBEjCAgIYOTIkYWutXv3bsLDwws+rlmzpuHcjxw5QmxsbDHfsYiIiIiIlFcHDx6kXr16LmMsNpvNZsbNd+7cSYsWLVi/fj0dO3YEYOnSpfTt25dDhw5Rp06dIuekpqYSHR3NnDlzuO222wDYtWsXzZs3JyEhgS5duji810MPPcTOnTtZuXIlcH4G69SpU0RGRpYo/9TUVCIjIzl48GChIk1ERERERCqWtLQ0YmNjSUlJISIiwmWsaTNYCQkJREZGFhRXAL169cJqtbJ27VpuueWWIuds3LiRnJwcevXqVXCsWbNm1K9f32WBlZqaSlRUVJHjbdu2JSsri1atWvHss89y5ZVXOs03KyuLrKysgo9Pnz4NQHh4uAosEREREREx9OiQaU0ukpKSiizJCwwMJCoqiqSkJKfnBAcHF5l1qlWrltNzVq9ezaeffsqIESMKjtWuXZtp06bx+eef8/nnnxMbG0v37t355ZdfnOY7adIkIiIiCl5aHigiIiIiIsVV7AJr7NixWCwWl69du3aZkWsR27Zt4+abb2b8+PH07t274HjTpk25//776dChA127duXDDz+ka9euTJ482em1xo0bR2pqasHr4MGD3ngLIiIiIiJSjhR7ieCYMWMYMmSIy5hGjRoRExPDsWPHCh3Pzc0lOTnZaSe/mJgYsrOzSUlJKTSLdfTo0SLn7Nixg549ezJixAj+9a9/uc27U6dO/PTTT07HQ0JCCAkJcXsdERERERERZ4pdYEVHRxMdHe02Li4ujpSUFDZu3EiHDh0AWLlyJfn5+XTu3NnhOR06dCAoKIgVK1YwYMAAwN4J8MCBA8TFxRXEbd++nWuvvZZ7772X559/3lDemzZtonbt2oZiRURERERESsK0JhfNmzcnPj6e4cOHM23aNHJychg5ciQDBw4s6CB4+PBhevbsyccff0ynTp2IiIhg2LBhjB49mqioKMLDwxk1ahRxcXEFDS62bdvGtddeS58+fRg9enTBs1kBAQEFhd8bb7xBw4YNadmyJZmZmcyYMYOVK1fy7bffmvV2RUREREREzN0Ha/bs2YwcOZKePXsWbDT81ltvFYzn5OSwe/duMjIyCo5Nnjy5IPbCjYbPmT9/PsePH+eTTz7hk08+KTh+ySWXsH//fgCys7MZM2YMhw8fplKlSlx++eV89913DjcfFhERERER8RTT9sEq69LS0oiIiCA1NVVt2kVEREREKrDi1AamtWkXERERERGpaFRgiYiIiIiIeIipz2CJiIiIVGi5uZDyK1gCoHp7X2cjIl6gAktERETE03JzYUlzSN9T+Hh4C4jfDIH6EUykvNISQRERERFPys2FeZWLFlcAaTvg8wjv5yQiXqMCS0RERMSTVsWDLdv5eF4G/HC79/IREa9SgSUiIiImSQLmAWt8nYh3HVvpPubQAvPzEBGfUIElIiIiHrYOqAHUBu4A4rA/9v2AL5PyIiNbjOaZnoWI+IYKLBEREfGgdUBn4ORFx/OA94DrvZ6RiIg3qYWNiEh58Nt+SDxx/uPgQIhr66tspEJzV0AtBX4DmnghFx8JDIfcNNcxwdW9k4uIeJ1msEREyrrvNxQurgCyc/86fsw3OUkFdRhINhB3r9mJ+FbHt93HxH1ifh4i4hMqsEREyrIfNrge/+2Ad/IQAeB/BuP2mpqFzzW6Bxq4KCKbPAx1472Xj4h4lQosEZGy6uxZY8/Sr9tqeioidvUMxoWamoVf6DoLbvgdqjYBS5D9Fd4SbjoIHd/0dXYiYiI9gyUiUkIZGRl8//335ObmEhcXR40aNbybwJ6DxuLOZpmbh0iB7th/d5vvJu4R81PxB+GN4cbdvs5CRLxMBZaISDHl5OTw6quvkp19fiPRX375BavVyoMPPui9QivTxUamIj7zN+AjF+PBwGgv5SIi4n1aIigiUgw5OTm88MILhYqrc/Lz83n33XdJTU31TjKxMd65j0ixzAKudTIWDOzyXioiIj6gAktEpBg++cR9568ZM2Z4IRMgxuBMWetG5uYhUsQKYCvQAfuGw7HAS0AW0NCHeYmImE9LBEVEiuHAAfdd+dLT072QyV/q1YJDR52PW4CoKK+lI3JeK8BNl0sRkXJIBZaISFl2aSxYrXAgsehYUAB0bef9nERERCowFVgiImVdw7r219mzcCAJIqoaXz4oIiIiHqUCS0SkGCpVqkRGRobLGKvVR4+3hoVBUz3fIiIi4ktqciEiUgz33HOP25g+ffp4IRMRERHxRyqwRESKoVatWvTu3dvpeJs2bejUqZMXMxIRERF/oiWCIiLFFBcXR8eOHfnvf//LgQMHsNlsREdHM2zYMIKCgnydnoiIiPiQCiwRkRIICgoytFxQREREKhYtERQREREREfEQFVgiIiIiIiIeogJLRERERETEQ1RgiYiIiIiIeIgKLBEREREREQ9RF0ERERHxjuxsWHY5nP4NsIG1Elz9BdTV5twiUn5oBktERETMt+NVmB8Cp3cDNvux/Az4Ph4W1PNpaiIinqQCS0RERMx1Nhk2PeFi/DAs7+61dEREzGRqgZWcnMygQYMIDw8nMjKSYcOGkZ6e7vKczMxMHnroIapXr06VKlUYMGAAR48eLRRjsViKvObOnVsoZtWqVbRv356QkBAaN27MrFmzPP32REREzJe4EjaOtf+3rFraxn3M8e/Nz0NExAtMfQZr0KBBJCYmsnz5cnJychg6dCgjRoxgzpw5Ts957LHHWLx4MfPmzSMiIoKRI0dy66238vPPPxeKmzlzJvHx8QUfR0ZGFvx537599OvXjwceeIDZs2ezYsUK7rvvPmrXrk2fPlrnLSIiZcDqIbD/o/Mf737J/t8Gf4OuH3s9nRUrVrBhwwby8/OpVq0agwcPpkqVKsZOPnvIWFx2OgQbvKaIiJ+y2Gw2mxkX3rlzJy1atGD9+vV07NgRgKVLl9K3b18OHTpEnTp1ipyTmppKdHQ0c+bM4bbbbgNg165dNG/enISEBLp06WJP2mJhwYIF9O/f3+G9n3rqKRYvXsy2bdsKjg0cOJCUlBSWLl1qKP+0tDQiIiJITU0lPDy8OG9dRESkdFbdDEe+cj5e+wbo8bVXUjly5Ajvv/++w7FLL72UwYMHu7/IHIuxm8Vvh6gWxchORMQ7ilMbmLZEMCEhgcjIyILiCqBXr15YrVbWrl3r8JyNGzeSk5NDr169Co41a9aM+vXrk5CQUCj2oYceokaNGnTq1IkPP/yQC+vEhISEQtcA6NOnT5FrXCgrK4u0tLRCLxEREZ9wVVwBJC7yShq5ublOiyuAP/74g8WLF7u/kMXgghlnxVXyFphXzV6ozbHCwgb257pERPyQaQVWUlISNWvWLHQsMDCQqKgokpKSnJ4THBxcaLkfQK1atQqdM3HiRD777DOWL1/OgAED+Mc//sHbb79d6Dq1atUqco20tDTOnj3r8N6TJk0iIiKi4BUbG1uctysiIuIZ/7vRWNzK683NA/jyyy/dxmzYsMH9hVqMcx9jCXZ8fHFb+zNcOSl/HbBBxp+woDqsf8T9dUVEvKzYBdbYsWMdNpm48LVr1y4zci3w73//myuvvJJ27drx1FNP8eSTT/LKK6+U6prjxo0jNTW14HXw4EEPZSsiIlIMJ52vtigct87cPLAv9zciJSXFdUCbiWANcx3TZ33RY2tHQOpm5+f8/hYcM/j5EhHxkmI3uRgzZgxDhgxxGdOoUSNiYmI4duxYoeO5ubkkJycTExPj8LyYmBiys7NJSUkpNIt19OhRp+cAdO7cmeeee46srCxCQkKIiYkp0nnw6NGjhIeHExbm+At8SEgIISEhLt+XiIiI6QIqAyfdxwVWMj0Vo49pX/x926GBGbDgEjh7oPBxSyD02QhRlxc9548Z7m++qg/coWX9IuI/il1gRUdHEx0d7TYuLi6OlJQUNm7cSIcOHQBYuXIl+fn5dO7c2eE5HTp0ICgoiBUrVjBgwAAAdu/ezYEDB4iLi3N6r02bNlGtWrWCAikuLo4lS5YUilm+fLnLa4iIiPiFK+fCd13dx8XNNj2V4OBgMjMz3ca5+iVoIbf8af/vn/Mg4xA0uBfColycYKDAyz1t7N4iIl5iWpv25s2bEx8fz/Dhw5k2bRo5OTmMHDmSgQMHFnQQPHz4MD179uTjjz+mU6dOREREMGzYMEaPHk1UVBTh4eGMGjWKuLi4gg6CX3/9NUePHqVLly6EhoayfPlyXnjhBR5//PGCez/wwAO88847PPnkk/z9739n5cqVfPbZZ8YexBUREfGlmnH2WR1brvMYSyDEdDM9lfj4eBYuXOgyJigoiNDQ0OJd+JLbS56UiIifM3Wj4dmzZ9OsWTN69uxJ3759ueqqq5g+fXrBeE5ODrt37yYjI6Pg2OTJk7nhhhsYMGAA3bp1IyYmhi+++KJgPCgoiHfffZe4uDjatm3Le++9x+uvv8748eMLYho2bMjixYtZvnw5bdq04bXXXmPGjBnaA0tERMqGAWdw/i3aAgNOeSWNNm3aULlyZZcxhtq0i4hUIKbtg1XWaR8sERHxuY1j4bfX7LNZlkC4bCR0nOz1NKZOnVrkueqAgADuvvtuGjVqZN6N50VATjGer+owBZo+aF4+IlJhFac2UIHlhAosETFky7Nw8AsIqwtXzYPgKr7OSMQUubm5rFu3jjNnztC6dWvjz12Vxtlkezv24qh9I/Rws4+YiEgxqcDyABVYIuLS2hHwh4MNWIOqwe3aAFXEY9KT4OtLwJZt/JxbMsBJ12ARkZIoTm1g6jNYIiLl0vpHHBdXADmnYK6x9tn/+hfcfDO87+RShWRmQsIm+OkX2P674VRFyrwqMXBXFtx2Ghr9w9g5S0xctijek74fFl4CcyznX59VhT/n+zozEZc0g+WEZrBExKk5FvcxcXOh4Z0Ohzp3hnUO9oh94AGYOvWig5mZsHab43tUC4fLm7jPRaS8+KoppP9mLPZu/XhTpqVshyWtnI+3/Ld9A2sRL9EMloiIWQ4sNBa3dqjDw82aOS6uAKZNg+HDL76Ok+IK4FQa7NhjLB+R8sCW7+sMxFuWdXQ9vv05yHWxlYGID6nAEhEpjoPzjMXlZxU5lJgIu3e7Pm3GjAs+2LXP/X2OpxjLR6Q86DzTWFxILXPzEHOd2gJ57je45mftpyb+SQWWiEhxVO9iLM4SUORQjx7GTn3++b/+cPSksRNOphiLEynrYq4yFtfXwC8nxH/99o6xuOM/mpuHSAmpwBIRKY5mo4zFNXm4yKHDh42dumxZMfIB44WYlF05ObC8O3wWAfOiYOvzbk8pt3o7WWN7Tsz16iBY1llDjMVZ9GOs+Cf9zRQRKa5qV7iP6fBqkUOVjDUXJDa2mPlUjyjmCVKmrB4C84Lh+PeQm2bvVLn1X/ZmK8fX+Do776txBdx0CCzBRcfavgHXLvF6SuJhrccbi2vwN3PzECkhdRF0Ql0ERcSlLxvDmT8cj92UaG8tfZHPP4fbbnN/6exsCAoCNu2C1HT3J1zj5mFwKbu2Pm8vply5/dxfGOOO//orB5Yvp3rr1jS4/vpSJChikvk1INvN7Lw6RYoXaaNhD1CBJSJupSfB8k6QeRSswdDmBbdLCENC7AWUM5deCnsubAz4/QbXOVQOg44tjecsZcucQCDPdUz1rtDnZ0OX2zlrFr++8kqR4zU7daLXTIMNJES8ITcT5lUFm5NOgd2+hno3eDcnqdBUYHmACiyRMujsWdiwA/Iv+rLWuglE+ce/44wMiIy0P1JzsVq1ICnpooOu9sEKDoK4Np5OUfyJkT3XsMDd7tuXb//wQza/9prT8fDGjbnhyy+LkZyIyXJzYfVdcGghkAtYIKI19FgGlYquEhAxU3Fqg0Av5SQiYq7kNNjqZAPSrb9B/drQsK53c3KgUiX7DNbs2TBqlL0mjIqCxYuhbVsHJ4SG2pcA7jsIB47ajwUFQPvm9jEpv3IyDAYa+z2pq+IKIG3PHtKPHKFKnToG7ytissBA6GZwawwRP6ICS0TKB2fF1TkHEv2iwDpn0CD7y7CGsfaXVBxBBruiUHRLgIvt/+YbQ1f6YdjN9P1mvcH7ioiII+oiKCJlX+IxY3G/7jA3DxFPsxqYpbzkTrch+xcvNnAzGyHWY/BFDOS5ee5LRESc0gyWiJR9fxwyFpdmdMlVGbZxO6SfLXws0GpvhBFicG+ZcirlUArkQmSDSF+nYtzVX8D3fV0EWOHK2W4vUznG2PMqwSH59qYtyzpA303GchQRkUJUYIlI2adWPXY/bARHfYty82HNVujSusIVWbm5uczoNIOjvx4tdLxGixrcv/l+AgP9/Ntg3euhyyewZnDRsYBKcOtxQ5dp+9RT/P7f/7qNax53wv6HlM2QfRaCtWGviEhxaYmgiJR9dWsaiwt1sDFpebF7n+Pi6kIbtnsnFz/ySrVXihRXACd2nODFqi+Sm+ukBbQ/aTTIvt9Pl0+gxpUQ0xtu+hPuPGP4Oa2goCAq13X1DKKNqlHZ1Kibef7Qnimly1tEpIJSgSUiZV+jesbiOl9ubh6+lORmQ06wz2RVIJ8P+pzsdOebjuVl5jH3hrlezKiUGg2C3j/BtcugSv1in37zt98SFB6Ofcr3whdUicymz7CLNs7OTSttxsZ92RTmWGFOAHzbzXv3FRExgZ+vjRARMahRPdjr4lmsalW9l4v4he2fup+x++PbP9zGlCe3JySQ9OEN7FmyidOnggkOzaN53EnqNDpTNPiSgeYn9P3NcPirCw7Y4MSP9v2/Wk2Ay58xPwcREQ9TgSUi5UNsjH0J4I69jseMznJJuWHLM/BwXgV8fi/m3i+ICXXzLF5QJEQ0NzeRLRMvKq4usm081LsJotqam4eIiIepwBKR8iM6Cq6J8nUWIv4tIBhaT4StzmaHLNDre/Pz2Pas+5hlXeCuTPdxIiJ+RAWWiEh50DgW9hx0HVOpYnUQDI0KJTPZ9Q/nwVXKceMTV1r/G6peChtGQXby+eNVGkOPpfYxo3JzYfd+OJkK2CAwEJo1hKgINycamWHMMp6HiIifUJMLEZHyoG4t1wWUxQJXtC74cNs2uPJKaNQI4uJg0ybzU/S2mz68yW3M9W9f74VM/FSDu+G2k/YOhXfm2v970+/FK65S0uDnTXAixd7F0gbk5MLW32HtFpMSFxHxbyqwRETKiyta22eyLhZdDbp1KPiwdWv7a/Vq2LcP1qyBdu2gSRPIy/NiviZrfnNz2g9v73S81d2taDukrfmJ5ORA9+5gtdoLXYsFmjWDEyfMv7dRAQHFPyc3Fzb/5nw8Mxs27yp5TiIiZZTFZnO3cUrFlJaWRkREBKmpqYSHh/s6HRERj+jcGdatcz7eqhVs3eq9fLzh5J6TzOk3h5S9KdiwEREbwV1f30XNlgb3TyuNEycgOtr5+LffwnXXmZ+HGXbuhWPJ7uOu6ej4+JwAwM3WAUFRcLuBLQhERExWnNpABZYTKrBEpLxJTYXISPdxhw6Byz1pxbjQUMhy8xxRWf02/OMvkG9gb7XWlzl+HivpJ1h5tetzbz1r/xyKiPhYcWoDLREUEakgnnjCWNzo0ebmUWFs2+a+uAIYN878XMxgtDDMdLLZc8xV0HWe8/P67lNxJSJlkgosEZEKIjHRWNzx4+bmUWE8/LCxuPfeMzcPswQbbEQc5WKT7wa32ZtrdJwOoXWgUn3o/p39WGQDj6QpIuJtKrBERCqIrl2NxbV33hdCiiPbyczNxYwss/NHzQ10G7RajM1CNRkOtx6G/n9CnZ6lz01ExIdUYImIVBBPPmlvYOfOSy+Zn4srf6z4g2Wjl3FwnZt9vfzd448bi7vazXNI/iqiClQJcx1zeVPv5CIi4kfU5MIJNbkQkfLo6afhhRecjz/8MLz5pvfyudCs7rP48/s/ixxvPbg1t/7nVh9k5AFWq/tnlbKzISjIO/mYYdseOJlS+JjVCpc3sRdhIiLlgN80uUhOTmbQoEGEh4cTGRnJsGHDSE9Pd3lOZmYmDz30ENWrV6dKlSoMGDCAo0ePFozPmjULi8Xi8HXs2DEAVq1a5XA8KSnJzLcrIuL3nn8eJk6EwIsenwkIsM9w+aq4erfluw6LK4Ctn2zlk76feDkjD1m82PX4mDFlu7gCaNXY3oq9QzNo0QiubAtXt1dxJSIVlqkzWNdffz2JiYm899575OTkMHToUK644grmzJnj9JwHH3yQxYsXM2vWLCIiIhg5ciRWq5Wff/4ZgLNnz5KamlronCFDhpCZmcmqVasAe4HVo0cPdu/eXajCrFmzJlarsZpSM1giUt4tWgTr10PbtnDLLb7LIzMzk5fC3K9LHG8b74VsTLB+PfTsCadPnz8WHAzvvAPDh/suLxERMcwv9sHauXMnLVq0YP369XTsaN9kcOnSpfTt25dDhw5Rp06dIuekpqYSHR3NnDlzuO222wDYtWsXzZs3JyEhgS5duhQ55/jx49StW5cPPviAv/3tb8D5AuvUqVNEGtn0xQEVWCIi3vFGgzdI/TPVbVzTm5oy8MuBXshIRESkML9YIpiQkEBkZGRBcQXQq1cvrFYra9eudXjOxo0bycnJoVevXgXHmjVrRv369UlISHB4zscff0ylSpUKCrILtW3bltq1a3PdddcVzICJiJ86fBQOHDG2b5CUK2mH0wzF7Vu1z+RMRERESs/gJhbFl5SURM2aNQvfLDCQqKgop89CJSUlERwcXGTWqVatWk7P+eCDD7j77rsJCzvfyah27dpMmzaNjh07kpWVxYwZM+jevTtr166lvZP+w1lZWWRd8INdWpqxb/giUgpZWbB2W+EmAPuO2P/bpTWEhPgmL/Eqa4CVvNw8t3GBYaZ9yxIREfGYYs9gjR071mmTiXOvXbt2mZFrEQkJCezcuZNhw4YVOt60aVPuv/9+OnToQNeuXfnwww/p2rUrkydPdnqtSZMmERERUfCKjY01O30RWbPVeYe1NVs1m1VBdP+/7obiBn6l5YEiIuL/il1gjRkzhp07d7p8NWrUiJiYmIKufufk5uaSnJxMTEyMw2vHxMSQnZ1NSkpKoeNHjx51eM6MGTNo27YtHTp0cJt3p06d2LNnj9PxcePGkZqaWvA6eLCM778i4u82bncfs26b+XlUCD8BwYDlgldlwD86q171+FXugywQ20m/+BIREf9X7PUW0dHRREdHu42Li4sjJSWFjRs3FhRAK1euJD8/n86dOzs8p0OHDgQFBbFixQoGDBgAwO7duzlw4ABxcXGFYtPT0/nss8+YNGmSobw3bdpE7dq1nY6HhIQQouVIIt6TftZ9TL626Su9iYCj7nsZQG3gR8BAgWOyR/Y9wpsNnfeIfyrjKS9mIyIiUnKmLWhv3rw58fHxDB8+nGnTppGTk8PIkSMZOHBgQQfBw4cP07NnTz7++GM6depEREQEw4YNY/To0URFRREeHs6oUaOIi4sr0kHw008/JTc3l8GDBxe59xtvvEHDhg1p2bIlmZmZzJgxg5UrV/Ltt9+a9XZFRPyUu9bmVwO+L2QjG0Qy3ja+8GbDFmg+oDl3zLvDt8mJiIgUg6lPDM+ePZuRI0fSs2dPrFYrAwYM4K233ioYz8nJYffu3WRkZBQcmzx5ckFsVlYWffr0YcqUKUWu/cEHH3Drrbc6bMOenZ3NmDFjOHz4MJUqVeLyyy/nu+++o0ePHqa8TxER/3SdwbhZwBDz0iiGIauG+DoFERGRUjF1o+GyTPtgiZjshw3GJk6u6eg+RpwIBNx354OawFGTcxERESm7/GIfLBERl5o2dB8TXc38PMo1o78/yzc1CxERkYpEBZaI+Eat6hAV4Xw8NBhaXOq9fMolo7N//zY1CxERkYpEuzaKiO+0vsy+19X67ZD31yyKxWI/Xk1Lc0tvLfaW7O48bHYiIiIixh1dBZv/DWcPQVA4NBkFjf4O1rIxN6QCS0R8KyQErmrv6yzKseHA+y7G53krEREREdfy82FFDzj+Q+Hj64bD5n9Cvx0QWsM3uRVD2SgDRUSkhKYDMyk6kxWIfYbrNq9nJCIi4lDC4KLF1TlZx2FpB+/mU0IqsEREyr0h2BtZ2C545QCdfJiTiIjIBXKz4cBnrmMyDsBRJwWYH1GBJSIiIiIivnV4AdgMbC3y21vuY3xMBZaIiIiIiPhWdqqxuNwMc/PwABVYIiJSZnXt2hWr1YrVaqVBgwZkZmb6OiURESmJmJ7G4mp0NTcPD1CBJSJiVFaW/SU+N3HiRCwWCwkJCdhsNmw2G3/++SdhYWH07dvX1+mJiEhxVb0UKjd0HWMJgJZjvZNPKajAEhFxJSsLfvoFvt8Aa7baX99vsB/zQ1lZWaQdOODrNEy1bt06xo8f73T8m2++4ZlnnvFiRiIi4hHXfA0WF7tIdZwCVv/fZcpis9lsvk7CH6WlpREREUFqairh4drwVKTC+n6D6/FrOnonDzdWDh9O0urVhQ9aLFzz3nvUvfJK3yRlktDQULLczCRaLBby8/O9lJGIiHhM2u+Q8Dc4uQ5711ugciPoMBnq3eS7tIpRG6jAckIFloiwZgtkZbuOCQmGLpd7Jx8nvrz+es64mLW68s03uaRXLy9mZC6L5eI9vRzTtzcRkTIsPxcykyC4BgSG+jqbYtUGWiIoIuKMu+LKaIyJ0g4ccFlcWfLzOXLPPWCx2F9Vq8KOHV7MUEREpASsgVCpnl8UV8Xl/4sYRUSEpC1JzL1pLmkH07DZbIRWC+XG929k20tDnZ5TNSuL7gcOUDUnBxtgAUhPh5Yt4fLLYfNmb6Uv4h++qAOZiec/DqwKNyRBpUq+y0lEyh3NYImI+LnF/1jMe23eI/XPVGz5NrBBZnIm8wbMIyPZ8TK4wLw8ev75J5VzcoC/iqsLbdkCo0aZm7hJatSo4TbGatW3N7nA6QMwx1K4uALIPQ0LK8P+Bb7JS0TKJX0HEhHxYwd+OsCGqc4bbeRmV8LRo0aNUlMJy811/UX+nXdKnZ8vHDx40G1MQkKC07GkJGjSBCIj7f9NSvJgcuKfvr7E9fjqW72Th4hUCCqwREScqVPTMzGlMO+OeS7HU1Ma4qjnQ2xamkkZ+V5oaCj79u1z2uxi4cKFdOrUyeFYzZpQuzb8/jukptr/W7s2GJgUk7LK6OzUD3eZm4eIVBgqsEREnLmsPoQGOx8PDbbHmCg9Md31eFoDcnODihwPyssruiywHGnQoAH5+fn8+OOPNG7cmPr16zNz5kxsNhs333yzw3Nq1YLjxx1f7+RJe/El5dCavxmLO/SpuXmISIWhJhciIq50vhyOnoTd+wq248ACNG8E0VG+zKzAwb29aNjkm0LHUkJCiMzKKve/Rbvqqqv4/fffAUhOts9GXbjkr3JlWL4cmjaFY8dcX+v4cfs1ovzj/1bxmDyDcWrrLyKeUd6/94qIlF6t6tCto31T4Ws62v/speIqMMz978Fyc6syICGBkAvWuR2sWtXtF/j/0Y3ISJg6tXQ5+oMdO6B69aLPU505A127QosWxq5z1VWez018rPYNxuJCYszNQ0QqDG007IQ2GhYRf/Djiz+yctxKlzF1OtVh+NrhRQccPKN07gt+BmFEcIo8QgAICLAvk4uIKG3GvmG14rDZR3FFRdk/D1LOzDGwYPZu/TgkIs5po2ERkXLi6rFXE17f+Rdya4iVoT872QsrMxNCi27QuI2WRHGyoLgCyMuzzwCVRStXeqa4Aqhb1zPXET9Tf7Dr8YjLvZOHiFQIKrBEpGzq0ME+Q3Ph6/bbfZ2VKR778zGa3tK0yGZWtdrUYlz6OAIDnSwjDAmBs2chM5OcCc/Ti6UEkcnlbCObsCLheXnw/vsmvAGTjR7tuWutXu25a4mfSD8Mh1x046x9I/TTptsi4jlaIuiElgiK+LGQEMjOdjxWty4cOuTdfLwsNzfXeVHlxOOPw2uvuY+LjIRTp0qWl6+0aWPfN7m06tSBw4dLfx1xLDc3lyFDhvD555+Tk5NDWFgYzz77LGPGjDHvpvs+hYSBzsfj5kLDO827v4iUG8WpDVRgOaECS8RPtWsHmza5jhk/Hp591hvZlBk33giLFrmPCwuDjAzz8/GkRYvs78+d66+HrVsd19+u6vKFCxeyefP5GY42bdrQv3//kiVbQe3evZvmzZvj6EeOqlWrkpycXOxfGhiiZ69ExENUYHmACiwRP+Vkc9lCgoKcz3BVUB9+CMOGuY+LjYUDB8zPx9OMNLk4N56eDt27w8GD9ve7ahVUqVI0Pjs7m0mTJjm93rhx4wgOdrFPmhQIDAwkL895u/R69epx8OBBz950/SPw+1vu4y57GK5407P3FpFyR00uRKRiy8nxdQZ+5+9/Nxa3fLm5eZjl559dj//73+f/XKUKbNgAR4/a/+uouAJ48cUXXV7T3bjYvfvuuy6LK4BDhw6Rnu56U+1i2zPNs3F+4vTp0wwaNIjbbruN06dP+zodEXFABZaID2Vm2vsyXHFF2WwuIGXLk0+6Hq9f374hb1kUFwd//ml/huxCISHw9dcwcWLxrrd3716Hy9kuZLPZ2Lt3b/EuXAEZLUSfeOIJz97Y6AKdMrKQ5/Tp01gsFsLDw5kzZw6ff/454eHhWCwWFVoifkYFloiPVKlif95l/nz7b9FHjLCvfvNkR7QKq1IlX2fgl156yXmRdfnl9gKlLKtf396gw2Y7/8rMhBsM7jN7oYULF3o0riLLysoyFJeSkuLZG9eO92ycD50+fdrlkqTw8HAVWSJ+RAWWiA8EBcGZM47HJk9WkeWSkeYC06ebnkZZ9dJL9sJj1iz7p/LJJ+0fb1aX6kJyc3M9GleRtW/f3lDcvffe69kb9/jKs3E+FHnx1GwJY0TEO1RgiXjZlCng7meyyZO9k0uZtGCBvZ+2M9dcA4MGeS8fXzh7Fn7cCN9vOP9aW7w+5ffea/9UvvSSSTn6k9xcGDwYIiLsU8ft20NSkstT6tWrZ+jSsbGxnsiwXDMyyxcQEEB8vAkzSU3c/LbK3bifyM/P90iMiHiHCiwRLxs1yljcN9+Ym0eZdvgwPP20fSrwnEqV4JNP7C3hyrMDibBuO+Rf9NxIZra90Dp71jd5ufHnT3/yf2H/xwTLBCZYJvBc8HNs+2yb+Tf+8kv735PZsyEtzT51/OuvULs23Ol8/6O7777b0OXvuusuT2VaboWGhrqdnfr888/NuXnH16DjDCdjM+zjIiIeZlqBlZyczKBBgwgPDycyMpJhw4a57RA0ffp0unfvXvDQpqP12Eauu2XLFq6++mpCQ0OJjY3l5Zdf9uRbEykVo79knOajxlYZGRl88sknTJs2jd9++803SRjxf/9nb8V+7mGbM2fK/8wVwD43O+Gu2+6dPIrhg6s+YNbVs8jLPN9JLj8nn8/v/JzJsSZO16akuF5S+tlnMGGC0+HmzZu7vLy7cTlv1qxZPPvss0X2uqpcuTLffvstN998s3k3bzLMvtfVxa8mBvYtEBEpAdP2wbr++utJTEzkvffeIycnh6FDh3LFFVcwZ84cp+e88cYbZGZmAvb9RU6dOlVkTbG766alpdGkSRN69erFuHHj2Lp1K3//+9954403GDFihOH8tQ+WmMXINk5g/7nvmWfMzaXoPR3/sNmnTx+6dOni3WSkqF92wGkDuwC3bQER/tHoY+MHG1l0n+sdjpvf0Zw7Pr3D8zfv3BnWrXMd42bPtMWLF7Nhw4Yix9u3b8+NRnY3liLS09P57bffaNGiBaGhob5Op0ywGPzGoa1NRczj842Gd+7cSYsWLVi/fj0dO3YEYOnSpfTt25dDhw5Rx9XzE8CqVavo0aNHkQLLyHWnTp3K008/TVJSUsEGkGPHjmXhwoXs2rXL8HtQgSVmufJKWL3afZy3v086K67O6devX8G/O1+YOtW+l1Fysr1IjY2F996DPn18lpL3fV/0B32HggMhrq2pqRg1MXAitjz3f5nH28Z7/uaBgeBm/yXA0D+2o0ePsnfvXho1akStWrU8kJyIcf379+fLL790GdOnTx+WLl3qpYxEKh6fbzSckJBAZGRkoR/GevXqhdVqZe3ataZeNyEhgW7duhUUV2D/orN7925OnTrl9NpZWVmkpaUVeolc6IMPICDA/sP9ha+nnireddxtiAoQE1OyHEvqP//5j9uYxYsXeyETx+Li4B//gJMn7T8L5+fbW4rHx8MDD3joJnv22Dcle+ghD13Qhy5+PsuHjBRXpjG6HtdAJ8BatWoRFxen4kp8YuHChS67BEZERKi4EvEjphRYSUlJ1KxZs9CxwMBAoqKiSHLTuam0101KSiryDfDcx67uPWnSJCIiIgpe6gwlF3rhBbjvPsc/r738Mtx2W/Gu9913zscqV4bExOJdr7TcbpZqs78mhE5g1cRV3kipwLhxsGaN8/H33oMffijFDVassFfKl11m35RsyhT7xyEhpbioSQIMfsmuV9N9TEVgdPnZRc8F+YP8/HzS0tLUBl4KnDp1ilUOmvgsWbLE83uIiUipFKvAGjt2LBaLxeWrOMvw/Mm4ceNITU0teB08eNDXKYkfefpp1+PFbYDVs6d9Jub6688/kxUcbC+83PSC8Ygvv7Tfz2IBqxWys938gGn56xUG34//ngmWCSTtKvkvS4rjzTfdxwwr6bPqP/0EvXo5HsvONvbA3Obdhdulb/29hMkY0MFgU4VL6pqXQ1liZHrZ29PFbpw4cYL333+f5557jsmTJ/P888/z1ltv8fvvJv69kjLjmmuuwWazFXpdf/31vk5LRC5SrAJrzJgx7Ny50+WrUaNGxMTEcOzYsULn5ubmkpycTEwpvpkZuW5MTAxHjx4tFHPuY1f3DgkJITw8vNBLBOCf/zQWd8UVxb/2kiX2WTGbDbKy7IWX2UJC7I3VcnLsH9tssHVra/Lz3RQTNiD1/IfvNX/PrBQLMdJ1fN++El786qvdx/To4fh4Vpa9oEo5Xfh4cqr9eFZWCZNyISwMwtzMrNWv7fn7lkLdru6LvbCoMHNuPn48uNt8NSHBnHuXwNGjR5k6dSpHjhwpdPzUqVPMmTOHTZs2+SYxEREplmIVWNHR0TRr1szlKzg4mLi4OFJSUti4cWPBuStXriQ/P5/OnTuXOFkj142Li+OHH34g59xPj8Dy5ctp2rQp1apVK/G9peKaOdNY3Jbi7fPqExERjhumbd/eCqvVxbMyNiD/r/9eYErbKZ5Mzz8521drzVbX57kbL6lOraFaVcdjl8ZCQ/+avbrv5/uwWF0X748cecS8BE6dAkft1KtUgV27oEED8+5dTHPmzHG5WezXX3+tzWRFRMoAU57Bat68OfHx8QwfPpx169bx888/M3LkSAYOHFjQQfDw4cM0a9aMdRe00E1KSmLTpk3s2bMHgK1bt7Jp0yaSk5MNX/fuu+8mODiYYcOGsX37dj799FPefPNNRo8uG7u1i/+5oF+KS1Y/37Y7M9O+z6oj+/Y15ODBeuTlOfhB+FxR5aA/zfHNxz2WnzNGPv+1SzJp89fXlRJJOmEs7kRKye/hyuVN4ZqO9lenluf/XM8/GzA8k/eMw1mqgNAAxmaOJcTs59127LBP2U6dCpMmwcGDcPo0NG1q7n2L4fjx426bK+Xn55PgRzNuIiLimGlP9s6ePZuRI0fSs2dPrFYrAwYM4K233ioYz8nJYffu3WRknN/TZdq0aYVaRXfr1g2AmTNnMmTIEEPXjYiI4Ntvv+Whhx6iQ4cO1KhRg2eeeaZYe2CJXGjWLLj2WvdxDz9seiql4mofT5vNwpw5d3P77Z/RqNH+80XVuXrrd+Bbc/Nz5t574f33Xcdc8CXAuLBSLEvbc8BY3O59UKNdye9jRGnehxc9efJJAI7/cZyc0znUaet6uw6PCwz0YMtJz3PbaOYvej5YRMT/mbbRcFmnfbDkQkZ6Hfj7v6SmTeG339zHXXHFGvr1XQbZQArwH8BF4w1T9i+6yKWXgrOfP/v1g0Wu97F1zsj/sSEh9um/C/2w0dj/4QFWuKp9yXKTCmXTpk1u9zkCaNmyJbcVt22piIiUms/3wRIpby7qrVLEypXeyaM0BgwwFrdlSxfGPzseJgFTcVlcNbi2gQcyc++PP2DkyMJdtyMjYfLkUhRXAK1auY/Ztq3osSoGZ43CqxQvH6mwWrVqhcVAwV+a55hFRMQ7VGCJGBAdbZ/EuPzywsfr1bMfd9Zozp+88IKxuHN7dHUc1dF1IHDvintLkVHxvP02/LbyILu//o2cs7mcOgWPPlrKi27dClWdNIwAGDECGjcuerx9C2PXv7xJyfIq8/4HXAnEAd/4OJeyITAwkGbNmrmMiYyM1B6NIiJlgJYIOqElglIe3XwzfPWV8/GwMLjgsUg+u/0zds7fWTTQAk8lP0VopMGNXEvp/c7vc2Rd4dbVlWpU4sGdD1KlhgdmiZYsgZtugrw8+8dRUXDokOvnmzbtglQX03tREdD6stLnVqZsA9oCeRcdtwIb/xoTZ/Lz8/nwww85fPhwkbFKlSrx0EMPUalSJR9kZp7s7GxefvlltmzZQo0aNRg7diz169f3dVoiIkUUpzZQgeWECiwpr5wVWVFRcPKk43M+HfApe7/bS2BwILcvuJ0GVzUwNccLvVLzFTKOZzgetMCYY2M8U2SVxPY/4MSposdrRUGzRt7Px6dOANFuYv4E9MOzO3v37uV///sfp0+fJjQ0lE6dOtG2bVus/t6qtJief/55nnnmmSKt57t3787y5csJDDStD5eISLGpwPIAFVhS3r30Evz3v3DZZTBvnq+zceyH53/gf//6n8uYyAaRPLLPxH2UjEhLh5Q0qBYFVb0zq2ea2bNh+PDCOzz37g3Llrk5sRHgbsfnOkDR2RmpeN555x1GjRrldPzKK6/kp59+8mJGIiKuqcDyABVYIr73QuUXyMnIcRvnjU6GFcLtt8P8+Y7HgoIc71JdwEBHRqDIbtVSIYWHh3P69GmXMdu2baNly5ZeykhExDV1ERSRciHnrPviSjxk927nxRXYN+pt2NB7+Ui5lZCQ4La4AgrtiykiUpZogbOI+C2LxYIvJtmXPbmMNa+sKXK8+/Pdueaf15h45y7AWgB2ftWMDVMep/sLvYlt74XOcd27u4/Zv9/sLKQCMLqp8jF3+2OIiPgpFVgi4l/27IHx46FePaJbNODYNtc/ZFkCjS5NM+aj6z5i/3f7HY6tenoVJ387ya2zbvXoPeFdYCQA3z7Ri/XvdiL3bDBwiL3LPgAs3PrfW2k9sLXhK+6eO5dfXngBW14eWCzU6daN7lOmOD/h6FFjF962zcn+YTWwN7pwJcLYPdx6BpgHxAOTPXRN8ZY2bdoYimuoGVMRKaP0DJYTegZLxMuWLIF+/Qodyge+YADbcV5YXDnuSnq90MtjaUywuF+W5NlnvjKAygB8eusd7FrQAvtzShcWjvaPjRZZ/23TBltursOx3vPnU6N586IDVisY+Xbw66/Qtq2DgW3g4v8nux+Aq93fw6mGwH4Hx6MAJy0wK5jExETuvfdeUlJSuPvuu3m01JvFmaNmzZocP37cZczhw4epU6eOlzISEXFNz2CJSNmyYkWR4grsX6Bu43Ou5nuHp13a51KPFldvN33bUNx/4v/jsXuea21+aG0ddi04V/hcPCtn//iLu75we7XPOnfGlptLblAQafXrkxYbS94F7a6/ve02xydWr24sXYfFFUArYJaLE6dQuuKqFo6LK4BkoHztD1VcOTk5hIeHU6dOHZYvX8769et57LHHsFgsvP76675Or4j333/f5fjAgQNVXIlImaUCS0R8r5frIqkH/6Nd9b0EhAYQEBxAZKNIHtz2IIOXDvZoGsm/JRuK27fcXTvy4rDv8fXdU70NRaekpDgdy8rKIiszkwPXXsu+/v052qULR+Pi2HvLLRy65hry/9pHaY2j5gFLlri/eUyMm4B7sc+2PYR9VqkacN9fxx50f32X3D2PcxZw3zihvKpcubLTxhFjxoxxW9B4280338z8+fOL/BY4ICCABx98kP/+978+ykxEpPS0RNAJLREU8SKLweeoTP5yNcE6wVAXcUughWdynvHQXe3vfXL9x0g7GI7bdudPwOD7B3PppZcWGVp6//2sr16d/ODgop9Tm42As2e5ZNEiAgMCuGvLlqLX7tXLPpvoiNUKmZn2du1eVw9j+2eFAJkFH3XrBj/+eH7UYoH+/eEL9xOBZcq4ceN48cUXXcYEBASQ62TZqK+tWLGCdevWERMTw9/+9jdtMCwifklLBEWk7Nizx9cZFGh8U2NDcV2f6OrxeweGGmxJHwqffPKJw6EtViv5ISGOC1aLhbxKlTjVvDm2/HzH1/7uO3j55aJFVNu2PiyuAI4YjMsq+FPVqoWLK7DX5wsWQL16nsvMHxhZApiXl8eJE+6akPhGz549GTduHEOHDlVxJSLlggosEfGdOcC0ur7OosCghYMMxbl67mv94fU88s0jPPf9cwZnDOxF3eWDt+B29iocCLD/cd26dUWGz9as6XqWz2YjrXFjAitXdh7zxBP2DYVttvOvX3/1YXEFxf1W1bcvpKc7Hz98GP75z1Km5EdycowV5z/99JPJmYiICKjAEhFfqI69lhgEvBYG3IbbL0dhYaanBdDnrT4ux+/65i6Hx5fvWU7wc8F0mtGJt9a9xTOrniHo+SBavNPCzR1/B+Dqf/5IYFg2jtco/nXsuvNHVq5cWTTManW93NJiIS84mN6zZ7vJyd98bTDuSQCWLnUf6Yd9H0osICDAUFz79u1NzkREREAFloh40ulM+OkX+H6D/fXDBjh+UeOIIOxN3wp5GvvUjIvi4PffPZqqM11GdeHBvQ8W2V/LGmzl8aOP0yS+SZFzftj/A71n9yYnv+hMws6TO6nxcg03dz2DNdDG8A3vEVzl3DI32wUvC3THfRd0cDuDZcnPJ7KxsaWQ/uN6g3EvAcYe1cvKch9TVtx+++1uYywWC/Xr1/dCNiIiosXOIuIZW36DU2mFj9mAHXsh+CDEtYF/Ag5XzbUFvgLuBNKKDo8YAXW9t5SwZsOaxWpi0W9O0RbzFzp59iQf/PIBw9oPcxJRCbBRs8Uv3Lx2EGve7cyRxfXIywqCGOwzVxet6uvSpYvjS7lpGGIzaalfUlISXbp0ISUlha5du7LESFfCYknDvkbSGSNNMLzL0V5PNWvW5KjRTZ0NmjNnjtuue6NGjfLoPUVExDnNYIlI6R09WbS4ulB2Dmz9HVw2OovH3sxgyvlDUVGQkQHvveeZPE2QmZtJeo6LB37+8vjyxw1crT0tWuzkYM2G5A0NggeA/hQprgC6d+9e5Fi3bt1cX95i4dZbbzWQR/EEBQVRu3Zt/vzzT1JTU/nmm2+wWCx07erJZiBVsVfsV110vOVfx8/vmRQS4v5qUVGey8yRgIAAhxvpHjt2zPCSvuLYs2cPVqvjb+n9+/fnzTff9Pg9i2P+/Pn06tWLfv36sX37dp/mIiJiNhVYIlJ6uw3sC5WcaqAFemXgQZj+V3OFkye99uxVSW04ssFQXHq2+yLsHKezU3/p37+/w+M9evSgYcOGTs9r3bo1rVsbWWdonNVqddrMIyEhwWEhWDo/Unj55LYiEU8+6f4qZm4L1bx5c/KddWoE8vPzufzyyz16z0svvZS8vDymT59OzZo1iYyM5OqrryY7O5sFCxZ49F7FsXTpUgICArj99ttZsWIFS5YsoVWrVlStWtXlnm4iImWZ9sFyQvtgSfnzOfAw9n2CWgHfYX8gygO+N1Zk0L2jsbgfKTpR4aeS0pOo/Vptt3GhAaGc/ddZw9fdvHkzX375JRd+ibZYLNxzzz00aNDA5blnz57lnXfeISPDvolxREQEjz76qOF7G/XWW2/xyCOPuI3zxbeZrl0hIcHx2B13wKefmndvi8F93cr7t9/169fTqVMnp+MBAQFkZmaqNbuIlAnFqQ1UYDmhAkvKjxPYH+TJczDWF1hc+lsYLbAGdISTBuLK2FelgIkB5Nucz1gA3N3qbmYPKGvd+1wLCAhwOVNzzosvvshTTz1V+hump8Mvu87//agcCh1bOQ2fP9/++N65iZKaNWHOHLj22tKn4ooKLLvq1auTnFyko00hAwcOdPv8mIiIP9BGwyJygZo4Lq4AlgBDvJeKkT2Fy+AmsKM6uW8g8NHNH5maw9nkZM66+WHW04wUVwBffPFF6W+2ehNs3FW4+D6TaS/uDzjeiPi22yA5GfLz7a+kJPOLKznPXXEF8Pnnn3shExER79K8vEi59hjup4M+AmaV7jbhlSHtjOsYqwUigX8DzzmJCQUOli4VX3gj/g32ndrHV799VWTMgoVN928ybRnUF926kXmy8LRgaPXq3PrDD6bc70IWi8XQLMyVV15Zuhtt3AE5LjZt3ncEosKhSpXS3Ue8zugmySIiZYlmsETKtSnuQwBYc/6Publw111Qpw5ccgm88Yb709s1dx/T6a+lXBOBs0CtC8aswPS/jpdRX971JcfHHKdTnU5EhkRSo1INJnafSP74fC6P8WxDg3PmtG5dpLgCyDx5kjkebmbhyAMPPGAo7vXS7uqbnuE+ZuOu0t3Dg4YPH+42ZuTIkV7IxP+Z0VFRRMTX9AyWE3oGS8qHAMDIMq7XgNHw9NPwwgtFh61WWLMGrrjC9WV++gXyLrqfBejc2ljvbDFsyW23kbJzp8uYyObN6Tt/vql5uHveqGHDhuzdu7d0NzH6jN81BpuoeEFsbCyHDh1yONagQQP27TPQebOMq1q1Kunprrtn9uzZk++++85LGYmIlJyewRKRvxgtanrDBx84Lq7A/gBLp06Qmen6Mle1t/+Q27IxNG0AXVpDt47lo7h6H2gMNAEW+jYVwG1xZTSmtE6dOuV0rGbNmqUvrtz8gO6vDh48yJQpUwrN0AQEBDBjxowKUVwBfPSR++cOFy1a5IVMRES8SwWWSLn2fwZiLEArGOW+UQM33mjstjUiIaaGfxZWSUlwzz32DgjLl7uPX4T9UzQC+AP4HbgF+1fPLSbmWUZERkZis9mYPn06ISEhWK1WqlevzqlTpzh69Gjpb1CGn6t68MEHyc3NxWazYbPZyM3NZdiwYb5Oy2tuvfVWXn31VYdjAQEBbN68mdDQUC9nJSJiPi0RdEJLBKX8CAWyXIz/H/A0GGktHRBgf0arLMrMhNhYOHGi8PHAQFiyBK67rug5W4A2bq57EojyUI7FMKdlS0Nxd2/fbnImXmBkiWCA1T6DKn5p1KhRLF68mKCgIMaNG8eQIUN8nZKISLFoHywPUIEl5UcOUANIczD2JPCS/Y9GCiyLxb5csCy66Ta4pgeknILXJ0HGRY0T1q0r+oxZZcBdf4U6wGEP5mnQnNat3f9/YbVy99at3knITL/tg0Q3G6j50fNXIiJS/ugZLBG5QBCQChwBrgEuB8Zjb9/+UjEvFeTh3Lxg+x5YtR5GPwUdO0OveFi8Cr5YWjiuT5+i5xpoXofjLZhM1+rBBz0S46/mzdtObOxkwsKeJ7zjXN78+pDzlvAdmnk3ORERERc0g+WEZrCkwqle3b4rqyvDh8P06d7JxxN27YOjJ8FmKzxDd+7jzLMQ363w8QsZmNSzn1fqTEvEVSdBb3QQNEurVlPYvv14keMWC+z8sh9Nz31JbhoLMbWKxImIiHiaZrBEpPiWLnU9HhRUtoorsBdXUHT547mPQ0LhltuxAanh4RzYu5fcMvSMWd/587nmvfewXtAowBoayjXvvVdmi6sbb/yvw+IK7PVvi/6LybuqnX1JoIorERHxQ6bNYCUnJzNq1Ci+/vprrFYrAwYM4M0336SKi45Q06dPZ86cOfzyyy+cPn2aU6dOERkZWTC+f/9+nnvuOVauXElSUhJ16tRh8ODBPP300wQHBxfENGzYsMi1ExIS6NKli+H8NYMlFdLKlfalchcXGVFRcPgweKjjV25uLsuXLyc5OZmGDRvStWtXj1y3kNQM2LTDZYjNZiM/J5spX39KcvXqBccbNWrEnXfeSXDVYMh2c59qgJuJPzHOap1QZCLxYo891oXXX3ewpFNERMQkxakNAs1KYtCgQSQmJrJ8+XJycnIYOnQoI0aMYM6cOU7PycjIID4+nvj4eMaNG1dkfNeuXeTn5/Pee+/RuHFjtm3bxvDhwzlz5kyRVrDfffcdLS/oslX9gh+eRMSJa6+FnBx7+/KpU6FSJXj1VYiJ8dgt3n//fY4cOf/g0p49e1i+fDlXXHEFffv29dh9SDLWItwSFExyVOE2gHv37uXNN9/ksc8eI7C/my+T60qaYPmy57s9zL5udqFj1iArjyU/5vIXaxdatWqf2+IKYO7cbSqwRETEb5kyg7Vz505atGjB+vXr6djR3tlp6dKl9O3bl0OHDlGnTh2X569atYoePXoUmcFy5JVXXmHq1KkFm1mem8H69ddfadu2bYnfg2awRDxvypQpHD/uePkXQKdOnbj++us9c7NjJ2Gn6w1dbTYbeTYbz//wjcPx9u3bc+OWG+FfTi4wF7izdGmWB58P/pxts7c5Hb//1/uJaeu+SF+wYCe33vqZ27gaNSpx/PgTxcpRRESkNHz+DFZCQgKRkZEFxRVAr169sFqtrF271qP3Sk1NJSqq6CY0N910EzVr1uSqq67iq6++cnudrKws0tLSCr1ExHNSUlJcFlcA69Z5cDqopvtZa4vFwpYk5z3Wt27dCk9jb2JxJ1AFqAo8fMExcVlcAbzX7j1D17n22gaG4lq0qGEoTkRExBdMKbCSkpKoWbNmoWOBgYFERUWRlJTksfvs2bOHt99+m/vvv7/gWJUqVXjttdeYN28eixcv5qqrrqJ///5ui6xJkyYRERFR8IqNjfVYniICc+fONRS3fv16z900NNjpkM1mI99m4+vftjiNycnJOf/BXOA09u3E3vRYhmXeq3VfdR8E7PzKcbfDC0VEhFGvnvsVA3Pm3Gronv4kLS2Nue3bM6dlS+a0asWhTZt8nZKIiJikWAXW2LFjsVgsLl+7du0yK9dCDh8+THx8PLfffjvDhw8vOF6jRg1Gjx5N586dueKKK3jxxRcZPHgwr7zyisvrjRs3jtTU1ILXwYMHzX4LIhXK6dOnDcXt37/fczftfDkEBDgcsgFvrV7h8nSLkc2XC3kM+5dVywWvurjvlFHWZADvA79w5sgZQ2csGLzAUNyaNX/HanX+eR86tA1160YYupa/mHP55SyKiyM/K8t+wGbjh0GDmHPBc8IiIlJ+FKvJxZgxYxgyZIjLmEaNGhETE8OxY8cKHc/NzSU5OZkYDzwsf+TIEXr06EHXrl2ZbqBtdOfOnVm+fLnLmJCQEEJCQkqdm4g4FhISQkaG+517Pd6Q5qp2cPYsrN9xfp+r6pF8+ftWUnOyXJ5avK9X7YFfHRw/AoRgn/4y1uzBf30DFG5E8uTJUL4b25Nf3r/C5Zm2PGOP+9atG8Hhw6Pp0eMjdu06UXC8UqUgJk7szpgxJnScNNGctm0hL8/5eMuW3L19u/cSEhER0xWrwIqOjiY6OtptXFxcHCkpKWzcuJEOHToAsHLlSvLz8+ncuXPJMv3L4cOH6dGjBx06dGDmzJlYre4n4TZt2kTt2rVLdV8RKZ3+/fszc+ZMt3HdunVzG1NsYWHQrUOhQ30a1WHbtm3k5+c7Pc14w40EHBdXF6oFGJvt8U8LgKJL80KrZXLDe4up3iSZ5U847+x35T+vNHynmJgq7Nz5EACpqWepUiWYACczkX7vwmWmTmx+7z3aXLDUXUREyjZTnsFq3rw58fHxDB8+nHXr1vHzzz8zcuRIBg4cWNBB8PDhwzRr1qzQQ+1JSUls2rSJPXv2APYHzDdt2kRycnLBOd27d6d+/fq8+uqrHD9+nKSkpELPdX300Uf897//ZdeuXezatYsXXniBDz/8kFGjRpnxVkXEoPr16xPqZh+tBg0aEBho2u4RhVSqVIl7773X4Q/uFouFm2++uRjPYhppGZ6BqUsFTwCtgHDstdznnr6B4+eezq2ijBuTQHj9U07P7v509xLdNSIirMwWVwuuu85Q3Pa33jI5ExER8SbTfpKZPXs2I0eOpGfPngUbDb91wTeRnJwcdu/eXWjJ0LRp05gwYULBx+d+kz1z5kyGDBnC8uXL2bNnD3v27KFevXqF7ndht/nnnnuOP//8k8DAQJo1a8ann37KbbfdZtZbFRGDxowZw2uvvUZmZmaRsdq1a3Pvvfd6NZ/69evzz3/+k9WrV7Nz505sNhsNGjSge/fuBZuXG2Ps+TJYAvQvfqLuXLw68TRwGxAEHAdK/cjSCZejFot99WWfV79l3h1FWyv2fqt3aRMok84eNbYXm4iIlC+m7INVHmgfLBHzHDx4kK+++orMzEyqVavGHXfcYXgzWv9ktBnGUozNdhVDL8BVr44AILe0NxkFvOMywmaDlP0RvNXosULHB3w2gFa3typtAmXSF927k+lma4Jz9ByWiIh/K05toALLCRVYImJcTexTRe6Y8OXWSG03GXi0NDd5HxhhMFbfUi5kpFPgZX//O1eMGeOFbEREpKR8vtGwiEjFstpAjIe7IwJMNRj379LeyL4Vhvtfx1Uq7Y3KHwPPj6m4EhEpX1RgiYiUWmNggItxK+6eYyoRo3syu+5Gb0heXljBs1YXs9nsr//8RzswX+zuLVvOdwJxNK6lgSIi5Y4KLBERg47uOEp6crqT0fnAZ9j3vLrQ1YDzfZBK5SaDcVVLf6sPPniDkyerAecLqguLrU8/vZ29ew+X/kbl0N3btnFDQkKh2awuL7+s4kpEpJzyTj9kEZEyKvlAMlObTiU3s3CniFptavHApgcuir79r5eX9DcY90npb5WUlMQ77zxCkyY7uOmmrwkNzcRms7J/f31mz74be8tCOHHiBDVq1Cj9DcuZ8PBw+2yWiIiUeyqwREScSD6QzNuXvO1w7Ojmo7xY7UXGnhrr5awuMgZ4zcV4JGB0v2QXzvVD+u23Frz6aguncampqSqwRESkQtMSQRERJ6Y2ddVFwkZWSia7GvSGb77xWk5FvAo87GSsLuB8799iMboBdP369T1zQxE/8NFHH2GxWApeVatW5cyZM75OS0T8nAosEREnLl4WWJi9ccH6P2tB375gtcKPP3onsYu9ib07+iSgK/Z+G2eAQ567xfXXu58GCwwMJCgoyHM3FfGhoKAghgwZUuhYeno6VapUKXJcRORCKrBEKrrDSfD9hsKvn3+FjAxfZ+ZTyXuSDcWdINr+B5sNunWDbdtcn5CYCA0b2jvLWSwQGAh//3sps/3LWOBn7P02PNwxvX379lSq5Pqif/fU+xDxsejoaHJznf+C5aOPPuLYsWNezEhEyhIVWCIV2e59sMfBNEduHqzfAacrbpFVJaaKobg0Lorr1s158NSpUKcO7N9//lheHsycCSEhkJNT/ES96IknnqBWrVpFjlutVkaMGEHt2rV9kJWI55044X5bhXr16nkhExEpi9TkQqSiysmBpJOuY37ZAdd09E4+fia4SrCBKAtg5Vda0Y6/Zq5OOXnoKScH/vEP55fKzobYWEhKKm6qXvXAA/bOib///junT5+mWbNmbme2RMqSffv2GYrL8fNfiIiI76jAEjHRkdWr+fmxx8hJt++dZAkIoMngwXR48kkfZwZs+c1Y3OkMqFoxf4Cu1qoap7a57xKxiP7nCyxnrrvO/Q2PHrUXYmXgOabLLrvM1ymImGKL2umLSClpiaCISX56/HFWDR9eUFwB2PLy2P3RR8yLi/NhZn/JyDQWdzDR3Dz8WJPrmhiKyyfAbUzm+vV8e911vH/ffcwYNow1XbqQ7yjwjTeKlaOIeFavXr18nYKIlHGawRIxwYlt2zjgonV3Tloay+6+mz5z5ngxqxKyVtzfw1RvVr34J4WGFjk0c+ZMDjz+uP1z+dd+UodjY1nRsyf3zpxJvSNHzgcnG2uuISLmqFy5sqG4Hj16mJyJiJRVFfcnJxETrbr/frcxJzdv9kImLlQLNxZ3acV9kPuKEVcYiLJQifOzlMyaVWj022+/5cCBA/aOgXC+eyCQGxjIzGHDyLiwKHvY2aZWIuItsy76d+zIypUrzU9ERMokFVgiJshOSTEU59OHpFsZeIbGai0TzwOZyRJocTFqA2w8zOv2D8ePhzvvLBSRkJDw14UcXMdiIT8ggO/OPZ8VHAx+2okvOzubXr160aRJE+0BJOXevffey3vvvedwzGq1YvtrJlpExBEVWCIV2WX1XY93ae2dPPzYMznPuBxvZ9lIyIBb7Uv/nn22+Dew2djdtKn9z376cH3t2rUJCQlhxYoV/P7773z00UdYLBZ69uzp69RETDNixAhsNhtr1qyhW7duDBo0CJvNRl5enq9TExE/p2ewREwQEBJCXlaW27ggX88O1akJkVVg0++F92AKrwKtLq3ws1fnjLeNZ2b3mRz4/sD5gxa4/5cHiGkbU7qLWyxkBwfDnj1w6aWlu5YJqlevTrKT58JWrlxJz549WbFihZezEvGezp078/333/s6DREpQ1RgiXjYnj17SLzhBgJ27CAwI4Oqhw5hdfAbzzAHG7b6RKVK0LWNr7Pwe0NXDTXt2rnBwX5ZXG3ZssVpcXVOaZ5DuesumDu38LF+/WDRohJfUkRExOdUYIl4SG5uLq+99hqZmZkQEACtWgFwvH17ojdtIuLCzSstFm5YtsxHmYo3BQcHk52d7TKmfn03SzV95Doje3cBDz30EO+++26xrh0ZCampRY8vXgyVK8OZM8W6nIiIiN/QM1giHlJQXJ3zV7c4W2Agx664grTYWAAqx8Zy+6+/+n55oHjF/QY6Sg4dat7sWGmkGGzW8sMPPxTrunfd5bi4OicjA665pliXFBER8RuawRIppbfeeotTp045D/ire9zRuDjuW7SIwED9s6tIoqKiGDVqFO+++y75+YW3Fg4KCuLxxx/3UWbuhYWFuZ19A2h6rkmHQRcvC3SkmDWbiIiI37DY1GvUobS0NCIiIkhNTSU83OB+QVLhTJgwoVjxbdu25eabbzYpG/F3ycnJJCQkEBwczDXXXENwcLCvU3Jp4cKF3HLLLW7jsrKyivVeHHWsd0TfnURExF8UpzbQr9JFSujtt98u9jmprtZFSbkXFRVFv379fJ2GYf379ycoKMjlfm0NGjTw+0JRRETEm/QMlkgJueuu5kiDBg08n4iIidLT050+L1irVi32Xdi8RURERFRgiXhTt27dfJ2CSLGc64K4dOlSoqKiCA0NpV69evz5558kJSWV6Jo33eQ+pmPHEl26QklPz+bSS9/EYpmAxTKBoKDnmDw5wddpiYhUeCqwRLzkiiuu8HUKxbZr9mx+evxx9i1e7OtUxMf69OnDyZMnOXv2LAcPHixVa/kvv7Rvv+ZMUBCsX1/iy1cIkycnULXqJPbuTSk4lpubz+jR31K16gu+S0xERFRgiXhD586d6du3r6/TMOzbe+5hTsuW/PLCCxz45hsSnnySOS1bsvrpp32dmpQTZ85A9+5Fj19xBRhoXFihZWdnM3r0t07H09NzaNz4TS9mJCIiF1KTC5ESqlmzJseOHXMZExERwaOPPuqdhDxk0c03k7Znj8Ox/QsXkpeRwdWTJ3s5KymP/vc/X2dQNrVv/77bmD/+SDE/ERERcUgzWCIl9OCDD2Jx0W/aYrGUueIqOz3daXF1zsFvnf/mXETMt2PHCUNxy5b9bnImIiLiiAoskVJ45plnqFOnTpHj9erV45lnnvFBRqWz7M47DcWtLYPvTaS8MLo/2IXPZ4mIiPdoiaBIKQ0fPtzXKRSWDtwEbAPCgAnAEGOnnjHYFS5x7doSpSYXmDMHRo6EtDT7zrsdO8I330BkpK8zEz8XFhbI2bO5buPuvLO5F7IREZGLaQZLpDy5HagK/A84DhwAhgLBwCH3pwcY3DA2REVA6TRuDIMGwalTkJcHubmwZg1UqwazZvk6O/Fzc+bc6jYmMNBCVFQVL2QjIiIXM63ASk5OZtCgQYSHhxMZGcmwYcNIT093ec706dPp3r074eHhWCwWUlJSisQ0aNAAi8VS6PXiiy8WitmyZQtXX301oaGhxMbG8vLLL3vyrYn4p0eB+U7GcoBL3F+i07PPGrpV96lTjeVUTP/4x2JCQ/+PgIAJBAc/x+23f0Zurvvf1Jcp8fHwxx/Ox4cOhcxM7+XjZ2bNmkVAQEDB1/fKlSuzf/9+X6flV/r3b06tWi763AMJCcO8lI2IiFzMtAJr0KBBbN++neXLl7No0SJ++OEHRowY4fKcjIwM4uPj+ec//+kybuLEiSQmJha8Ro0aVTCWlpZG7969ueSSS9i4cSOvvPIKzz77LNOnT/fI+xLxW2+7Gc8HxrgOuaRPH7C6/rIQWKkSYVFRxUrNnfT0TIKDJzJ16gaysvLIz4ecnHzmz99JcPDz7N+f4tH7+dSyZe5j4uPNz8MPRUZGMnToUPLz8wuOZWRk0LBhQ2644QYfZuZ/kpKeoFOnos9/BgZaWb/+Pjp2rOuDrEREBMBisxl9XNa4nTt30qJFC9avX0/Hjh0BWLp0KX379uXQoUMOmwJcaNWqVfTo0YNTp04RedFSpAYNGvDoo4867c42depUnn76aZKSkgj+a7nT2LFjWbhwIbt27TL8HtLS0oiIiCA1NZXw8HDD54n4xG6gmYG4ytif0XIhOz2d+V26OHyS3hIUxF2bNpUgQdciIiaRluZ886OgICvZ2f/2+H29bs8euOwyaI59xtEKvAVsvSguOBiysrydnU9dccUVbNiwwWXMd999R8+ePb2UUdlx4EAy+/en0aVLnYLveyIi4lnFqQ1MmcFKSEggMjKyoLgC6NWrF1arlbUeeDj+xRdfpHr16rRr145XXnml0BKihIQEunXrVuibTJ8+fdi9ezenTp1yes2srCzS0tIKvUTKjO0G4wxs4BpcpQp3b9tGywcewBIYCBYL1pAQOv3f/5lSXO3efcJlcQXnZrN2ePzeXpeywv5s3HZgBHAfsBk4CTS4IO6CGZyyaxYQAVj+eoUBzzmNdldcAVx//fWeSa2cqV8/im7dGqi4EhHxE6Z0EUxKSqJmzZqFbxQYSFRUFEkGu5Q58/DDD9O+fXuioqJYvXo148aNIzExkddff73g3g0bNix0Tq1atQrGqlWr5vC6kyZNYsKECaXKTcRnuhmMq2r8km1GjaLNBctvzTJ6tIElc8Azz6zktttamJyNCW4BlgMh6XBklL3hyIXbp1mAasDvQA0gFYiO9nqantUHuHi/tEzgGeAT7FOuxZeTk1O6tERERLygWDNYY8eOLdJg4uJXcZbhlcTo0aPp3r07l19+OQ888ACvvfYab7/9NlmlXE4zbtw4UlNTC14HDx70UMYiXlADCDUQN9nsRIrvzBljPzRnZeWZnImHvY69eFoInAGe/jeE5BQurs6xYP911xd/fTx3rldSNMccihZXF/oNeNBLuYiIiHhfsWawxowZw5AhQ1zGNGrUiJiYGI4dO1boeG5uLsnJycTExBQ7SVc6d+5Mbm4u+/fvp2nTpsTExHD06NFCMec+dnXvkJAQQkJCPJpbeZKdnc3Ke+8lecuWgmPhl15K7/nztSzFX8wGBrgYjwLu8VIuxTBkSFu+//5Pt3F9+jT2QjYesoaiDUXu+RhsOC6wwD7WDWjfHroZnZL0Rw8ZiHkfMKcTpYiIiK8VawYrOjqaZs2auXwFBwcTFxdHSkoKGzduLDh35cqV5Ofn07lzZ4++gU2bNmG1WguWJMbFxfHDDz8UWkqyfPlymjZt6nR5oLiWnpTE/HbtChVXAGl//MH8du04sW2bjzKTQm4FPsLxv+pG2J/z8UNDhrTF4qzouMCUKf3MT8ZT+jo4Fn7aeXEF9rEA4IKvm2VTioGYorORgYHuf9+nZ7BERKQsMKXJRfPmzYmPj2f48OGsW7eOn3/+mZEjRzJw4MCCDoKHDx+mWbNmrFu3ruC8pKQkNm3axJ49ewDYunUrmzZtIjk5GbA3sHjjjTfYvHkze/fuZfbs2Tz22GMMHjy4oHi6++67CQ4OZtiwYWzfvp1PP/2UN998k9GjR5vxViuEr3r1cjn+7Z13eimT8uPzez9ngmUCEywT+L/Q/+P06dOeufA92H92XQgMBEYCZwEX2y75A3cbp06c2N07iXiKo346Z8Pss1SuGCg0yyt3y7ItFgtLlizxUjYiIiIlZ9o+WLNnz6ZZs2b07NmTvn37ctVVVxXaiyonJ4fdu3eTkZFRcGzatGm0a9eO4cOHA9CtWzfatWvHV199BdiX8c2dO5drrrmGli1b8vzzz/PYY48Vum5ERATffvst+/bto0OHDowZM4ZnnnnG7R5c4lhiQoLDdt0X2/Hhh17Ipuw7suEIEywT2Pbx+Vm/vKw8Xg9/nReqvuC5G90M/Bf73lhGns3ysYEDW/Ptt4OoWrXwctOwsEBmzryZf//7Gh9l5kFf3GIgKMj0NMxX2UBM0W89MTEx7Nu3D4uD6czw8PBCe2OJiIj4M1P2wSoPtA+W3VfXX0/6gQNu44LCw7k9IcELGZVtEyyuO1VWqVuFMYfc7AZcAWRm5hIaakqT02LZsgWGDIE//4SQEBg+HAw1G3U0ExWZDEfqQGhWkXGbjb+WSS4CytBSSIfeBh52EzMAmO90NDMzk7lz5xIbG6t9r0RExC/4fB8sKT/ysg1snATY8spYhzcfeKX2K25j0g+72QW4gvCH4qpnT2jTBn79FZKTITERJk6EoCBw+9hhcwfHUqKgzWY4XqPg0Llfb+XlWYB/U/aLK4BRQGsX4zVwVVwBhIaGMmTIEBVXIiJSJqnAEpca9e9vKC66fXtzEykHMpIy3AcBm2dvNjkTcWf4cFi50vFYbi60a+fmAs72zP29KXm1jrLlpums/imWNWvq8sADfQkKGg9MLEXG/mYLMJrCSx6twG3Yd1oWEREpv7RE0AktETxvTsuWbmPu3r7dC5mUbe6WB57TsE9D7lnqh/3UK5DAQHA3KfvMM26WCyYCDSA1O5OIvx6ESyWTqWxgPKvIvqiTns02vlQ5i4iIiHm0RFA8qu0Y188EXTrA1eZLUlydH/HsVgZSPBs3ui+uAGbMcBNQG8iC+kzmCt6nI9OJ4TXGsaJIcSUiIiLlhwoscavF3/9O50mTsAQEFB6wWLj8scfoPLE8LW0yT1SzKENxTa9vanIm4srhw8bisrKMxVWpE8wGjrCRRDLJdRhTs2Ylg9mJiIiIv9MSQSe0RFDM4G6ZYK32tXhg4wNeykYcSU6G6tXdx3XoABucPWt1EYub/9+1PNC3Zs2axXPPPYfFYuH555/nznK4t19iYipvvbWesLBAHn/8SipVKg9bAoiIeE9xagMVWE6owBIznD59mtfDX3c4FtEogkf/eNS7CYlDMTFw9KjrmK1boVUr49cMD5/E6dOFu3JWqRLE6dP/LEGG4gnLli0jPj7e4djq1auJi4vzckaed+BAKk2bvkNmZuHZ02rVQjl6dDRBQSq0RESMUIHlASqwxEzbPt/GF3d+gS3fRrXG1Xj4N3f7BpVtWVlZTGk8hbRDaQBYrBY6jepE/BuOf7j1tS1b7C3anendG5Yt814+4nk7duygpZsGPomJicTExHgpI887cSKD6Gjn20NYrZCXp9lTEREjVGB5gAosEc9YN3Ud3/zjG4djlgALz+Q+4+WMjNmyxV5IXTiTFRAA990H06b5Li/xjIiICNLS0lzGhEeHU2d8HZLPJlM9rDpvxr/JdZde56UMSy8m5lWOHj3jMiY+/lK++WawlzISESm7VGB5gAosEc9w99xZcEQw41LGeSmb4ktOhtWroV49aNvW19mIp1gsFgNBwEUTPLHhsex7eB8BFzf98UPunv2zx0B+vmaxRETcUZt2EfEL09q5n+rJTs12G+NLUVFwww0qriokB79+PJh2kFZTi/HwnT+rcxjbNf8jbkYco5eNJj073dcZiYiUCyqwRMQ0Rze56RTxl2+f+tbYBXcDMdhnFs69GgOpJUqvlDKB64B2wBxfJCBmczLJtevkLg6nGuzn74/CMmDITBjxPlz9I2sOr2HymsmETwrn7bVv+zo7EZEyTwWWiPhc2gHXz8IA8DnQDLi4ZvsDiAS2eTorV0KAMOA7YBMwCPtP4696MwkphbCwMPdB9ZwPjV0x1nPJmCQoyMG3eEs+DP4EYg/YPw7ILxiyYePhpQ/z1e6vvJShiEj5pAJLREwTWCnQUFzXp7q6D7rNzXhbQ7fyACvgbFnjE8AUbyUixbR8+XJuu+02RowYwfz5892fcIfzoRMZJzyXmEmef/7aogcb74G6RyDA+ePXY5aNMTErEZHyTwWWiC9lANUpvOTtbz7NyKMGLzPWnaxO2zquA54ycJE8YLmh25XCXTh8MKeQhzx6x2XLoHJlewfDkBB45x2PXr5C+PTTT7FarfTu3ZvPP/+c999/n379+lG5cmXHJwQA9wFVnV+ze4PuJmTqWU88cSW33da88MGW2yHP9bf+Paf2kJ+f7zJGREScU4El4ivvApWB5IuOf4LTZz/KmkuuuoTKMU5+iP3L7V/c7v5CHxq84dMG40psrsG4FI/cLTQU4uMhIwPy8yE7G0aNsu9flJXlkVuUe4sXL2bgwIE4aph75swZwsLCuPfee6latSpVq1ZlxIgRWP5tcbk80IKFp64yUvX73rx5d3D8+BM0aRJlXzIYdhas7oun3PxctzEiIuKYsfU7IuJ5I92MBwE53kjEXI8nPs70K6aTuCGx0HGL1cKdX91J035N3V/E6GYSecXPzxxLgLuLHE1OzuCtt9aSnJxJ5871uOuullitjn/PVbWq8yLKZoNKlSDPb96v/7rllltcjp89e5ZmzZoV2hOr3vf1eGaV8/3Znr/2eY/l5w01alRi9+5RAHR+fxHrjriOD7AEEBwY7IXMRETKJ+2D5YT2wRJTXQrsNRB3Bqhkci5elHYojcTNiTTo1YCQkBDjJz4IGNncdwHQv2S5GWN0anEn9o4cdvn5+dx4439ZsmRPoajQ0EDef/8GBg9uU+h4WhpERLi/y6efwh0unhOq6DIyMpwvA7xAaGgoZ8+eLXTsjTVvMPa7sWTlna9yQwNCefm6lxnVeZTHc/WW5X8sp/cnvV3GxF8azzeDHW8OLiJSUWmjYQ9QgSWmMvpzeh9gqZmJlCHuPmcWwPTHRuoCzn/9n5MHZ7IhMqzwl9Vu3Wby448HnJ43f/7tDBjQouDjdu1g0yb32VSuDOnausip5cuX07u362IC7JsOO3vmaFPiJjYmbqRD7Q60rd3Wwxn6Rs+PerJy/0qHY5WCKvHno39So1INL2clIuLfVGB5gAosMZXRAqsVsNXMRMqQqcA/XIz/AFxtdhKZ2NuzF/bJZhi7Ag6fPn+sUmAl1t63ltzE6rRrN93lVWNiqpCYeL5zW506kJjo4oS/BAXZn8vyFz/s/4Ep66fQILIBE6+ZSHCwb5eZJSYmUqeOmwYqQEBAALm5FeuZowe+foAPN31ITv75dcgdandg0d2LiKkS48PMRET8kwosD1CBJaayYuy5olnAveamUqb8CNwAXLhtVk1gNfZll14xB/u+V3bj/wcTf3Aefc3GyXz/tfudkPfsGcWll0YBcMstsHCh+0xq1oSjxvZyNtWsX2Yx9OuhRY7HVI4h8XEDlaKJLBb3v83o3bs3y5Yt80I2/iU/P5/Vh1ZzOus0cbFxRIZG+jolERG/pQLLA1Rgiam+AfoaiNO/Tj/WjSOpP1L3DTdh6VXhVff7Ci1ZcjfXX39ZwccG6gKOHYPoaPdxZvrPpv9wz5f3OB2vFFiJM0+f8WJGhd1zzz385z//cRmTnZ1NUFCQlzISEZGyqDi1gdq0i/jC9YC7Hg9lowt0BfYDbd4z8JxK5dMQ6L6nepMm1Qt93NXN3svR0b4vrgDu/dL1FGtGbgZT10/1UjZFffzxx1x9tfO1owkJCSquRETEo1RgifhKJvblbY5MBF70Yi5mSkqCSy+19x2vWxe2bPF1Ri79uWwZW957j/SkJLexpzJPub+gBbjkT5chMTFVCpYHnvPzz9Ctm+P4+vXts1e+diD5ADYD06yPLn3U/GRc+OGHHzh+/DhNmzYlNDSUKlWqMGbMGGw2G126dPFpbiIiUv5oHywRXzr3/MwCYCXwBFDfd+l4XExM4YeE0tOhTRu/bH/3ZXw8Zw4eLPh421tvgdVKj+nTqR0X5/Aci8ViaBlns0Z12PWH8/EpUxyvF/3+e/t/H30UvvsOWreG//7X/f28Zc62OYbisvN934mjRo0a7Nq1y9dpiIhIBaAZLBF/cAvwNuWruIqNdd6B4cwZqFLFu/m48FmnToWKqwL5+fzvvvtI2rCh4NDxtOMMWTCEv3/5dzrW6mjo+tuXvkS/fpcVOR4WFsh//tOfW25p7vL8N96Abdv8q7gCaFe7naE4q77ViIhIBaIZLBHxvOxsOHTIdcyZM/blgpdf7p2cLpYD/AS/bfmM3DOumzCsHDqUbj9/wyVvXEK+rXibbTWr0Qyr1cqiRXeTnJzB22+v4+TJs8TF1eOuu1qX4g34Xp/L+hiK696gu7mJiIiI+BF1EXRCXQSlOKZP38Bnn+0gMjKEd97pR0yM/8zO+ES3bvDjj+7jYmKMbfjkSX8AlwMZ9g9t2EiqlMD26Pc5Vnmdw1PyyOee23cZ37/sL+Eh4aSOdd+ivSzr/H5n1h1x/Hk7xzZe32ZERKRsUxdBES959911WK0TuP/+xaxYsY/PP99F7dqvERPzaoXbuLSQP103dSiQluY+xpP+ABpTUFwBWLBQK6MTPf/8gIYp/R2e9kvddLfFVYAlAMtf/wsJCOG13q+V++IKYO3wtcSGxzodX/G3FV7MRkRExPe0RFCkhD7+eDMjR37jcOzo0TNUq/Yyp0//s5hXfRV7t4vh2B/MKqNatoQDB9zH1TDQ5tyT2jo+bCUQGzY6H3mWxCo/khl4stD4d5cm25tZuCiy8mx5FXam5sBjB9hweAPX/ec60rLSsFqt3NzkZubfOd/XqYmIiHidCiyREhox4muX4+npObz99hpGjTLSBroBcOGsz7nC7TZgXony86mFCyHE3UZfGFtG6Ck5gIvGhRYsgJVGKbewo8aMQmP7q2UWe3lgRdOxbkdOjTXQtl5ERKSc0xJBkRI4cSKdrKw8t3H/+tf/DFwtisLF1YXmA45bePu14GB7T3FXqle3b+jkLbvdh9iAyMwmRY5nBVTMmSkREREpPtMKrOTkZAYNGkR4eDiRkZEMGzaMdDf73kyfPp3u3bsTHh6OxWIhJSWl0PiqVauwWCwOX+vXrwdg//79DsfXrFlj1luVCmjDBmONGc6edfcc1mHA3W/9HS9D9HtbtkCrVo7HYmLgxAnv5hNjJCifPEtmoSMhUVHkBbmfvgqwBJQsLxERESlXTCuwBg0axPbt21m+fDmLFi3ihx9+YMSIES7PycjIID4+nn/+0/FzK127diUxMbHQ67777qNhw4Z07Fh4P5rvvvuuUFyHDh089t5EWrWKNhQXHOzuh+6isyWODTMY52e2boWsLOjdG+rWhS5d4PRp73cOBDDwuJeVIBKr2H8ZYw0Npdu77zLgxx957prn3J770c0flTZDERERKQdMadO+c+dOWrRowfr16wsKn6VLl9K3b18OHTpEnTp1XJ6/atUqevTowalTp4iMjHQal5OTQ926dRk1ahT//ve/AfsMVsOGDfn1119p27Ztid+D2rSLO4GBE8nLc/3P58knu/LSS9e5iDD6YE8UcNJtlDM3zrmRxb8vxoY93yZRTfh52M/UqOTlJhO+1gtw19QuBYgoerjnrJ6s/HOlw1PuaH4Hn97xaSmTExEREX/l8zbtCQkJREZGFppV6tWrF1arlbVr13rsPl999RUnT55k6NChRcZuuukmatasyVVXXcVXX33l9lpZWVmkpaUVeom4Mn78NS7HAwIsboorMF5gOfiJ34CMnAysE6ws+n1RQXEF8Fvyb0S/Es3i3YtLdN0y6zvA1WNfk3H6qV4xZAXHHjtGdNj52cs6VeqQOjZVxZWIiIgUMKXASkpKombNmoWOBQYGEhUVRVJSksfu88EHH9CnTx/q1atXcKxKlSq89tprzJs3j8WLF3PVVVfRv39/t0XWpEmTiIiIKHjFxjrf10UE4N//voYHHnC89DQkJICkpNEGrjLc4N02GM7rQtGvRBcqrC52w9wbSnTdMu1P7L1DqsBfjQOhFfaZq0ddnxodHs2xJ49hG2/DNt7G4TGHCQ/RDLeIiIicV6wCa+zYsU6bTJx77dq1y6xcCzl06BDLli1j2LDCz6bUqFGD0aNH07lzZ6644gpefPFFBg8ezCuvvOLyeuPGjSM1NbXgdfDgQTPTl3Ji6tQbyMl5mjvuaMEll0TQvHl1vv76LjIz/0WNGlUMXOE9AzFW7EsEiycxNZGMnAy3cQ8uerDY1y7zBgCngXwgD9hKSScJRURERAop1j5YY8aMYciQIS5jGjVqRExMDMeOHSt0PDc3l+TkZGJiDLXycmvmzJlUr16dm266yW1s586dWb58ucuYkJAQQozs2yNykcDAQD799PZSXGELcLmLcfft4B0ZtXSUobjZW2cz9YapJbqHiDsJBxO4fd7tJKXbVy/UDa/LgjsW0L5Oex9nJiIiYo5iFVjR0dFER7vvnhYXF0dKSgobN24s6N63cuVK8vPz6dy5c8kyvYDNZmPmzJncc889BAUFuY3ftGkTtWvXLvV9RczRGvsOTF2BhAuOPwy8WeKrnsk5Yygu35Zf4nuIuHLr3FtZsHtBoWMHUg/Q4f0ODGo9iE9u/cRHmYmIiJjHlGewmjdvTnx8PMOHD2fdunX8/PPPjBw5koEDBxZ0EDx8+DDNmjVj3bp1BeclJSWxadMm9uzZA8DWrVvZtGkTycnJha6/cuVK9u3bx3333Vfk3h999BH//e9/2bVrF7t27eKFF17gww8/ZNQoY7/NF/Gd1dgLrXOvkhdXAE9e+aShuLYxbUt1HxFHpm+YXqS4utDsrbOZu3WuFzMSERHxDtP2wZo9ezbNmjWjZ8+e9O3bl6uuuorp06cXjOfk5LB7924yMs4/IzJt2jTatWvH8OH2B/+7detGu3btijSo+OCDD+jatSvNmjVzeO/nnnuODh060LlzZ7788ks+/fRTh50GRcqzHg17YLW4/yf+v7/9z2P33J+yn4cXP8yUdVM8dk0pm0Z/677Jy4OLK+DzfyIiUu6Zsg9WeaB9sKQ8+N++/3Htx9c6HR/aZigf9v+w1Pf55vdv6Dunb5HjESERpIxNKfX1peyxTDC2BYFtvL4FiYiI//P5Plgi4h96NOzBryN+pWpw1ULHA62BvNb7NY8UVyv+WOGwuAJIzUolcGKxHvUUERERKdP0k49IOde2dlvSxpm3cfZ1n7jeTDnPlsfoZaN5vc/rpuUg/seKlXxcN1AJsAR4KRsRERHv0QyWiJRYZmamy42Mz3ljzRueueEIoDZwKbDDM5cUc9zU1P0WGgNbDfRCJiIiIt6lAktESmzRnkWG4owUYS79HbAA7wNJwF6gJfY5+KzSXVrMsWDgAioHVXY6HhkSqTbtIiJSLqnAEpESa1Wjlfk3GQ3MdDKWB4San4KUTMqTKXSs3bHI8bh6cZwae8oHGYmIiJhPz2CJSIk1i3G8VcLFokKjSn6TyQZi/g6Uvl+HeFhgYCDrR6wHID0zHYAqoVV8mZKIiIjpVGCJSKk0jGzIvpR9LmP+eOSPkl3c6HNWH+HTAqv5O83ZdXIXYG/usPKelVzT8BrfJeSHVFid98trr7Hrwwv+wlostHvySZrfc4/vkhIREY/REkERKZW9j+x1+azNuK7jiAyNLNnFFxqMc92sjrNnz7JgwU6Sk8+WLA8npq+fjmWCpaC4sqeST/ePuxPyXIhH7yXlw2edOhUurgBsNn596SW+6NHDN0mJiIhHqcASkVJL/2c6s/vPxmo5/yWldpXanH3qLC9c90LJL+y6A7xbDz20CItlApUqvcytt35G9eovY7FMID7+P6W78F/uX3K/07Hs/GxqvFTDI/eR8uG7v/+d3DNnnI5nHjvGhpde8mJGIiJiBovNZitle6/yqTi7NYuIiSwGYq4Dvi18aODAeXz6qfM1hi1aVGf79pElTqv6S9VJzkx2G2cbry+xYjenZUtDcXdv325yJiIiUlzFqQ00gyXl2ESgChAG9PRxLlJi/QzEfFv0kKviCmDHjpOcPVvyJYNGiiuAuVvmlvgeIiIiUvaowJJy6Cfs0x7jgTNAJrDyr2ODfZiXlMgioL2L8T1FD91++6eGLt2kydQSpVQcS/csNf0e5cIh4GagOzDNt6mIiIiUhgosKWeygKtdjM/GPrMlZcpG7HXyZUAQUAl4CbABlxYNX7LEQdXlwOHDpz2VoVOPX/W46fco03KBaCAW+Ar4HngQ+3cnFVoiIlIGqcCScsbIvkzjTc9CTBAC/AZkY5+YfNJFaEiAoUtarUYe8HLs5sY3G4prVdMLmzGXZeHACQfHbdgLrXK0wjI4yv1+cNUMPqclIiL+SwWWlDP7fZ2A+IF58243FPfQQx1KfI+Fgxa6jflH+3+U+PoVwvOAu8fghnojEe+47ccfXQdYLFz/2WfeSUZEREyjAktEyp2ePR2sG3TgzTeNdNBwzjbehsVJm8N7W9/Luze+W6rrl3svGojJBNLNTsR77t6+ncDKRfeNC6lenbu3bfNBRiIi4mmBvk5ARMQMJ08+SfXqLzsd//HHez1yn/zx+Zw5c4aOMzuScjaF+zvcz7PXPuuRa5d7mQbjNgFXmZiHl92xbp2vUxARERNpHywntA9WWdUc2OUmxgLkeyEX8Qft2k1j06ajBR/Xrx/On38+5sOMpEAl3C8RBDgFRJqbioiIiCvFqQ00gyXlzCYg1E2Mlm1VJL/++oCvUxBnhgNvuYkJRMWViIiUKXoGS8qZEBxujFTg39hbk4mYJAfI8HUShe1J3sMjix/h063G9gfzmjcBdw0fX/BGIiIiIp6jJYJOaIlgebAA+Af2vt53A2/7Nh0BYMGCHQwc+AXZ2XkAdOlSh4SE4T7OygN6ASsuOlYT2Id9KZwPvPrzqzzx3RNFjteqXIukx5N8kJEDSUAD7FvYXewR4A1vJiMiIuJYcWoDFVhOqMAS8bzw8Bc4fTrH4dgXX9zOLbe08HJGHhIDHHUxfgavF1kv/vAi4/43zul4aGAoZ5828gCUlywCnsL+TNZVwIdoEbuIiPiN4tQGWiIoIl7RsOFkp8UVwK23zvNiNh70Bq6LK4BGXsjjIq6KK4DM3EwW7FzgpWwMuAHYDuwFPkbFlYiIlFn6FiYiXrF/f5rbmCuumM769SO8kI0Hua5j7NwVYB5mtHC6/bPbyR2fa3I2vrUwPp6MgwcLHavepg195szxUUYiIlLeaQZLREz3wQcbDcVt2JBociYmMLqXk/PJO497ZfUrhuLyyDM5E9+a06pVkeIK4OTmzczt0MEHGYmISEWgAktETLd163Ffp+B7Qd67VcPIht67mZ9a0KsXuHjEOD8zk+8fecSLGYmISEWhAkvKvPXrD1O79muEhv4fERGTeOONNb5OSS4yYoSx2YLAwDL4JamWrxMoavaA2YbimkU1MzkT3zmb6H429PB333khExERqWjK4E8zIufVrfsanTrNICkpnaysPNLSsnnssWUEBEzgxAk/24yoAmvRItpQ3Pvv32ByJiYw8jN6vOlZFBEa6G7Dbdg5aqcXMhEREalYVGBJmdWixbscOZLucCw/H2rVMvYcinjHjBmui6ewsECGDGnnpWw8qBXwkIvxesA3XsrlAmefPosFi9PxGf1meDEbERGRikMFlpRJOTk57Nx5wmVMfj5MnbrOSxmJO8OGdXBaZNWuXZmMjKe9nJEHvQNsxb4f1rmaJhSYAhTtseA1+ePzGd15dKFj9arWI+PJDIZ1HOajrERERMo3bTTshDYa9m+PPbaUN95Y6zYuMjKEU6fGeiEjKY716w/x7rvradEimiefvMrX6Ug59FW/fqTv3+8ypuEttxD3f//nnYRERKRMK05toH2wpEzasye54M/XUJ8Z3EQ9wskHNpHIXXzBAVLJyirfbajLqiuuqMesWfV8nYaUYzctXsycyy+HPMdfAwKrVFFxJSIiptASQSmTbrutOQBfMZD/MYRLiSKEQMIIJI5Y9vMIj9GZ6OjKPs5URHzl7i1bqNG2bZHjsddfzx1r3c+Ai4iIlISpBVZycjKDBg0iPDycyMhIhg0bRnq646YE5+JHjRpF06ZNCQsLo379+jz88MOkpqYWijtw4AD9+vWjUqVK1KxZkyeeeILc3NxCMatWraJ9+/aEhITQuHFjZs2aZcZbFB+59952vEhPbqAJAJaL/gfwGn1Yed/ffJmmiPhY79mzuXv79kKvq1991ddpiYhIOWZqgTVo0CC2b9/O8uXLWbRoET/88AMjRoxwGn/kyBGOHDnCq6++yrZt25g1axZLly5l2LDzD2Pn5eXRr18/srOzWb16NR999BGzZs3imWeeKYjZt28f/fr1o0ePHmzatIlHH32U++67j2XLlpn5dsXLHqcrgMNOaeeOXTq+uldzEhEREZGKzbQmFzt37qRFixasX7+ejh07ArB06VL69u3LoUOHqFOnjqHrzJs3j8GDB3PmzBkCAwP55ptvuOGGGzhy5Ai1atl3+Jw2bRpPPfUUx48fJzg4mKeeeorFixezbdu2gusMHDiQlJQUli5daui+anJRBjjvQF2Y2rj4ThrwBXA1cKmPcxEREREpoeLUBqbNYCUkJBAZGVlQXAH06tULq9XK2mKsfT/3JgIDAwuu27p164LiCqBPnz6kpaWxffv2gphevXoVuk6fPn1ISEhwep+srCzS0tIKvUSkhKZiL4AjgKFA478+7uHLpERERETMZ1qBlZSURM2aNQsdCwwMJCoqiqSkJEPXOHHiBM8991yhZYVJSUmFiiug4ONz13UWk5aWxtmzZx3ea9KkSURERBS8YmNjDeUoIhd5GfiHk7FVaCZLREREyrViF1hjx47FYrG4fO3atavUiaWlpdGvXz9atGjBs88+W+rruTNu3DhSU1MLXgcP+nB3UDEmyEBMH9OzkIs95WZ8L3DIG4lIcZ09fpyj69aRrq9/IiIiJVbsfbDGjBnDkCFDXMY0atSImJgYjh07Vuh4bm4uycnJxMTEuDz/9OnTxMfHU7VqVRYsWEBQ0PmfpGNiYli3bl2h+KNHjxaMnfvvuWMXxoSHhxMWFubwniEhIYSEhLjMS/xMKlDJxbgVMPbInXjKLINxbYET5qUhxXNs40YSxo3jzOHDBcdCqlWj/VNP0fDGG32YmYiISNlT7AIrOjqa6Ohot3FxcXGkpKSwceNGOnToAMDKlSvJz8+nc+fOTs9LS0ujT58+hISE8NVXXxEaGlrkus8//zzHjh0rWIK4fPlywsPDadGiRUHMkiVLCp23fPly4uLiivVexc+FARlAVeDivUSrox/gfeFNg3GnTM1CiiEpIYGVw4fDRf2Osk6dImHsWLJSUmj2N213ICIiYpRpz2A1b96c+Ph4hg8fzrp16/j5558ZOXIkAwcOLOggePjwYZo1a1YwI5WWlkbv3r05c+YMH3zwAWlpaSQlJZGUlERenv0n6N69e9OiRQv+9re/sXnzZpYtW8a//vUvHnrooYIZqAceeIC9e/fy5JNPsmvXLqZMmcJnn33GY489ZtbbFV8JA3KxdwrM+Ou/NlRc+Yr7373YGe0AKab7+fHHixRXF/r1lVfIv2ifQREREXHO1H2wZs+eTbNmzejZsyd9+/blqquuYvr06QXjOTk57N69m4yMDAB++eUX1q5dy9atW2ncuDG1a9cueJ17JiogIIBFixYREBBAXFwcgwcP5p577mHixIkF123YsCGLFy9m+fLltGnThtdee40ZM2bQp48eyCnXHK/+FG+abzBugKlZiEHHN20iKyXFZYwtL49dH33knYRERETKAdP2wSrrtA+WSAmFAZluYvRVxy/s+OADNr3+utu42Ouu4+o33jA/IQ/afWI3N8y5gT9O/QFA5eDKvNLrFR644gEfZyYiImWRX+yDJSIV1Flcf2VR4xG/EVytmqG4oKpVTc7EsyasmkCzd5ux59QebH/9Lz07nQeXPMilb2qfABERMZcKLBHxvDzgQQp/hWmJfWZLK3X9RoMbbgCr+28DLe67zwvZeMahlEM8+/2zTsf3puzllrm3eC8hERGpcFRgiYg5pmAvtM41HtkGaCcEvxIYHGwvslyo1qIF4Zdc4qWMSq/PbPcV/Je7v/RCJiIiUlGpwBIRvzN79mbeeWcNWVlZvk6l3Os6aRK1r77a4Vh4o0Zc95//eDmj0tl9crfbGBs2MnPdPSgoIiJSMsXeB0tExCwREZNIS8su+HjUqGUEBFjYv/9R6tVTsxmz9Jg2jZQ9e9j8xhtkJCYSXK0arUaMoFanTr5OrdhsBjuoZOZmEhoY6j5QRESkmFRgiYhfCAiYSH5+0R+O8/JsxMZO5tixx4iOVpFllsjGjbnmnXd8nUapRYVGceKs+43wIkMjzU9GREQqJC0RFBGf69FjpsPi6kINGrztpWykLJvZf6bbmEurqZOgiIiYRwWWHxs1aglBQROxWCZgsUzAap3AjTfO8XVaIh63atUBtzEZGbleyETKuhua3ED72u2djgdYAtj2j21ezEhERCoaFVh+qlWrKbzzznpyc8//Vt9mg0WLficq6iUfZiYi4t82jtjIAx0eIMASUOh40+pNSf9nup69EhERU+kZLD/0xhtr2L79uNPxU6cy6ddvNosXD/JiViIiZcfUG6Yy9Yapvk5DREQqIM1g+aF//Wul25hvvtnjhUxEvCMkJMB9kIiIiEgZoALLD505k+M2xmasE7FImfDll3e6jRkwoJkXMhEREREpHRVYIuJzffpcxh13NHc63qBBBPPnuy/CRERERHxNBZYfqlbN/QPYAQEWL2Qi4j2ffnoHBw8+RlRUWMGx0NAAVq68h337HvVdYiIiIiLFoCYXfmjGjBsZMGCey5jhw523IRYpq+rVC+fkySd9nYaIiIhIiWkGyw/demsLbrqpidPxpk2rM3XqDV7MSEREREREjFCB5ae+/PIu5s27vdBywUqVgnj11evYtWukDzMTERERERFnLDab+tE5kpaWRkREBKmpqYSHh/s6HRERERER8ZHi1AaawRIREREREfEQFVgiIiIiIiIeogJLRERERETEQ1RgiYiIiIiIeIgKLBEREREREQ9RgSUiIiIiIuIhKrBEREREREQ8JNDXCYiIuJKVlcXS/v05c+AAAJbAQDr/3//R6MYbfZyZiIiISFGawRIx0ZR1U+j5UU8mrpro61TKpB0ffcTn7dsXFFcAttxc1owdy2edO/swMxERERHHLDabzebrJPxRcXZrFrlY30/68s0f3xQ53iSqCbtH7fZBRmVPVlYWn7dv7zKmasOG3LhokZcyEhERkYqqOLWBZrBEPKz7rO4OiyuA35J/I+bVGC9nVDYt6dfPbczpffu8kImIiIiIcSqwRDzs+z+/dzl+9MxRUjJTvJNMGXY2MdFQ3PEtW0zORERERMQ4FVgiHjTg0wGG4hq/1djkTCqOlN1acikiIiL+QwWWiAet2r/KUNypzFPmJlIeWI19earfp4/JiYiIiIgYpwJLxIOqhlQ1FBdo0Q4J7lz+yCPugywWQtSERkRERPyIqQVWcnIygwYNIjw8nMjISIYNG0Z6errL+FGjRtG0aVPCwsKoX78+Dz/8MKmpqQUxmzdv5q677iI2NpawsDCaN2/Om2++Weg6q1atwmKxFHklJSWZ9l5FANYMWWMo7pVer5icSdnX6r77sAYHu4y55r33vJSNiIiIiDGm/hp90KBBJCYmsnz5cnJychg6dCgjRoxgzpw5DuOPHDnCkSNHePXVV2nRogV//vknDzzwAEeOHGH+/PkAbNy4kZo1a/LJJ58QGxvL6tWrGTFiBAEBAYwcObLQ9Xbv3l2ojWLNmjXNe7MiQExkDIGWQHJtuS7jHo572EsZlW0Df/2VL7p3J/P48SJj10yfTt0rr/RBViIiIiLOmbYP1s6dO2nRogXr16+nY8eOACxdupS+ffty6NAh6tSpY+g68+bNY/DgwZw5c4bAQMf14EMPPcTOnTtZuXIlYJ/B6tGjB6dOnSIyMrJE+WsfLCkN6wQrNhz/00p8JJGYSLVqL64/Fi4k/eBBGt56K+F16/o6HREREalA/GIfrISEBCIjIwuKK4BevXphtVpZu3at4eucexPOiqtzMVFRUUWOt23bltq1a3Pdddfx888/u7xPVlYWaWlphV4iJZU/Pp8Xe7yI1WL/J2bBwr2X34ttvE3FVQld2r8/bUaNUnElIiIifs20JYJJSUlFluQFBgYSFRVl+FmoEydO8NxzzzFixAinMatXr+bTTz9l8eLFBcdq167NtGnT6NixI1lZWcyYMYPu3buzdu1a2rdv7/A6kyZNYsKECYbyEjHiqW5P8VS3p3ydhoiIiIh4UbFnsMaOHeuwgcSFr127dpU6sbS0NPr160eLFi149tlnHcZs27aNm2++mfHjx9O7d++C402bNuX++++nQ4cOdO3alQ8//JCuXbsyefJkp/cbN24cqampBa+DBw+W+j2IiIiIiEjFUuwZrDFjxjBkyBCXMY0aNSImJoZjx44VOp6bm0tycjIxMa6XSJ0+fZr4+HiqVq3KggULCAoKKhKzY8cOevbsyYgRI/jXv/7lNu9OnTrx008/OR0PCQkhJCTE7XVEREREREScKXaBFR0dTXR0tNu4uLg4UlJS2LhxIx06dABg5cqV5Ofn07lzZ6fnpaWl0adPH0JCQvjqq68IDQ0tErN9+3auvfZa7r33Xp5//nlDeW/atInatWsbihURERERESkJ057Bat68OfHx8QwfPpxp06aRk5PDyJEjGThwYEEHwcOHD9OzZ08+/vhjOnXqRFpaGr179yYjI4NPPvmkULOJ6OhoAgIC2LZtG9deey19+vRh9OjRBc9zBQQEFBR+b7zxBg0bNqRly5ZkZmYyY8YMVq5cybfffmvW2xURERERETF3H6zZs2czcuRIevbsidVqZcCAAbz11lsF4zk5OezevZuMjAwAfvnll4IOg40bNy50rX379tGgQQPmz5/P8ePH+eSTT/jkk08Kxi+55BL2798PQHZ2NmPGjOHw4cNUqlSJyy+/nO+++44ePXqY+XZFRERERKSCM20frLJO+2CJiIiIiAj4yT5YIiIiIiIiFY0KLBEREREREQ9RgSViUFpaFh98sJGffvrT16mIiIiIiJ8ytcmFSHmwbNnv9O07h/z8wscvuyyK334b5ZukRERERMQvaQZLxIVly34nPr5ocQXw++/JRERM8n5SIiIiIuK3VGCJuBAfP8fleFpaNrNnb/ZSNiIiIiLi71RgiThx6FCaobihQ78yORMRERERKStUYIk4MWXKWkNxOTkO1g+KiIiISIWkAkvEiSZNavg6BREREREpY1RgiTgxZEg7Q3GXXRZlciYiIiIiUlaowBJxoU6dKm5jtm4d4YVMRERERKQsUIEl4sLhw2MIDQ1wOv7aa9cREhLixYxERERExJ+pwBJx4+zZf/HSSz2xWi0Fxxo0CCczcyyjR3f1YWYiIiIi4m8sNpvN5usk/FFaWhoRERGkpqYSHh7u63RERERERMRHilMbaAZLRERERETEQ1RgiYiIiIiIeIgKLBEREREREQ9RgSUiIiIiIuIhKrBEREREREQ8RAWWiIiIiIiIh6jAEhERERER8RAVWCIiIiIiIh6iAktERERERMRDVGCJiIiIiIh4SKCvE/BXNpsNgLS0NB9nIiIiIiIivnSuJjhXI7iiAsuJ06dPAxAbG+vjTERERERExB+cPn2aiIgIlzEWm5EyrALKz8/nyJEjVK1aFYvFUmgsLS2N2NhYDh48SHh4uI8yrHj0efcNfd59R59739Dn3Tf0efcNfd59Q5933yjN591ms3H69Gnq1KmD1er6KSvNYDlhtVqpV6+ey5jw8HD9o/ABfd59Q59339Hn3jf0efcNfd59Q59339Dn3TdK+nl3N3N1jppciIiIiIiIeIgKLBEREREREQ9RgVUCISEhjB8/npCQEF+nUqHo8+4b+rz7jj73vqHPu2/o8+4b+rz7hj7vvuGtz7uaXIiIiIiIiHiIZrBEREREREQ8RAWWiIiIiIiIh6jAEhERERER8RAVWCIiIiIiIh6iAsuJ5ORkBg0aRHh4OJGRkQwbNoz09HRD59psNq6//nosFgsLFy4sNHbgwAH69etHpUqVqFmzJk888QS5ubkmvIOyqSSf9/vvv59LL72UsLAwoqOjufnmm9m1a1ehGIvFUuQ1d+5cM99KmWLW511/310r7uc9OTmZUaNG0bRpU8LCwqhfvz4PP/wwqampheL09901sz7v+vvuWkm+zkyfPp3u3bsTHh6OxWIhJSWlSEyDBg2K/H1/8cUXTXoXZY9Zn/fS/JxUUZTkc5SZmclDDz1E9erVqVKlCgMGDODo0aOFYvQ1vrB3332XBg0aEBoaSufOnVm3bp3L+Hnz5tGsWTNCQ0Np3bo1S5YsKTRus9l45plnqF27NmFhYfTq1Yvff/+9eEnZxKH4+HhbmzZtbGvWrLH9+OOPtsaNG9vuuusuQ+e+/vrrtuuvv94G2BYsWFBwPDc319aqVStbr169bL/++qttyZIltho1atjGjRtn0rsoe0ryeX/vvfds33//vW3fvn22jRs32m688UZbbGysLTc3tyAGsM2cOdOWmJhY8Dp79qzZb6fMMOPzrr/v7hX3875161bbrbfeavvqq69se/bssa1YscJ22WWX2QYMGFAoTn/fXTPj866/7+6V5OvM5MmTbZMmTbJNmjTJBthOnTpVJOaSSy6xTZw4sdDf9/T0dJPeRdlj1ue9ND8nVRQl+Rw98MADttjYWNuKFStsGzZssHXp0sXWtWvXQjH6Gn/e3LlzbcHBwbYPP/zQtn37dtvw4cNtkZGRtqNHjzqM//nnn20BAQG2l19+2bZjxw7bv/71L1tQUJBt69atBTEvvviiLSIiwrZw4ULb5s2bbTfddJOtYcOGxfocq8ByYMeOHTbAtn79+oJj33zzjc1isdgOHz7s8txff/3VVrduXVtiYmKRAmvJkiU2q9VqS0pKKjg2depUW3h4uC0rK8vj76OsKc3n/UKbN2+2AbY9e/YUHLv4/ws5z6zP+/+3d/chTX1/HMA/P9Mtzdw0H9aTUKD2gKURmxZhlJRiIREIGj39Y6IhlGD2hxkZFmhZSFII2R9CYYEkRE+gFekSFCUpjTINis1QcZrl0/x8/+i3mze3qfNeNfd+wZCdc3a8973jmcftHjHe7ZMq9/LyclYoFDwyMiKUYbzbJlfuGO/2zTT36upquwuswsJCCY924ZArd6l+jhYyRzLq7e1lNzc3vn//vlDW0tLCRMR6vV4owxz/h1ar5bS0NOG+2WzmFStW8KVLl6y2T0hI4Li4OFGZTqfjEydOMDPz2NgYazQazs/PF+p7e3tZqVTy3bt3p3xc+IigFXq9ntRqNW3dulUoi46OJhcXF6qrq7P5uJ8/f1JSUhLduHGDNBqN1X5DQ0MpICBAKNu7dy/19fXRu3fvpD2Jf5CjuY83MDBApaWltGbNGlq9erWoLi0tjXx9fUmr1dLt27eJ8S/giEi+3DHe7ZMidyIik8lEXl5e5OrqKirHeLdOrtwx3u2TKndbLl++TMuWLaPw8HDKz8/HRzP/T67c5X4+FwJHMmpoaKCRkRGKjo4WytatW0eBgYGk1+tFbTHHEw0PD1NDQ4MoLxcXF4qOjp6Ql4Verxe1J/o9V1vat7e3k9FoFLVRqVSk0+ls9mmN6+RNnI/RaCR/f39RmaurK/n4+JDRaLT5uFOnTtG2bdsoPj7eZr/jX3yJSLhvr19n4WjuRETFxcWUmZlJAwMDFBISQs+fPyeFQiHUX7hwgXbt2kUeHh707NkzSk1NpR8/flB6eros5/IvkSt3jHf7ZpK7RVdXF+Xm5lJycrKoHOPdNrlyx3i3T4rcbUlPT6ctW7aQj48P1dbW0tmzZ8lgMNDVq1dn1O9CIFfucj6fC4UjGRmNRlIoFKRWq0XlAQEBosdgjv+tq6uLzGaz1bn372vCLWzN1ZZ8LV/ttZkKp3oHKysry+qFgeNvtp6QyVRWVlJVVRVdu3ZN2oNeAOTM3eLQoUPU2NhIL1++pODgYEpISKDBwUGhPjs7m7Zv307h4eF05swZyszMpPz8/Jme2rw2H3J3RrOROxFRX18fxcXF0YYNG+j8+fOiOoz3ucndGc1W7vacPn2adu7cSZs2baKUlBS6cuUKFRUV0dDQkKzfdy7Nh9yd1XzI3hnn+H+NU72DlZGRQceOHbPbZu3ataTRaOj79++i8tHRUerp6bH60T8ioqqqKmpra5vwV4eDBw/Sjh076MWLF6TRaCbsbGLZGcZWvwuBnLlbqFQqUqlUFBQURBEREeTt7U0VFRWUmJhotb1Op6Pc3FwaGhoipVI5rfP5V8x17hjvts009/7+foqJiaGlS5dSRUUFubm52W2P8f6bnLljvNs209ynS6fT0ejoKHV0dFBISIikfc8Xc537bD6f842c2Ws0GhoeHqbe3l7R75OdnZ12c3WGOd4aX19fWrRo0YRdFu3lpdFo7La3fO3s7KTly5eL2oSFhU394KZ8tZYTsVyYWF9fL5Q9ffrU7oWJBoOBm5ubRTci4uvXr/Pnz5+Z+c9F0ON3Nrl16xZ7eXnx4OCgvCf1D3Akd2sGBwfZ3d2dS0tLbba5ePEie3t7z+RwFwy5csd4t8/R3E0mE0dERHBUVBQPDAxM6XthvP8hV+4Y7/bNdJ6xt8nF38rKytjFxYV7enpmcsgLgly5S/W6sZA5kpFlk4sHDx4IZa2trRM2ufibM8/xWq2WT548Kdw3m828cuVKu5tc7Nu3T1QWGRk5YZOLgoICod5kMk17kwsssGyIiYnh8PBwrqur49evX3NQUJBoa82vX79ySEgI19XV2eyDbGzTvmfPHm5qauInT56wn58ftvEdZ7q5t7W1cV5eHtfX1/OXL1+4pqaG9+/fzz4+PsIvOpWVlVxSUsLNzc388eNHLi4uZg8PDz537tycnON8JEfuGO+Tm27uJpOJdTodh4aG8qdPn0Rb9Fq2x8d4n5wcuWO8T86R11WDwcCNjY1cUlLCRMSvXr3ixsZG7u7uZmbm2tpaLiws5KamJm5ra+OysjL28/PjI0eOzPr5zVdy5D6VfsGx7FNSUjgwMJCrqqq4vr6eIyMjOTIyUqjHHC927949ViqVfOfOHX7//j0nJyezWq0WdnQ9fPgwZ2VlCe1ramrY1dWVCwoKuKWlhXNycqxu065Wq/nhw4f89u1bjo+PxzbtUunu7ubExET29PRkLy8vPn78OPf39wv17e3tTERcXV1ts4+/F1jMzB0dHRwbG8vu7u7s6+vLGRkZou2Vnd10c//27RvHxsayv78/u7m58apVqzgpKYlbW1uFxzx+/JjDwsLY09OTlyxZwps3b+abN2+y2Wye7dObt+TInRnjfTLTzd3y12Rrt/b2dmbGeJ8KOXJnxnifjCOvqzk5OVZzt7xT3tDQwDqdjlUqFS9evJjXr1/PeXl5eNdwHDlyn0q/4Fj2v3794tTUVPb29mYPDw8+cOAAGwwGoR5z/ERFRUUcGBjICoWCtVotv3nzRqiLiorio0ePitqXl5dzcHAwKxQK3rhxIz969EhUPzY2xtnZ2RwQEMBKpZJ3797NHz58mNYx/Y/ZCfd1BAAAAAAAkIFT7SIIAAAAAAAgJyywAAAAAAAAJIIFFgAAAAAAgESwwAIAAAAAAJAIFlgAAAAAAAASwQILAAAAAABAIlhgAQAAAAAASAQLLAAAAAAAAIlggQUAAAAAACARLLAAAAAAAAAkggUWAAAAAACARLDAAgAAAAAAkMh/w63c7EAxnj8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2933, -0.1160], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "-0.2933128774166107\n",
            "datasets/birds-16/birds_dataset_triplets/birds_query_rank0_dissimilar: 0.8166666666666667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8166666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\", \"jpg\",ResNet,True)"
      ],
      "metadata": {
        "id": "DJdvWsN8UV5M",
        "outputId": "e2db6462-c23b-4953-9c8c-b9ee2f9a933d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DJdvWsN8UV5M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets/hsj/hsj_triplets/hsj_query_rank0_rank1: 0.724\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.724"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#-------------------------------Resnet-18-------------------------------------#\")\n",
        "ResNet.cuda()\n",
        "layer = ResNet._modules.get(\"Feature_Extractor\")\n",
        "ResNet.eval()\n",
        "t = torch.rand(1,3,224,224)\n",
        "ResNet()\n",
        "apply_framework_model(ResNet,layer,[1, 512, 1, 1])"
      ],
      "metadata": {
        "id": "u9ZsX_rfRFQ8"
      },
      "id": "u9ZsX_rfRFQ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKs_DfmpYJtH"
      },
      "id": "NKs_DfmpYJtH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet.eval()\n",
        "layer = ResNet.Feature_Extractor\n",
        "apply_framework(\"datasets/hsj/hsj_triplets/hsj_query_rank0_rank1\", \"jpg\", ResNet, layer, True, [1,100])"
      ],
      "metadata": {
        "id": "f5JIiRaqjCgG"
      },
      "id": "f5JIiRaqjCgG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet._modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uFO9er7NH9A",
        "outputId": "5c5cc9ff-e5a1-44b7-c258-d17043827647"
      },
      "id": "9uFO9er7NH9A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('Feature_Extractor',\n",
              "              ResNet(\n",
              "                (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                (relu): ReLU(inplace=True)\n",
              "                (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "                (layer1): Sequential(\n",
              "                  (0): BasicBlock(\n",
              "                    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                  (1): BasicBlock(\n",
              "                    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                )\n",
              "                (layer2): Sequential(\n",
              "                  (0): BasicBlock(\n",
              "                    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (downsample): Sequential(\n",
              "                      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (1): BasicBlock(\n",
              "                    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                )\n",
              "                (layer3): Sequential(\n",
              "                  (0): BasicBlock(\n",
              "                    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (downsample): Sequential(\n",
              "                      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (1): BasicBlock(\n",
              "                    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                )\n",
              "                (layer4): Sequential(\n",
              "                  (0): BasicBlock(\n",
              "                    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (downsample): Sequential(\n",
              "                      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (1): BasicBlock(\n",
              "                    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                    (relu): ReLU(inplace=True)\n",
              "                    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  )\n",
              "                )\n",
              "                (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "                  (1): LeakyReLU(negative_slope=0.01)\n",
              "                  (2): Linear(in_features=512, out_features=10, bias=True)\n",
              "                )\n",
              "              )),\n",
              "             ('Triplet_Loss',\n",
              "              Sequential(\n",
              "                (0): Linear(in_features=10, out_features=2, bias=True)\n",
              "              ))])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tutorial: https://discuss.pytorch.org/t/feature-extraction-in-torchvision-models-vit-b-16/148029/4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "ResNet.eval()\n",
        "layer = ResNet._modules[\"Feature_Extractor\"]\n",
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()\n",
        "    \n",
        "\n",
        "\n",
        "def get_vector(path):\n",
        "  with torch.no_grad():\n",
        "    p0 = Variable(normalize(to_tensor(scaler(Image.open(path).convert('RGB')))).unsqueeze(0)).cuda()\n",
        "\n",
        "    x_p0 = model._process_input(p0)\n",
        "\n",
        "    n = x_p0.shape[0]\n",
        "\n",
        "    batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "\n",
        "    x_p0 = torch.cat([batch_class_token, x_p0], dim=1)\n",
        "\n",
        "    x_p0 = encoder(x_p0)\n",
        "\n",
        "    x_p0 = torch.flatten(x_p0[:,0])\n",
        "\n",
        "    return x_p0\n",
        "\n",
        "\n",
        "def get_image_embeddings(path, image_type, isHSJOrBirds):  # returns a list of tensors\n",
        "    # path of form: traditional/ref/\n",
        "    feature_tensor_dict = dict()\n",
        "    feature_tensor_dict[\"ref\"] = []\n",
        "    feature_tensor_dict[\"p0\"] = []\n",
        "    feature_tensor_dict[\"p1\"] = []\n",
        "    feature_tensor_dict[\"decision\"] = []\n",
        "    \n",
        "    for image_no in range(1000):\n",
        "        im_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".\" + image_type\n",
        "        \n",
        "        feature_tensor_dict[\"ref\"].append(get_vector(path+\"/ref/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p0\"].append(get_vector(path+\"/p0/\" + im_no_name))\n",
        "        feature_tensor_dict[\"p1\"].append(get_vector(path+\"/p1/\" + im_no_name))\n",
        "\n",
        "        # now load decision\n",
        "        if isHSJOrBirds:\n",
        "          feature_tensor_dict[\"decision\"].append(0)\n",
        "        else: \n",
        "          decision_no_name =  (6-len(str(image_no)))*\"0\" + str(image_no) + \".npy\"\n",
        "          decision = np.load(path+\"/judge/\"+decision_no_name)\n",
        "          if decision[0] <= 0.5: \n",
        "                feature_tensor_dict[\"decision\"].append(0)\n",
        "          else: \n",
        "                feature_tensor_dict[\"decision\"].append(1)\n",
        "        \n",
        "    return feature_tensor_dict\n",
        "\n",
        "########################## GET MODEL PREDICTIONS\n",
        "def get_predictions(embeddings):\n",
        "  cosine_similarity_dict = dict()\n",
        "  cosine_similarity_dict[\"ref_and_p0\"] = []\n",
        "  cosine_similarity_dict[\"ref_and_p1\"] = []\n",
        "  cosine_similarity_predictions = []\n",
        "\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  # cos_sim = cos(query_vector.unsqueeze(0), ref2_vector.unsqueeze(0))\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      cosine_similarity_dict[\"ref_and_p0\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0)))\n",
        "      cosine_similarity_dict[\"ref_and_p1\"].append(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0)))\n",
        "      \n",
        "      if cosine_similarity_dict[\"ref_and_p0\"][image_no] >= cosine_similarity_dict[\"ref_and_p1\"][image_no]:\n",
        "          cosine_similarity_predictions.append(0)\n",
        "      else:\n",
        "          cosine_similarity_predictions.append(1)   \n",
        "  return cosine_similarity_predictions\n",
        "\n",
        "########################## GET MODEL ACCURACY\n",
        "def get_accuracy(predictions,embeddings):\n",
        "  decisions = embeddings[\"decision\"]\n",
        "\n",
        "  number_wrong_predictions = 0\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      if predictions[image_no] != decisions[image_no]:\n",
        "          number_wrong_predictions += 1\n",
        "      \n",
        "  accuracy = (1000 - number_wrong_predictions)/(1000)\n",
        "  return accuracy\n",
        "\n",
        "def get_average_reference_similarty_for_dataset(predictions,embeddings):\n",
        "  similarities_p0_p1 = []\n",
        "  similarities_p0_ref = []\n",
        "  similarities_p1_ref = []\n",
        "  \n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "  for image_no in range(1000):\n",
        "      similarities_p0_p1.append(float(cos(embeddings[\"p0\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "      similarities_p0_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p0\"][image_no].unsqueeze(0))))\n",
        "      similarities_p1_ref.append(float(cos(embeddings[\"ref\"][image_no].unsqueeze(0), embeddings[\"p1\"][image_no].unsqueeze(0))))\n",
        "\n",
        "  similarity = np.average(similarities_p0_p1)/(float(np.average(similarities_p0_ref)+np.average(similarities_p1_ref)))\n",
        "\n",
        "  return similarity\n",
        "\n",
        "def apply_framework(path, image_type,isHSJOrBirds):\n",
        "  embeddings = get_image_embeddings(path, \n",
        "                                    image_type,\n",
        "                                    isHSJOrBirds)\n",
        "  predictions= get_predictions(embeddings)\n",
        "  accuracy = get_average_reference_similarty_for_dataset(predictions, embeddings)\n",
        "  print(path + \": \" + str(accuracy))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "f78RRBARj5H3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "081f44cc-16d1-4f32-cc4d-975b7dec59e9"
      },
      "id": "f78RRBARj5H3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-bd419b44a5d7>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m_get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index {} is out of range'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m%=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of range"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bca6d128c81c4b1596e23411c8939c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3e1a8617cb748a39f282c11b9491e0a",
              "IPY_MODEL_d3e31a9020364e64be5c1ee8950e48b3",
              "IPY_MODEL_7548b65877f94225ae7889578eab5db2"
            ],
            "layout": "IPY_MODEL_75708c28b4694cee90680050814f3aa4"
          }
        },
        "f3e1a8617cb748a39f282c11b9491e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893c1dca55e44bcaa5e35d8c22fa8636",
            "placeholder": "​",
            "style": "IPY_MODEL_5372c90c9580459ab96dc596ac1361d5",
            "value": "100%"
          }
        },
        "d3e31a9020364e64be5c1ee8950e48b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f7d3090a62545049b4ebc69e7fd5f21",
            "max": 87319819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdd5022ec0ab422db94c6392eb0202f1",
            "value": 87319819
          }
        },
        "7548b65877f94225ae7889578eab5db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_009229f9541b4e25b6aa5b8a9724865c",
            "placeholder": "​",
            "style": "IPY_MODEL_71eaa211a3a9407083de38c547cc3305",
            "value": " 83.3M/83.3M [00:01&lt;00:00, 63.4MB/s]"
          }
        },
        "75708c28b4694cee90680050814f3aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "893c1dca55e44bcaa5e35d8c22fa8636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5372c90c9580459ab96dc596ac1361d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f7d3090a62545049b4ebc69e7fd5f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdd5022ec0ab422db94c6392eb0202f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "009229f9541b4e25b6aa5b8a9724865c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71eaa211a3a9407083de38c547cc3305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6678c3284bba4788bbe9d4e45414ffc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bb4ca5b79eb47cba9040c4866621628",
              "IPY_MODEL_0dca7d41c2c6492d94e48aa4ddac2b39",
              "IPY_MODEL_cffc167a23f346938726878ea7968396"
            ],
            "layout": "IPY_MODEL_ce4cd73fae2c4ca6ac365353239ad9fe"
          }
        },
        "0bb4ca5b79eb47cba9040c4866621628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff89d302c7b447b7865767d11316ab95",
            "placeholder": "​",
            "style": "IPY_MODEL_dec2a15e52af45b8aef578feec6d82c6",
            "value": "100%"
          }
        },
        "0dca7d41c2c6492d94e48aa4ddac2b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c88bfb26f94466f8ce31588b5e6faff",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2e0448838f24045bb09baa6a82f7d60",
            "value": 178793939
          }
        },
        "cffc167a23f346938726878ea7968396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c786b0d9c3a4f908b4bef29511cdf54",
            "placeholder": "​",
            "style": "IPY_MODEL_229cdc96df7c40029253c5e17f6edf74",
            "value": " 171M/171M [00:03&lt;00:00, 59.1MB/s]"
          }
        },
        "ce4cd73fae2c4ca6ac365353239ad9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff89d302c7b447b7865767d11316ab95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec2a15e52af45b8aef578feec6d82c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c88bfb26f94466f8ce31588b5e6faff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e0448838f24045bb09baa6a82f7d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c786b0d9c3a4f908b4bef29511cdf54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229cdc96df7c40029253c5e17f6edf74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29ec99b1c78842238a5f1e9a1f7bd471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1a3ed3a66f5412b908e14b58b920c67",
              "IPY_MODEL_053f280125b1431aba72492658913b36",
              "IPY_MODEL_4561cbe2c1214254a8d670b9f31caed1"
            ],
            "layout": "IPY_MODEL_1dac5859a10945a1adadb28dfc93ea0e"
          }
        },
        "d1a3ed3a66f5412b908e14b58b920c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f2ceb4c94c4ea091afc17ab882936f",
            "placeholder": "​",
            "style": "IPY_MODEL_48d0319c81204308a655424e9d974f41",
            "value": "100%"
          }
        },
        "053f280125b1431aba72492658913b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_917e6cbf826c4da399841399330f9dd9",
            "max": 241627721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92a20835c1ab41af8eee379d345dc6e0",
            "value": 241627721
          }
        },
        "4561cbe2c1214254a8d670b9f31caed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97d9fc3cfe04e60aebd1723a4cdf263",
            "placeholder": "​",
            "style": "IPY_MODEL_516db9148baa45f485cbcca81248a1f1",
            "value": " 230M/230M [00:02&lt;00:00, 96.4MB/s]"
          }
        },
        "1dac5859a10945a1adadb28dfc93ea0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f2ceb4c94c4ea091afc17ab882936f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48d0319c81204308a655424e9d974f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "917e6cbf826c4da399841399330f9dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92a20835c1ab41af8eee379d345dc6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d97d9fc3cfe04e60aebd1723a4cdf263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516db9148baa45f485cbcca81248a1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70b50fdf983e42098f0435261aec9fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e7907f27f8d4b49a3baeaab7fe5e4e7",
              "IPY_MODEL_5137bb0cc231464f9cfd9ed392242948",
              "IPY_MODEL_c1b2fa2f253245898d9b4e05a58080fa"
            ],
            "layout": "IPY_MODEL_0dd0bbaf98b145d59b62451b7f086a4a"
          }
        },
        "5e7907f27f8d4b49a3baeaab7fe5e4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_119d4d6375e44ac0be64515d49e889da",
            "placeholder": "​",
            "style": "IPY_MODEL_a174d6a993554ebfa2ad306f4dd9a76a",
            "value": "100%"
          }
        },
        "5137bb0cc231464f9cfd9ed392242948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c1d47cf2d1a4100bb6b94ee967daf4c",
            "max": 21444401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d49b8b4c2f24517bc3d444598143e26",
            "value": 21444401
          }
        },
        "c1b2fa2f253245898d9b4e05a58080fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef9cf3c084684193876d0b3438cba00d",
            "placeholder": "​",
            "style": "IPY_MODEL_5ec4fbb76c3f4effa82e83be008388e0",
            "value": " 20.5M/20.5M [00:00&lt;00:00, 62.4MB/s]"
          }
        },
        "0dd0bbaf98b145d59b62451b7f086a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "119d4d6375e44ac0be64515d49e889da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a174d6a993554ebfa2ad306f4dd9a76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c1d47cf2d1a4100bb6b94ee967daf4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d49b8b4c2f24517bc3d444598143e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef9cf3c084684193876d0b3438cba00d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec4fbb76c3f4effa82e83be008388e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "454d4a72e6d947629edb3e4d4d1d94f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_674d6cbdf514454496b8e40e45e1b511",
              "IPY_MODEL_a02ae67e2c9d4003a2f9c2b0e42585b0",
              "IPY_MODEL_555c13acdcf44b00a9c8c493ef5e6e19"
            ],
            "layout": "IPY_MODEL_4697c877055847ef96d9623af9283562"
          }
        },
        "674d6cbdf514454496b8e40e45e1b511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c5a5a7462524faca041881f07315276",
            "placeholder": "​",
            "style": "IPY_MODEL_bd3c1581e728426fab64db29b85b7d1e",
            "value": "100%"
          }
        },
        "a02ae67e2c9d4003a2f9c2b0e42585b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fab99da8597f423cad4263503d5d4d74",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93be17e1287b434181e33305f3e3a33d",
            "value": 10
          }
        },
        "555c13acdcf44b00a9c8c493ef5e6e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8218176d5af4cc093500ab9846984f9",
            "placeholder": "​",
            "style": "IPY_MODEL_fc122f29b68f4636b4337629915a9081",
            "value": " 10/10 [00:03&lt;00:00,  5.11it/s]"
          }
        },
        "4697c877055847ef96d9623af9283562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5a5a7462524faca041881f07315276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd3c1581e728426fab64db29b85b7d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fab99da8597f423cad4263503d5d4d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93be17e1287b434181e33305f3e3a33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8218176d5af4cc093500ab9846984f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc122f29b68f4636b4337629915a9081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a37de47d89b4f698eac7644dfdbc567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19e037905af34eee848cf04d4af5754c",
              "IPY_MODEL_7b09bcfe79ea4e12ab069f7b15f7cbe4",
              "IPY_MODEL_80990932e3a340d799e43d33e033fc3a"
            ],
            "layout": "IPY_MODEL_9f6b3a0532b44eafb5fdb9fdaba184b7"
          }
        },
        "19e037905af34eee848cf04d4af5754c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e73af0ba41444d69e514de760620e34",
            "placeholder": "​",
            "style": "IPY_MODEL_211111650c3f4d1183f2b98169d058ac",
            "value": "Epochs:   0%"
          }
        },
        "7b09bcfe79ea4e12ab069f7b15f7cbe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f471e3898354de18f5c33bcca1c8d3c",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0fab6beb33945b3bfb03ebddc5c430a",
            "value": 0
          }
        },
        "80990932e3a340d799e43d33e033fc3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83a11934a9c74410afdb860a15c7a81e",
            "placeholder": "​",
            "style": "IPY_MODEL_a57393348a614e509c635b460930f908",
            "value": " 0/15 [00:04&lt;?, ?it/s]"
          }
        },
        "9f6b3a0532b44eafb5fdb9fdaba184b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e73af0ba41444d69e514de760620e34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "211111650c3f4d1183f2b98169d058ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f471e3898354de18f5c33bcca1c8d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0fab6beb33945b3bfb03ebddc5c430a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83a11934a9c74410afdb860a15c7a81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a57393348a614e509c635b460930f908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "989373861fc740228308265601ec5b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0da2093d9dd464ebe309ad18df186be",
              "IPY_MODEL_1e9f10c0e55a4a498d82acd03158932f",
              "IPY_MODEL_41b92fca9733439b8095bb378f8b2269"
            ],
            "layout": "IPY_MODEL_00e3fd75243a4997b53529dd2ec556c6"
          }
        },
        "d0da2093d9dd464ebe309ad18df186be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5b8d537f780483d9841096034c9998f",
            "placeholder": "​",
            "style": "IPY_MODEL_52040a39ae0042fdb2bbe983227433f8",
            "value": "Training:   0%"
          }
        },
        "1e9f10c0e55a4a498d82acd03158932f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e44466fb9ef47da9593fc3ed4f0f046",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23f82c9a0a8b46f1b6f0b6ff1dba1312",
            "value": 0
          }
        },
        "41b92fca9733439b8095bb378f8b2269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82416691ed514d4bb524c675192af724",
            "placeholder": "​",
            "style": "IPY_MODEL_81a9f870cbe2473ab4f773e9431b5095",
            "value": " 0/8 [00:04&lt;?, ?it/s]"
          }
        },
        "00e3fd75243a4997b53529dd2ec556c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b8d537f780483d9841096034c9998f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52040a39ae0042fdb2bbe983227433f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e44466fb9ef47da9593fc3ed4f0f046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f82c9a0a8b46f1b6f0b6ff1dba1312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82416691ed514d4bb524c675192af724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a9f870cbe2473ab4f773e9431b5095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}